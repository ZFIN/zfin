<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
<HEAD>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">

<TITLE>Implementation and Development, ZFIN Documentation
</TITLE>
<link rel=stylesheet href="doc.css" type="text/css">
</HEAD>

<script language="JavaScript" src="/header.js" type="text/javascript"></script>

<!-- :KLUDGE: Close out the table that is opened by header.js -->

</td></tr></table>

<div class="zdoc">


<CENTER>
<H1>ZFIN Documentation:<br>Implementation and Development
<br>
This documentation is preserved for historical purposes, and NOT updated. Updated documentation is found here: <a href="http://almost.zfin.org/doc">http://almost.zfin.org/doc</a><br>
</H1>

<p>$Id: impl.html,v 1.65 2006-03-30 23:17:38 peirans Exp $
<p><a href="index.html">Back to Table of Contents</a>
<br><a href="db.html">Previous Section</a>
&nbsp;&nbsp;&nbsp;&nbsp;
<a href="standards.html">Next Section</a>


</CENTER>

<p>This document is one of several that describe the Zebrafish Information
Network, or ZFIN.  This document focuses on how ZFIN is implemented and
how development and maintenance is done in ZFIN.


<!-- =================================================================== -->
<!-- ========= BIG PICTURE ============================================= -->
<!-- =================================================================== -->

<h1 class="zdoc"><a name="Big Picture">Big Picture</a></h1>



<!-- ========= ARCHITECTURE ============================================ -->

<h2 class="zdoc"><a name="Architecture">Architecture</a></h2>

<p>The ZFIN web site is implemented using 
<a href="admin.html#Apache">Apache</a> for the web server, 
<a href="admin.html#Database Administration">Informix</a> for the 
database management system, and the Informix 
<a href="#Web Datablade">Web DataBlade</a> product to generate 
dynamic pages.  ZFIN also uses some CGI scripts that access the database
as well.

<center>
<br>
<img src="Implementation/bigPicture.gif" alt="ZFIN Implementation Big Picture">
<br>
</center>


<!-- ========= DEVELOPMENT AND DEPLOYMENT ============================== -->

<h2 class="zdoc">
<a name="Development and Deployment">Development and Deployment</a>
</h2>

<p>ZFIN uses 
<a href="#Source Code Control and CVS">CVS</a> and 
<a href="#Makefiles and the ZFIN_WWW Source Tree">gmake</a>
for development and deployment.  We also use
some <a href="#Generic vs. Specific">custom scripts and configuration files</a>
to map files to 
<a href="#Web Site / Database / Machine Matrix">different test and production 
web sites and databases</a>.  The remainder of this section gives a high
level, somewhat graphical, overview of how we use CVS and gmake to develop and
deploy changes.  This material is covered in more detail in the 
<a href="#ZFIN Development Environments">ZFIN Development Environments</a>
section.

<p>The explanations and diagrams here assume that you are creating the 
<a href="#Development DB Matrix">zezem.zfin.org</a> 
development web site, and that the zezem web site has <i>zezdb</i> as
its backing database.



<h3 class="zdoc">
<a name="Creating a Test Web Site">Creating a Test Web Site</a>
</h3>

<p>First, lets talk about creating your test web site.  This section assumes
that you have already created the backing database.  This topic is covered
in much more detail in the 
<a href="#Creating a ZFIN Development Environment, An Example">Creating 
a ZFIN Development Environment, An Example</a> section.  For brevity,
some of the necessary steps that are covered in that section are omitted 
from this section.

<table class="zdoc">
 <caption>Creating a test web site</caption>
 <tr>
  <th>Shell commands</th>
  <th>Purpose</th>
  <th>What happens</th>
 </tr>
 <tr>
  <td>
    <span class="shell">cd <a href="#ZFIN Users">/research/zusers</a>/<i>username</i><br>mkdir zezem</span>
  </td>
  <td>
    Create a place to hold source files.  (Directory can actually have
    any name.)
  </td>
  <td>
   <img src="Implementation/ddmkdir.gif" alt="mkdir">
  </td>
 </tr>
 <tr>
  <td><span class="shell">cd zezem<br><a href="#Source Code Control and CVS">cvs</a> <a href="#CVS checkout">checkout</a> <a href="#Makefiles and the ZFIN_WWW Source Tree">ZFIN_WWW</a></span></td>
  <td>
    Get current copies of all source files and makefiles
  </td>
  <td>
   <img src="Implementation/ddcheckout.gif" alt="cvs checkout">
  </td>
 </tr>
 <tr>
  <td><span class="shell">cd ZFIN_WWW<br><a href="#Makefiles and the ZFIN_WWW Source Tree">gmake</a></span></td>
  <td>Create the web site, and update the database.
  </td>
  <td>
   <img src="Implementation/ddgmake.gif" alt="gmake">
  </td>
 </tr>
</table>

<p>After doing the above, 
<a href="#Creating a ZFIN Development Environment, An Example">plus some other
steps</a>, you will now have a working web site.  



<h3 class="zdoc">
<a name="Updating a File">Updating a File</a>
</h3>

<p>The next step is to modify 
something, deploy it to your test web site, test it, and then commit it to CVS.
This topic is covered in much more detail in the 
<a href="#Updating a File, An Example">Updating a File, An Example</a> section.
For brevity, some of the necessary steps that are covered in that section are 
omitted from this section.

<table class="zdoc">
 <caption>Updating a file</caption>
 <tr>
  <th>Shell commands</th>
  <th>Purpose</th>
  <th>What happens</th>
 </tr>
 <tr>
  <td><span class="shell">cd /research/zusers/<i>username</i>
cd zezem/ZFIN_WWW/<i>somedir</i>
cvs <a href="#CVS edit">edit</a> <i>somefile</i></span></td>
  <td>
    Tell CVS you are going to edit the file.
  </td>
  <td>
   <img src="Implementation/ddedit.gif" alt="cvs edit">
  </td>
 </tr>
 <tr>
  <td><span class="shell">emacs/vi/whatever <i>somefile</i>
gmake</span></td>
  <td>
    Modify the file and then push modified version to zezem web site / DB.
  </td>
  <td>
   <img src="Implementation/ddtest.gif" alt="gmake">
  </td>
 </tr>
 <tr>
  <td><span class="shell">cvs <a href="#CVS commit">commit</a> <i>somefile</i></span></td>
  <td>
    Put modifications into CVS where they can be picked up by others
  </td>
  <td>
   <img src="Implementation/ddcommit.gif" alt="cvs commit">
  </td>
 </tr>
</table>



<h3 class="zdoc">
<a name="Deploying a Change to Production">Deploying A Change to Production</a>
</h3>

<p>The changes are now in CVS where they can be pushed to the 
<a href="intro.html#Pre-Production and Sandbox Site">prepoduction site</a>,
and then to the <a href="intro.html#Production Site">production site</a>.
This topic is covered in much more detail in the 
<a href="#Putting Changes Into Production"> Putting Changes Into Production</a>
section.  This section only shows the last part, putting the changes into 
production.

<table class="zdoc">
 <caption>Putting change into production</caption>
 <tr>
  <th>Shell commands</th>
  <th>Purpose</th>
  <th>What happens</th>
 </tr>
 <tr>
  <td><span class="shell">cd <a href="#ZFIN Prod">/research/zprod</a>/users
cd zfin.org/ZFIN_WWW
cvs -q <a href="#CVS update">update -dP</a></span></td>
  <td>
    Pick up modified file from CVS
  </td>
  <td>
   <img src="Implementation/ddupdate.gif" alt="cvs update">
  </td>
 </tr>
 <tr>
  <td><span class="shell">gmake</span></td>
  <td>
    Apply changes to production web site / database
  </td>
  <td>
   <img src="Implementation/dddeploy.gif" alt="gmake">
  </td>
 </tr>
</table>





<!-- =================================================================== -->
<!-- ========= MACHINES AND INFORMIX ENGINES =========================== -->
<!-- =================================================================== -->

<h1 class="zdoc"><a name="Machines and Informix Engines">Machines and Informix Engines</a></h1>

<p>ZFIN has several servers, each with its own Informix Engine running on it.
On all the ZFIN servers, there is a 1:1 correspondence between machines and 
Informix engines.
Multiple databases can be run under one Informix engine.  On 
<a href="#Production Server">the production server</a> we
only have one database, but on the <a href="#Development Server">development
server</a> we have many databases.

<p>Each Informix engine has a corresponding C shell script that can be
sourced to set up the needed environment variables for that engine.  These
scripts are in 

<div class="fileline">
<a href="#ZFIN Commons">/private/ZfinLinks/Commons</a>/env/<i>InformixEngineName</i>
</div>

<p>Note that there is some confusion about the word <i>server</i>.  The
machines that host Informix engines are clearly servers.  The 
Informix documentation also refers to the Informix engines as servers.
However, within the Informix community (and when dealing with 
<a href="admin.html#Dealing With IBM Corporation">IBM 
Technical Support</a>) they are known as engines.  <i>Engine</i> 
is the term used in this documentation.  <i>Server</i> or <i>machine</i> 
will be used in this documentation to indicate actual machines.

<p>Each Informix engine has a unique name that identifies it.  They are
named after mutants that start with "w".


<!-- ========= PRODUCTION SERVER ======================================= -->

<h2 class="zdoc"><a name="Production Server">Production Server</a></h2>

<p>Helix usually hosts the 
<a href="intro.html#Production Site">production web site</a> and the 
<a href="#Mirror Sites">internal mirror site</a>.

<p>Wildtype is the name of the Informix engine running on helix.  There is 
<a href="#Production DB Matrix">a single database running in wildtype</a>.

<p>The C shell script that can be sourced to define the environment
variables needed to use wildtype is at

<div class="fileline">
<a href="http://cvs.zfin.org/cvs/cvsweb.cgi/Commons/env/wildtype">/private/ZfinLinks/Commons/env/wildtype</a>
</div>

<p>Helix is a Sun Fire V440 with 4 Sun UltraSparc IIIi 1.3 GHZ 64-bit CPUS,
and 16 GB of memory.  It is a hardware twin of the 
<a href="#Development Server">development server</a>.  The disk layout for
helix is described in the 
<a href="admin.html#Production Server Disk">Production Server Disk</a> section.
Helix became the production server for ZFIN in
2004/11, replacing <a href="#Past Servers">chromix</a> in that job.



<!-- ========= DEVELOPMENT SERVER ====================================== -->

<h2 class="zdoc"><a name="Development Server">Development Server</a></h2>

<p>Embryonix is the main development machine for ZFIN.  Every test
<a href="#Development DB Matrix">ZFIN domain name</a> maps to embryonix.  
This includes the 
<a href="intro.html#Pre-Production and Sandbox Site">preproduction/sandbox</a>
site.
Embryonix also hosts the 
<a href="intro.html#Production Site">production web site</a> when the
<a href="#Production Server">production server</a> is down.

<p>Wanda is the Informix engine running on embryonix.  It supports all of the 
development databases, and the 
<a href="intro.html#Beta Testing Site">beta test site</a> database as well.

<p>The C shell script that can be sourced to define the environment
variables needed to use wanda is at

<div class="fileline">
<a href="http://cvs.zfin.org/cvs/cvsweb.cgi/Commons/env/wanda">/private/ZfinLinks/Commons/env/wanda</a>
</div>

<p>Embryronix is a Sun Fire V440 with 4 Sun UltraSparc IIIi 1.3 GHZ 64-bit CPUS,
and 16 GB of memory.  It is a hardware twin of the 
<a href="#Production Server">production server</a>.  The disk layout for
embryonix is described in the 
<a href="admin.html#Development Server Disk">Development Server Disk</a> 
section.  Embryonix became the development server for ZFIN in 
2004/03, replacing <a href="#Upgrade Server">bionix</a> in that job.


<!-- ========= UPGRADE SERVER ========================================== -->

<h2 class="zdoc"><a name="Upgrade Server">Upgrade Server</a></h2>

<p>From 2001/02 to 2004/03, Bionix was the main development machine for ZFIN.
Prior to that <a href="#Past Servers">chromix</a> was both the 
production and development machine.

<p>Bionix is now used mainly to test software upgrades before applying them
to the <a href="#Development Server">development</a> and 
<a href="#Production Server">production</a> servers.  It is also used as
a terminal server for ZFIN users to do tasks
that don't involve an Informix database.

<p>Wavy is the Informix engine on bionix.  However, wavy is only up and 
running on those rare occasions when we are testing software upgrades.  All
development is done on the <a href="#Development Server">development server</a>.

<p>The C shell script that can be sourced to define the environment
variables needed to use wavy is at

<div class="fileline">
<a href="http://cvs.zfin.org/cvs/cvsweb.cgi/Commons/env/wavy">/private/ZfinLinks/Commons/env/wavy</a>
</div>

<p>Bionix is a Sun Enterprise 220R with 2 Sun UltraSPARC-II 450 MHz 64-bit 
CPUs, and 2 GB of memory.
The disk layout for bionix is described in the 
<a href="admin.html#Upgrade Server Disk">Upgrade Server Disk</a> section.
bionix was acquired by ZFIN in 2000/10.



<!-- ========= BLAST SERVER ============================================ -->

<h2 class="zdoc"><a name="BLAST Server">BLAST Server</a></h2>

<p>As of 2005/02, ZFIN's BLAST server is embryonix, the ZFIN 
<a href="#Development Server">development server</a>.  We are in the process
of moving it from the development server, to its own server, 
<a href="http://genomix.cs.uoregon.edu">genomix</a>, an Apple G5 Xserve cluster,
running <a href="admin.html#BioTeam iNquiry">BioTeam's iNquiry software</a>. 

<p>Genomix is unique among ZFIN servers.  It is the only one that is not
a Sun Solaris box.  We also have distinct ways of 
<a href="#BLAST">maintaining</a> and 
<a href="admin.html#BLAST Server Administration">administering</a>
this server.

<p>Genomix is an Apple G5 Xserve cluster with one head node, five compute
nodes, and its own network switch to tie the head node and the compute nodes.
The head node has two 2.0 GHz PowerPC G5 CPUs, 5 GB of DDR400 ECC memory,
and 3 internal 250 GB Serial ATA drives, configured as 
<a href="admin.html#RAID">RAID 5</a>.  The compute nodes all have
two 2.0 GHz PowerPC G5 CPUs, 5 GB of DDR400 ECC memory, and one internal
80 GB Serial ATA drive.



<!-- ========= FOGBUGZ SERVER ========================================== -->

<h2 class="zdoc"><a name="FogBUGZ Server">FogBUGZ Server</a></h2>

<p>FogBUGZ is a project management / defect tracking tool used by ZFIN 
to keep track ob bugs, new feature development, suggestions, and technical
discussions.  FogBUGZ runs on ZFIN's only Windows server, 
<a href="http://zfinwinserver1.uoregon.edu/fogbugz/">zfinwinserver1.uoregon.edu</a>, 
which resides in Room 333 of the ZFIN office space.  
See the <a href="intro.html#FogBUGZ">FogBUGZ section</a> for more
information on how FogBUGZ is used at ZFIN.



<!-- ========= NON-SERVER MACHINES ===================================== -->

<h2 class="zdoc"><a name="Non-Server Machines">Non-Server Machines</a></h2>

<p>ZFIN owns a number of machines that are used purely as clients.  They are:

<table class="definition">
  <caption>ZFIN Client Machines</caption>
  <tr>
    <td class="term">poetix</td>
    <td>
      A Sun Workstation.  This resides on some CS grad 
      student's desk.  Can be used as a terminal server.
    </td>
  </tr>
  <tr>
    <td class="term">Lots of Windows PCs</td>
    <td>
      If you are having troubles with the Windows
      machine on your desk, Tom, Kevin, and Brock are good places to start.
      After that you can try Mike McHorse or Don Pate in Neuroscience.
    </td>
  </tr>
  <tr>
    <td class="term">Lots of Macs</td>
    <td>
      If you are having trouble with the Mac on your
      desk, Erik and Dave F are good places to start.  After that Mike and
      Don in Neuroscience may be able to help.
    </td>
  </tr>
</table>

<p>In addition ZFIN also owns an <a href="admin.html#UPS">uninterruptable
power supply (UPS)</a> and a darn large rack in which all this equipment is
mounted.


<!-- ========= PAST SERVERS ============================================ -->

<h2 class="zdoc"><a name="Past Servers">Past Servers</a></h2>

<p>ZFIN has been around long enought to have a history.  You will occassionally
see references to servers that no longer exist.  Here are the details on those
machines.

<table class="definition">
  <caption>Past ZFIN Servers</caption>
  <tr>
    <td class="term">Zfishstix</td>
    <td>Zfishstix was the original ZFIN production server.  It was powered 
      down when chromix was brought 
      online on 2000/03/15.  It was not used after that.  It was officially
      decommissioned and recycled in 2002/09.
    </td>
  </tr>
  <tr>
    <td class="term">Chromix</td>
    <td>Chromix was the production server after zfishstix.  
      It was acquired by ZFIN in 1999 and it was the
      production server for almost 5 years.  It was decommissioned in 2004/11.
      For most of that time, and for most purposes, it had more power than we
      needed.  This changed in August 2004, when some critical mass of data, 
      features, and users was reached.  Overnight chromix was no longer 
      up to the task.  FogBUGZ 
      <a href="http://zfinwinserver1.uoregon.edu/fogbugz/default.asp?pg=pgEditBug&amp;command=view&amp;ixbug=452">case 452</a>
      chronicles our efforts to make chromix run faster during its last days.
      FogBUGZ 
      <a href="http://zfinwinserver1.uoregon.edu/fogbugz/default.asp?pg=pgEditBug&amp;command=view&amp;ixbug=454">case 454</a>
      describes how we picked <a href="#Production Server">chromix's replacement</a> 
      and then moved to it.
      Chromix was a Sun Enterprise 450 with 4 Sun UltraSPARC-II 300 MHz, 
      64-bit CPUs, and 4 gigabytes of memory.  We upgraded from 1 GB of memory 
      to 4 GB of memory in 2003.
    </td>
  </tr>
  <tr>
    <td class="term">Genetix</td>
    <td>Genetix was the original ZFIN development server.  It stopped serving
      that purpose in 2000 when chromix took over that role.  From then 
      until genomix was powered down in 2002/06, it was used as a workstation. 
      Genetix had a single Sun UltraSPARC 142 MHz 64 bit CPU, and 128 MB of 
      memory. It was officially decommissioned and recycled in 2002/09.
    </td>
  </tr>
</table>


<!-- =================================================================== -->
<!-- ========= DIRECTORIES AND FILES =================================== -->
<!-- =================================================================== -->

<h1 class="zdoc"><a name="Directories and Files">Directories and Files</a></h1>


<!-- ========= WEB SITES =============================================== -->

<h2 class="zdoc"><a name="Web Sites">Web Sites</a></h2>

<p>The files in the production web site are usually located at

<div class="fileline">
<a href="#ZFIN Prod">/research/zprod/</a>www_homes/zfin.org
</div>

<p>The files to support the internal ZFIN mirror site are usually located at

<div class="fileline">
<a href="#ZFIN Prod">/research/zprod/</a>www_homes/mirror
</div>

<p>The files in the other ZFIN web sites are located at

<div class="fileline">
<a href="#ZFIN Central">/research/zcentral/</a>www_homes/<i>VirtHost</i>
</div>

<p>Web sites also have source trees from which the web sites are actually
built.  See
<a href="#ZFIN Development Environments">ZFIN Development Environments</a>
for details.


<h3 class="zdoc"><a name="Web Site Directory Structure">
                          Web Site Directory Structure</a></h3>

<p>The ZFIN web site directory tree contains everything (almost) that goes into
making a ZFIN web site.  The actual directory structure of a web site very
closely parallels the source directory from which it was built.  The
structure described here is for both
the web sites themselves and the source trees from which they are built.

<br>
<table class="definition">
  <caption>Important directories in the web site directory structure</caption>
  <tr>
    <td>
      <span class="file">cgi-bin/</span>, or<br>
      <span class="file">cgi-bin_<i>VirtHost</i></span>
    </td>
    <td>
      The CGI bin directory for the web site.  Contains all server side 
      scripts that are directly executable through HTTP.
    </td>
  </tr>
  <tr>
    <td class="file">
      client_apps/
    </td>
    <td>Contains all ZFIN <a href="#Applets">applets</a>.  
      In other words this contains all 
      executables that are run on the client side.
    </td>
  </tr>
  <tr>
    <td class="file">
      <a name="home">home/</a>
    </td>
    <td>
      This is the document root of the web site.  All of the static
      web pages and the app pages are stored under this directory, as are 
      all the images.

  <tr>
    <td class="file">
      <font color="#808080">
      home/<br>
      </font>
      &nbsp;&nbsp;images/
    </td>
    <td>
      Contains most of the generic graphics files used in the web sites.  
      This directory does not contain pictures of specific mutants, fish 
      features, or zebrafish at specific developmental stages.
      Rather, it contains icons and generic pictures of fish.

  <tr>
    <td class="file">
      <font color="#808080">
      home/<br>
      &nbsp;&nbsp;images/<br>
      </font>
      &nbsp;&nbsp;&nbsp;&nbsp;LOCAL/
    </td>
    <td>
      This contains icons and graphics that are specific to the ZFIN site
      such as pictures of fish, or ZFIN logos.  The other images directories
      contain generic icons that could be used by any site.

  <tr>
    <td class="file">
      <a name="home/ZFIN">
      <font color="#808080">
      home/<br>
      </font>
      &nbsp;&nbsp;ZFIN/</a>
    </td>
    <td>
      Contains files that are used to access the database.

  <tr>
    <td class="file">
      <font color="#808080">
      home/<br>
      &nbsp;&nbsp;ZFIN/
      </font>
      <br>&nbsp;&nbsp;&nbsp;&nbsp;APP_PAGES/
    </td>
    <td>
      This is the app page hierarchy containing all of the web pages that
      are used to access the ZFIN database.  This contains mostly app pages,
      but also contains some HTML files as well.  Note that the 
      app pages that are actually run are defined in the database.  The app 
      pages defined in these directories are where the app pages in the 
      database 
      should be loaded from, but there is no guarantee that the two agree.
    </td>
  </tr>
  <tr>
    <td class="file">
      <font color="#808080">
      home/<br>
      &nbsp;&nbsp;ZFIN/<br>
      &nbsp;&nbsp;&nbsp;&nbsp;APP_PAGES/<br>
      </font>
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*/
    </td>
    <td>These directories contain the app page definitions.  

  <tr>
    <td class="file">
      <a name="home/ZFIN/misc_html">
      <font color="#808080">
      home/<br>
      &nbsp;&nbsp;ZFIN/<br>
      </font>
      &nbsp;&nbsp;&nbsp;&nbsp;misc_html/</a>
    </td>
    <td>
      Contains various HTML files that are used to support the app pages.

  <tr>
    <td class="file">
      <a name="home/zf_info">
      <font color="#808080">
      home/<br>
      </font>
      &nbsp;&nbsp;zf_info/<br></a>
    </td>
    <td>
      The static web page hierarchy.  This part of the web site
      is mostly managed by Monte and Sherry.  See the 
      <a href="#Static Web Page Updates">Static Web Page Updates</a> section 
      for details on how this is done.

  <tr>
    <td class="file">
      <font color="#808080">
      home/<br>
      &nbsp;&nbsp;zf_info/<br>
      </font>
      &nbsp;&nbsp;&nbsp;&nbsp;SEARCH_SITE/
    </td>
    <td>
      The home page has a "Search this site" button that searches the static 
      web pages on the ZFIN site for text strings.  The web pages to support 
      that are in this directory.

  <tr>
    <td class="file">
      <font color="#808080">
      home/<br>
      &nbsp;&nbsp;zf_info/<br>
      </font>
      &nbsp;&nbsp;&nbsp;&nbsp;anatomy/
    </td>
    <td>
      Pages about zebrafish anatomy including the anatomical dictionary.

  <tr>
    <td class="file">
      <font color="#808080">
      home/<br>
      &nbsp;&nbsp;zf_info/<br>
      </font>
      &nbsp;&nbsp;&nbsp;&nbsp;dbase/
    </td>
    <td>
      Public documents and published papers about the web site and the 
      underlying database.

  <tr>
    <td class="file">
      <font color="#808080">
      home/<br>
      &nbsp;&nbsp;zf_info/<br>
      </font>
      &nbsp;&nbsp;&nbsp;&nbsp;images/
    </td>
    <td>Images of zebrafish at specific stages of development.
      The images in this directory can be viewed as data, whereas the images
      in home/images can be viewed as graphic design and decoration.

  <tr>
    <td class="file">
      <font color="#808080">
      home/<br>
      &nbsp;&nbsp;zf_info/<br>
      </font>
      &nbsp;&nbsp;&nbsp;&nbsp;monitor/
    </td>
    <td>Current and back issues of the Zebrafish Science Monitor.

  <tr>
    <td class="file">
      <font color="#808080">
      home/<br>
      &nbsp;&nbsp;zf_info/<br>
      </font>
      &nbsp;&nbsp;&nbsp;&nbsp;news/
    </td>
    <td>Contains current news about the ZFIN site, and 
      zebrafish related jobs and meetings.

  <tr>
    <td class="file">
      <font color="#808080">
      home/<br>
      &nbsp;&nbsp;zf_info/<br>
      </font>
      &nbsp;&nbsp;&nbsp;&nbsp;stckctr/
    </td>
    <td>
      Pages describing the Zebrafish International Resource Center, also 
      known as the stock center.
      These files are slowly being replaced by files on the ZIRC server.
    </td>
  </tr>
  <tr>
    <td class="file">
      <font color="#808080">
      home/<br>
      &nbsp;&nbsp;zf_info/<br>
      </font>
      &nbsp;&nbsp;&nbsp;&nbsp;zfbook/
    </td>
    <td>
      An online version of the University of Oregon Zebrafish Book, a 
      document describing the care and feeding of zebrafish and how to do
      zebrafish research.

  <tr>
    <td class="file">
      lib/
    </td>
    <td>
      Contains libraries, object or class files that run on the server.

  <tr>
    <td class="file">
      <a name="lib/DB_functions">
      <font color="#808080">
      lib/<br>
      </font>
      &nbsp;&nbsp;DB_functions/</a>
    </td>
    <td>
      Defines functions that are loaded into the database and are callable 
      from SQL.  The functions are divided into subdirectory based on 
      source language (zextend is the exception) rather than on a functional
      basis because of makefile issues.  The source language of the 
      function (either C or SPL) has a great deal of impact on the 
      makefile.  It was much easier to group them by language then deal 
      with multiple languages in one makefile.  See the 
      <a href="db.html#Database Functions">Database Functions</a> section
      for a description of each of the functions.
  <tr>
    <td class="file">
      <a name="lib/DB_functions/C">
      <font color="#808080">
      lib/<br>
      &nbsp;&nbsp;DB_functions/<br>
      </font>
      &nbsp;&nbsp;&nbsp;&nbsp;C/</a>
    </td>
    <td>
      ZFIN has several database functions written in C.  Functions that 
      have been around since before 2001 are defined in the zextend 
      directory.  Functions that are more recent or that have been 
      significantly modified are in this directory.  This directory 
      offers a cleaner implementation than zextend.  In zextend, all of
      the functions are defined in a single file, and dropping and 
      creating functions involves dropping and creating all of the functions
      in the one file.  In the C directory each function has its own file.

  <tr>
    <td class="file">
      <a name="lib/DB_functions/SPL">
      <font color="#808080">
      lib/<br>
      &nbsp;&nbsp;DB_functions/<br>
      </font>
      &nbsp;&nbsp;&nbsp;&nbsp;SPL/</a>
    </td>
    <td>
      Many of ZFIN's database functions are written in SPL, the Informix
      Stored Procedure Language.  ZFIN's SPL routines are defined in this
      directory, including the 
      <a href="db.html#Regen Functions">regen functions</a>.

  <tr>
    <td class="file">
      <a name="lib/DB_functions/zextend">
      <font color="#808080">
      lib/<br>
      &nbsp;&nbsp;DB_functions/<br>
      </font>
      &nbsp;&nbsp;&nbsp;&nbsp;zextend/</a>
    </td>
    <td>
      This directory contains database functions that were written in C,
      before 2001.  C functions added since then are defined in the C 
      directory, where each function gets its own file.  In zextend, all of
      the functions are defined in a single file, and dropping and 
      creating functions involves dropping and creating all of the functions
      in the one file.  See the directory's Makefile for details.

  <tr>
    <td class="file">
      <a name="lib/DB_triggers">
      <font color="#808080">
      lib/<br>
      </font>
      &nbsp;&nbsp;DB_triggers/</a>
    </td>
    <td>
      Defines all the triggers in the database.
    </td>
  </tr>
  <tr>
    <td class="file">
      server_apps/
    </td>
    <td>Applications that run on the server side, where we need 
      a separate instance of the application for each database/web site.

  <tr>
    <td class="file">
      <font color="#808080">
      server_apps/<br>
      </font>
      &nbsp;&nbsp;cron/
    </td>
    <td>
      Defines the ZFIN <a href="admin.html#cron">crontab</a> file and
      a means for starting it and stopping it.
    </td>
  </tr>
  <tr>
    <td class="file">
      <font color="#808080">
      server_apps/<br>
      </font>
      &nbsp;&nbsp;DB_maintenance/
    </td>
    <td>
      Database maintenance scripts, including those related to 
      <a href="admin.html#Backups">backups</a>.
    </td>
  </tr>
  <tr>
    <td class="file">
      <a name="server_apps/sysexecs">
      <font color="#808080">
      server_apps/<br>
      </font>
      &nbsp;&nbsp;sysexecs/</a>
    </td>
    <td>
      The name of this directory comes from the 
      <a href="db.html#Infrastructure Functions"><kbd>sysexec()</kbd></a>
      database function
      (defined under 
      <a href="#lib/DB_functions"><span class="file">lib/DB_functions</span></a>).  
      <kbd>sysexec()</kbd> is a C function that is used to call 
      executables/scripts on 
      the server from 
      within SQL in app pages.  This directory contains/defines executables
      that are called using the <kbd>sysexec()</kbd> function.

  <tr>
    <td class="file">
      <a name="#server_apps/WebSiteTools">
      <font color="#808080">
      server_apps/<br>
      </font>
      &nbsp;&nbsp;WebSiteTools/</a>
    </td>
    <td>
      Contains web site management tools.  It contains the 
      <a href="#checklinks">checklinks</a> link validity checker,
      and the signs of life script that runs every few minutes.

</table>


<!-- ========= ZFIN CENTRAL ============================================ -->

<h2 class="zdoc"><a name="ZFIN Central">ZFIN Central</a></h2>

<p>ZFIN Central is the home of all ZFIN source code, documentation, test data,
development executables and scripts, and all of the test web sites.  It 
contains
almost everything you need to do develop and test a ZFIN web site.
ZFIN Central does not contain the production web site or the actual test 
or production data or Informix executables.

<p>ZFIN Central is at:

<div class="fileline">/research/zcentral</div>

<p>See the <a href="admin.html#Development Server Disk">Disk Usage</a>
section for where ZFIN Central is physically located.

<table class="definition">
  <caption>ZFIN Central Subdirectories</caption>
  <tr>
    <td class="fileterm">
      /research/zcentral/loadUp
    </td>
    <td>
      ZFIN development loadUp directories are stored here.
    </td>
  </tr>
  <tr>
    <td class="fileterm">
     /research/zcentral/Commons
    </td>
    <td>
      Development documentation, environment, 
      and /bin scripts (see details below).
    </td>
  </tr>
  <tr>
    <td class="fileterm">
     /research/zcentral/data
    </td>
    <td>
    </td>
  </tr>
  <tr>
    <td class="fileterm">
     /research/zcentral/ftp
    </td>
    <td>
      ZFIN development ftp directories
    </td>
  </tr>
  <tr>
    <td class="fileterm">
     /research/zcentral/www_homes
    </td>
    <td>
      ZFIN development website target directories
    </td>
  </tr>
</table>

<!-- ========= ZFIN CENTRAL LOADUP ======================================= -->

<h3 class="zdoc"><a name="ZFIN Central LoadUp">
                          ZFIN Central LoadUp directory structure</a></h3>

<p>ZFIN Central LoadUpis at:

<div class="fileline">/research/zcentral/loadUp</div>

<p>Every night, files in the loadUp directories are checked against
the production loadUp repository.  If files exist in /research/zcentral/loadUp
directories and not in /research/zprod/loadUp directories, those files
are deleted (a backup copy is tagged with 'loadupbkup' and stored in /tmp)
from the /research/zcentral/loadUp directories.  
If files are missing from /research/zcentral/loadUp 
that exist on /research/zprod/loadUp, those files are copied to development.
Only files that have changed (rsync checks time/date stamp and filename)
between production and development are updated (see rsync man 
pages for details).  

<p>Each of these subdirectories contains a /bkup directory.  
All directories in /loadUp/ and including /loadUp are owned by 
zfishweb:www.  Zfishweb must own these directories,
else the cgi that does the loading will fail.  See the 
<a href="admin.html#Apache">Apache configuration</a> for more details.

<table class="definition">
  <caption>ZFIN Central LoadUp Subdirectories</caption>
  <tr>
    <td class="fileterm">
      /research/zcentral/loadUp/imageLoadUp
    </td>
    <td>
      ZFIN development images stored here.
    </td>
  </tr>
  <tr>
    <td class="fileterm">
     /research/zcentral/loadUp/PDFLoadUp
    </td>
    <td>
      ZFIN development PDF files stored here.
    </td>
  </tr>
  <tr>
    <td class="fileterm">
     /research/zcentral/loadUp/embryonixLoadUp
    </td>
    <td>
     zfin.org on embryonix pdf/image files stored here
    </td>
  </tr>
</table>

<!-- ========= ZFIN COMMONS ============================================ -->

<h2 class="zdoc"><a name="ZFIN Commons">ZFIN Commons</a></h2>

<p>The ZFIN Commons directory hierarchy contains
files (e.g., scripts and documentation) that are used by the ZFIN team to
build and manage ZFIN web sites and databases.  (Note that these files are 
not used at runtime.  The web site can be up and running if this directory
is unavailable.)

<table class="definition">
  <caption>ZFIN Commons Subdirectories</caption>
  <tr>
    <td class="fileterm">
      <a href="http://cvs.zfin.org/cvs/cvsweb.cgi/Commons/bin/">bin/</a>
    </td>
    <td> 
      Binaries and scripts that are used to set up test web sites or the
      production web site, or that are otherwise useful for web site 
      development.
    </td>
  </tr>
  <tr>
    <td class="fileterm">
      <a href="http://cvs.zfin.org/cvs/cvsweb.cgi/Commons/doc/">doc/</a>
    </td>
    <td>
      ZFIN documentation, including this document.
    </td>
  </tr>
  <tr>
    <td class="fileterm">
      <a href="http://cvs.zfin.org/cvs/cvsweb.cgi/Commons/env/">env/</a>
    </td>
    <td>
      Scripts and data files that are useful for setting up development 
      environments.
    </td>
  </tr>
</table>

<p>The <span class="file">bin</span> directory is placed in your 
$PATH when you source the 
<a href="#.env Files">.env file</a> for your 
<a href="#ZFIN Development Environments">ZFIN development environment</a>.  
(<i>Do <b>not</b> add the bin directory to your $PATH in your .cshrc file.
If the filesystem containing the ZFIN Commons is down, then you won't be able 
to log in.</i>)

<p>The Commons directory and its subdirectories are managed as the
<a href="http://cvs.zfin.org/cvs/cvsweb.cgi/Commons/">Commons</a> 
<a href="#Source Code Control and CVS">CVS project</a>.  There is a copy of
the Commons directory on both the <a href="#Production Server">production</a>
and <a href="#Development Server">development</a> servers. On both servers, the

<div class="fileline">
<a href="#/private/ZfinLinks">/private/ZfinLinks/Commons</a>
</div>

<p>sym link points at the local copy of the Commons directory.  Having a Commons
directory on both servers ensures that the production web site can still be
built, even if the development server is down.

<p>See <a href="#Updating Files in the ZFIN Commons">
Updating Files in the ZFIN Commons</a> for details on how to update files
in the Commons directory.

<p>See the <a href="admin.html#Disk Usage">Disk Usage</a>
section for where the 2 copies of ZFIN Commons are physically located.



<!-- ========= ZFIN PROD =============================================== -->

<h2 class="zdoc"><a name="ZFIN Prod">ZFIN Prod</a></h2>

<p>The directory

<div class="fileline">/research/zprod</div>

<p>is the <a href="intro.html">production web site</a> filesystem when the
production web site is running on the 
<a href="#Production Server">production server</a>.

<p>See the <a href="admin.html#Production Server Disk">Disk Usage</a>
section for where ZFIN Prod is physically located.


<!-- ========= ZFIN PROD LOADUP ======================================= -->

<h2 class="zdoc"><a name="ZFIN Prod LoadUp">
                          ZFIN Prod LoadUp directory structure</a></h2>

<p>ZFIN Prod LoadUp is at:

<div class="fileline">/research/zprod/loadUp</div>

<p>Each of these subdirectories contains a /bkup directory.  
All directories in /loadUp/ and including /loadUp are owned by 
zfishweb:www.  Zfishweb must own these directories,
else the cgi that does the loading will fail.  See the 
<a href="admin.html#Apache">Apache configuration</a> for more details.


<table class="definition">
  <caption>ZFIN Prod LoadUp Subdirectories</caption>
  <tr>
    <td class="fileterm">
      /research/zprod/loadUp/imageLoadUp
    </td>
    <td>
      ZFIN production images stored here.
    </td>
  </tr>
  <tr>
    <td class="fileterm">
     /research/zprod/loadUp/PDFLoadUp
    </td>
    <td>
      ZFIN production PDF files stored here.
    </td>
  </tr>
</table>

<p> Production copies of images and PDFs are stored in these directories.

<!-- ========= ZFIN loadup naming conventions  =========================== -->

<h3 class="zdoc"><a name="ZFIN LoadUp File Naming Conventions">
                          ZFIN LoadUp File Naming Conventions</a></h3>

<p>Files are named by their home-table ZDB-id 
in /research/zprod/loadUp/imageLoadUp and /research/zprod/loadUp/PDFLoadUp.
In addition, the type of file (for images) is specified with 
For example, images from fish_image table are named like:<br><br>

<i>ZDB-IMAGE-010101-1.jpg</i> (for images)<br>
<i>ZDB-IMAGE-010101-1_thumb.jpg</i> (for thumbnails)<br>
<i>ZDB-IMAGE-010101-1_annot.jpg</i> (for images with annotation)<br>


<!-- ========= ZFIN USERS ============================================== -->

<h2 class="zdoc"><a name="ZFIN Users">ZFIN Users</a></h2>

<p>The directory

<div class="fileline">/research/zusers</div>

<p>has a subdirectory for each ZFIN developer.  In general, 
developers do their ZFIN related work in these subdirectories.  The source
tress for the web sites that a developer is working on are generally found
under their users directory.  There are several subdirectories under 
users that do not correspond to particular ZFIN developers.

<table class="definition">
  <caption>ZFIN Users Special Subdirectories</caption>
  <tr>
    <td class="fileterm">almost</td>
    <td>Holds the source tree for the 
      <a href="intro.html#Pre-Production and Sandbox Site">preproduction/sandbox</a>
      site.  
    </td>
  </tr>
  <tr>
    <td class="fileterm">bionix</td>
    <td>Holds the source tree for the bionix.cs.uoregon.edu web site.  This 
      site is only used when testing software upgrades on 
      <a href="#Upgrade Server">bionix</a>.
    </td>
  </tr>
  <tr>
    <td class="fileterm">embryonix</td>
    <td>Holds the source tree for the 
      <a href="intro.html#Production Site">zfin.org web site</a>, <i>when it 
      resides on the <a href="#Development Server">development server</a>, 
       which is rarely</i>.
    </td>
  </tr>
  <tr>
    <td class="fileterm">helix</td>
    <td>This is a symbolic link to 
      <kbd><a href="#ZFIN Prod">/research/zprod/</a>users/zfin.org</kbd>,
      which is the source tree for the 
      <a href="intro.html#Production Site">zfin.org web site</a> when it resides
      on the <a href="#Production Server">production server</a>, which is most 
      of the time.  
    </td>
  </tr>
  <tr>
    <td class="fileterm">mirror</td>
    <td>This is a symbolic link to 
      <kbd><a href="#ZFIN Prod">/research/zprod/</a>users/mirror</kbd>,
      which is the source tree for the 
      <a href="intro.html#Mirror Sites">internal ZFIN mirror site</a>,
      <a href="http://mirror.zfin.org">mirror.zfin.org</a>.  However,
      when the <a href="intro.html#Production Site">zfin.org</a> web site
      resides on the <a href="#Development Server">development server</a>,
      so does mirror.zfin.org.  When that happens, this is not a sym link,
      but is rather the source directory for mirror.zfin.org.
      See the <a href="#Mirror Sites">Mirror Sites</a> section for more details.
    </td>
  </tr>
</table>


<p>See the <a href="admin.html#Development Server Disk">Disk Usage</a>
section for where ZFIN Users is physically located.



<!-- ========= ZFIN UNLOADS ============================================ -->

<h2 class="zdoc"><a name="ZFIN Unloads">ZFIN Unloads</a></h2>

<p>The directory

<div class="fileline">/research/zunloads</div>

<p>stores recent unloads of the production database that were created with 
the <a href="#Loading Databases">unloaddb.pl</a> script.  As of 2004/11 we
can fit about 2 1/2 months of dumps in this filesystem before running out of
space.  We move the last dump of each month from this directory to the
<a href="#ZFIN Archive">ZFIN Archive filesystem</a> for long term storage.

<p>See the <a href="admin.html#Development Server Disk">Disk Usage</a>
section for where ZFIN Unloads is physically located.



<!-- ========= ZFIN ARCHIVE ============================================ -->

<h2 class="zdoc"><a name="ZFIN Archive">ZFIN Archive</a></h2>

<p>The ZFIN Archive  contains archival files that are no longer
actively used in ZFIN, but that we can't quite bring ourselves to delete.
It also contains long term <a href="admin.html#Backups">backup data</a>, and
past <a href="admin.html#Apache">Apache logs</a> for the production web site.  
The ZFIN Archive is spread across
several filesystems.  However it is all available, either directly or through
sym links, from

<div class="fileline">/research/zarchive0</div>

<p>Here is a list of some important directories in the ZFIN Archive.  
Most of these directories 
have a README file that explains the contents of the directories.
<br>

<table class="definition">
  <caption>ZFIN Archive Subdirectories</caption>
  <tr>
    <td class="fileterm">History/<br>
      &nbsp;&nbsp;1996-2000-IllustraRunning
    </td>
    <td>Archival files from the early years of ZFIN when it was running on 
      <a href="#Past Servers">zfishstix</a> and using the 
      <a href="db.html#History">Illustra</a> DBMS.  
      These files reflect the state of the site before it was ported to
      Informix and moved to <a href="#Past Servers">chromix</a>.
    </td>
  </tr>
  <tr>
    <td class="fileterm">History/<br>
      &nbsp;&nbsp;2000-InformixPort/
    </td>
    <td>Archival files from the port of the ZFIN web site from Illustra
      to Informix and simultaneously move it from 
      <a href="#Past Servers">zfishstix to chromix</a>.  
      This port happened on 2000/03/15,
      although it took a year of preparation prior to that.
    </td>
  </tr>
  <tr>
    <td class="fileterm">History/<br>
      &nbsp;&nbsp;2000-2001-InformixRunning/
    </td>
    <td>Archival files from the first 10 months of ZFIN running under 
      Informix on <a href="#Past Servers">Chromix</a>.  
      This also contains files that were used on 
      <a href="#Past Servers">genetix</a>.
    </td>
  </tr>
  <tr>
    <td class="fileterm">History/<br>
      &nbsp;&nbsp;2001-InfrastructurePort/
    </td>
    <td>In late 2000 and early 2001 all of the files that make up the web site
      were moved under a single <a href="#Source Code Control and CVS">CVS</a>
      project, and 
      <a href="#Makefiles and the ZFIN_WWW Source Tree">makefiles</a>
      were added to create the entire web site.  This process started on 
      2000/12/19 when the files were put under CVS and the new test server, 
      <a href="#Upgrade Server">bionix</a>, was brought up.  It was finished 
      on 2001/03/30 when the production server was upgraded from Solaris 7
      and Informix 9.20 to Solaris 8 and Informix 9.21.
    </td>
  </tr>
  <tr>
    <td class="fileterm">History/<br>
      &nbsp;&nbsp;2001-InfrastructureRunning/
    </td>
    <td>Archival files from when the website was running using the new
      CVS/Makefile infrastructure.
    </td>
  </tr>
  <tr>
    <td class="fileterm">databases/</td>
    <td>Contains historical dumps of databases, mostly from production.
      The dumps were done with the 
      <a href="#Loading Databases"><kbd>unloaddb.pl</kbd></a> script.
    </td>
  </tr>
  <tr>
    <td class="fileterm">Logs/</td>
    <td>Contains historical logs, mostly Apache logs from production.  Tom.</td>
  </tr>
  <tr>
    <td class="fileterm">users/</td>
    <td>This directory has subdirectories for each ZFIN developer.  These 
      subdirectories contain files that the developers want to archive.
    </td>
  </tr>
</table>

<p>See the <a href="admin.html#Development Server Disk">Disk Usage</a>
section for where ZFIN Archive is physically located.



<!-- ========= /PRIVATE ================================================= -->

<h2 class="zdoc"><a name="/private">/private</a></h2>

<p>Each ZFIN server machine has its own locally mounted 
<span class="file">/private</span> directory
that contains "system" executables that are needed on ZFIN servers, 
but that are not part of the standard system installation.  This includes
things like 
<a href="admin.html#Apache">Apache</a> and 
<a href="admin.html#Database Administration">Informix</a>.  See the 
<a href="admin.html">System, Web Site, and Database Administration</a>
document for more details on <span class="file">/private</span>.




<h3 class="zdoc"><a name="/private/ZfinLinks">/private/ZfinLinks</a></h3>

<p>The <span class="file">/private/ZfinLinks</span> directory exists on 
both the 
<a href="#Production Server">production</a> and 
<a href="#Development Server">development</a> servers.  It contains symbolic
links that point to important ZFIN directories that are either different on
the two servers, or that switch when we 
<a href="admin.html#Moving zfin.org to a Different Machine">move zfin.org
from one machine to another</a>.  The sym links in this directory allow us
to write generic scripts that will run the same no matter what server they
are run on, and no matter where zfin.org resides.  There is a README file
in this directory that explains the purpose of each sym link, and
what the value of each symlink is when zfin.org is on each server.



<!-- ========= DATABASES =============================================== -->

<h2 class="zdoc"><a name="Databases">Databases</a></h2>

<p>Each Informix engine stores its databases in a collection of raw disk 
partitions that it owns on the machine it is running on.  You do not need
to know where these partitions are located in order to use Informix.  See the 
<a href="admin.html#Disk Usage">Disk Usage</a>
section for details on where the partitions actually are.



<!-- =================================================================== -->
<!-- ========= WEB SITE / DATABASE / MACHINE MATRIX ==================== -->
<!-- =================================================================== -->

<h1 class="zdoc"><a name="Web Site / Database / Machine Matrix">
             Web Site / Database / Machine Matrix</a></h1>

<p>This section describes which machines, Informix engines, and databases
support which web sites.
<br><br>
<table width="100%" border="0">
 <tr>
  <td width="45%" align="right"><a name="Production DB Matrix"><b>Machine:</b></a></td>
  <td><a href="#Production Server"><b>helix</b></a></td>
 </tr>
 <tr>
  <td width="45%" align="right"><b>Informix Engine:</b></td>
  <td><b>wildtype</b></td>
 </tr>
 <tr>
  <td width="45%" align="right"><b>C Shell Environment File:</b></td>
  <td class="file"><a href="http://cvs.zfin.org/cvs/cvsweb.cgi/Commons/env/wildtype">/research/zcentral/Commons/env/wildtype</a></td>
 </tr>
 <tr>
  <td width="45%" align="right"><b>Web Site Directory:</b></td>
  <td class="file"><a href="#ZFIN Prod">/research/zprod/</a>www_homes/{zfin.org,mirror}</td>
 </tr>
</table>

<br>
<table class="zdoc">
 <tr>
  <th>Domain Name</th>
  <th>Database</th>
  <th>Purpose</th>
 </tr>
 <tr>
  <td><a href="http://zfin.org">zfin.org</a>
  <td>zfindb
  <td><a href="intro.html#Production Site">Production web site</a> and database.
 </tr>
 <tr>
  <td><a href="http://mirror.zfin.org">mirror.zfin.org</a></td>
  <td><i>none</i></td>
  <td>Used to test and support our mirror web sites.  There is no
      database associated with this web site.  The mirrors redirect all 
      database access to the production web site.  Our external mirror
      sites are copies of this web site.  See the 
      <a href="#Mirror Sites">Mirror Sites</a> 
      sections for details.
  </td>
 </tr>
</table>


<br><br>
<table width="100%" border="0">
 <tr>
  <td width="45%" align="right"><a name="Development DB Matrix"><b>Machine:</b></a></td>
  <td><b><a href="#Development Server">embryonix</a></b></td>
 </tr>
 <tr>
  <td width="45%" align="right"><b>Informix Engine:</b></td>
  <td><b><a href="#Development Server">wanda</a></b></td>
 </tr>
 <tr>
  <td width="45%" align="right"><b>C Shell Environment File:</b></td>
  <td class="file"><a href="http://cvs.zfin.org/cvs/cvsweb.cgi/Commons/env/wanda">/research/zcentral/Commons/env/wanda</a></td>
 </tr>
 <tr>
  <td width="45%" align="right"><b>Web Site Directory:</b></td>
  <td class="file"><a href="#ZFIN Central">/research/zcentral/</a>www_homes/<i>VirtHostName</i></td>
 </tr>
</table>

<br>
<table class="zdoc">
 <tr>
  <th>Domain Name</th>
  <th>Database</th>
  <th>Production Web Sites</th>
 </tr>
 <tr>
  <td><a href="http://zfin.org">zfin.org</a></td>
  <td>zfindb</td>
  <td>This is the <a href="intro.html#Production Site">production site</a>
      on those rare occassions when the production site is running on the 
      <a href="#Development Server">development server</a>.  This is reachable
      as <a href="http://embryonix.cs.uoregon.edu">embryonix.cs.uoreon.edu</a>
      shortly before and after 
      <a href="admin.html#Moving zfin.org to a Different Machine">moving 
      zfin.org to or from embryonix</a>.  The filesystem and database
      backing this web site exist during these rare times.
  </td>
 </tr>
 <tr>
  <td><a href="http://mirror.zfin.org">mirror.zfin.org</a></td>
  <td><i>none</i></td>
  <td>This is the <a href="#Mirror Sites">internal ZFIN mirror site</a>
      on those rare occassions when the 
      <a href="intro.html#Production Site">production site</a> is running on the
      <a href="#Development Server">development server</a>. 
  </td>
 </tr>
 <tr>
  <th>Domain Name</th>
  <th>Database</th>
  <th>Preproduction Web Site</th>
 </tr>
 <tr>
  <td><a href="http://almost.zfin.org">almost.zfin.org</a></td>
  <td>almdb</td>
  <td>This is the <a href="intro.html#Pre-Production and Sandbox Site">
      preproduction and sandbox site</a> used as a final test platform for 
      changes before they go into  production.  This also allows 
      developers and curators to interact with a ZFIN database and/or 
      web site without impacting the production server.
  </td>
 </tr>
 <tr>
  <th>Domain Name</th>
  <th>Database</th>
  <th>Development Web Sites</th>
 </tr>
 <tr>
  <td><a href="http://albino.zfin.org">albino.zfin.org</a>
  <td>clemdb
  <td><a name="albino">Development web site, used by <b>Dave Clements</b></a>.
 </tr>
 <tr>
  <td><a href="http://beaky.zfin.org">beaky.zfin.org</a>
  <td>tomdb
  <td>Development web site, used by <b>Tom Conlin</b>.
 </tr>
 <tr>
  <td><a href="http://coral.zfin.org">coral.zfin.org</a>
  <td>kevdb
  <td>Development web site, used by <b>Kevin Schaper</b>.
 </tr>
 <tr>
  <td><a href="http://dino.zfin.org">dino.zfin.org</a>
  <td>judydb
  <td>Development web site, used by <b>Judy Sprague</b>.
 </tr>
 <tr>
  <td><a href="http://edison.zfin.org">edison.zfin.org</a>
  <td>yoldb
  <td>Development web site, used by <b>Prita Mani</b> 
 </tr>
 <tr>
  <td><a href="http://frost.zfin.org">frost.zfin.org</a>
  <td><i>none</i>
  <td>Unused.
 </tr>
 <tr>
  <td><a href="http://gorp.zfin.org">gorp.zfin.org</a>
  <td>brockdb
  <td>Development web site, used by <b>Brock Sprunger</b>.
 </tr>
 <tr>
  <td><a href="http://hoover.zfin.org">hoover.zfin.org</a>
  <td>hoovdb
  <td>Development web site, used by <b>Sierra Taylor</b>.
 </tr>
 <tr>
  <td><a href="http://iguana.zfin.org">iguana.zfin.org</a>
  <td>iguadb
  <td>Development web site, used by <b>Kevin Schaper</b>.
 </tr>
 <tr>
  <td><a href="http://junior.zfin.org">junior.zfin.org</a>
  <td>jrdb
  <td>Development web site, used by <b>Peiran Song</b>.
 </tr>
 <tr>
  <td><a href="http://kirby.zfin.org">kirby.zfin.org</a>
  <td>kirbdb
  <td>Development web site, used by <b>Judy Sprague</b>.
 </tr>
 <tr>
  <td><a href="http://lucky.zfin.org">lucky.zfin.org</a>
  <td>luckdb
  <td>Development web site, used by <b>Brock Sprunger</b>.
 </tr>
 <tr>
  <td><a href="http://manx.zfin.org">manx.zfin.org</a>
  <td>mnxdb
  <td>Development web site, used by <b>Tim Mason</b> for updates related to ZIRC
 </tr>
 <tr>
  <td><a href="http://manx.zfin.org">nagel.zfin.org</a>
  <td>nagdb
  <td>Development web site, used by <b>Ron Holland</b> for updates related 
    to ZIRC.
 </tr>
 <tr>
  <td><a href="http://ogon.zfin.org">ogon.zfin.org</a>
  <td>ogodb
  <td>Development web site, used by <b>Peiran Song</b>.
 </tr>
 <tr>
  <td><a href="http://ogon.zfin.org">polka.zfin.org</a>
  <td>plkdb
  <td>Development web site, used by <b>Prita Mani</b>.
 </tr>
 <tr>
  <td><a href="http://ogon.zfin.org">quark.zfin.org</a>
  <td>quadb
  <td>Development web site, used by <b>Paea LePendu</b>.
 </tr>
 <tr>
  <td><a href="http://runzel.zfin.org">runzel.zfin.org</a>
  <td>ruzdb
  <td>Development web site, used by <b>Sherry Giglia</b>.
 </tr>
 <tr>
  <td><a href="http://swirl.zfin.org">swirl.zfin.org</a>
  <td>swrdb
  <td>Development web site, used by <b>Sierra Taylor</b>.
 </tr>
 <tr>
  <td><a href="http://tango.zfin.org">tango.zfin.org</a>
  <td>tandb
  <td>Development web site, used by <b>Paea LePendu</b>.
 </tr>
 <tr>
  <td><a href="http://ukkie.zfin.org">ukkie.zfin.org</a></td>
  <td>ukidb</td>
  <td>Development web site, used by <b>Sergei Bogdanov</b> for updates related 
      to ZIRC.  
  </td>
 </tr>
 <tr>
  <td><a href="http://viper.zfin.org">viper.zfin.org</a></td>
  <td>vipdb</td>
  <td>Development web site, used by <b>Xiang Shao</b>.</td>
 </tr>
 <tr>
  <td><a href="http://whirly.zfin.org">whirly.zfin.org</a></td>
  <td>whrdb</td>
  <td>Development web site, used by <b>Xiang Shao</b>.</td>
 </tr>
 <tr>
  <td>
    <a href="http://xray.zfin.org">xray.zfin.org</a><br>
    <a href="http://yoyo.zfin.org">yoyo.zfin.org</a><br>
    <a href="http://zezem.zfin.org">zezem.zfin.org</a>
  </td>
  <td><i>none</i></td>
  <td>As of 2005/10 these are unassigned.</td>
 </tr>
 <tr>
  <td><a href="http://test.zfin.org">test.zfin.org</a>
  <td>testzfinorgdb
  <td>Used to do external beta testing of new ZFIN features.
 </tr>
</table>



<br>
<br>
<table width="100%" border="0">
 <tr>
  <td width="45%" align="right"><b>Machine:</b></td>
  <td><b><a href="#Upgrade Server">bionix</a></b></td>
 </tr>
 <tr>
  <td width="45%" align="right"><b>Informix Engine:</b></td>
  <td><b><a href="#Upgrade Server">wavy</a></b></td>
 </tr>
 <tr>
  <td width="45%" align="right"><b>C Shell Environment File:</b></td>
  <td class="file"><a href="http://cvs.zfin.org/cvs/cvsweb.cgi/Commons/env/wavy">/research/zcentral/Commons/env/wavy</a></td>
 </tr>
 <tr>
  <td width="45%" align="right"><b>Web Site Directory:</b>
  <td class="file">/research/zbionix1/www_homes/bionix
 </tr>
</table>

<br>
<table class="zdoc">
 <tr>
  <th>Domain Name</th>
  <th>Database</th>
  <th>Purpose</th>
 </tr>
 <tr>
  <td><a href="http://bionix.cs.uoregon.edu">bionix.cs.uoregon.edu</a></td>
  <td>biondb</td>
  <td><a name="bionix">
      Test web site used only to test software upgrades.  Most of the time
      this web site is not up and running.</a>
  </td>
 </tr>
</table>



<!-- =================================================================== -->
<!-- ========= ZFIN DEVELOPMENT ENVIRONMENTS =========================== -->
<!-- =================================================================== -->

<h1 class="zdoc"><a name="ZFIN Development Environments">ZFIN Development Environments</a></h1>

<p>A ZFIN development environment is a complete copy of the ZFIN web site and
ZFIN database.  Each environment has a source directory 
tree where development is 
done and a target directory tree, which is the web site itself and which is
produced by makefiles in the source tree.
Each development environment has its own URL.  
Each developer has one or more development environments, depending on the 
projects they are working on at the time.
Each development environment is tied to one 
<a href="#CVS and Large Projects">CVS branch</a>.

<!-- ========= ENVIRONMENT VARIABLES =================================== -->

<h2 class="zdoc"><a name="Environment Variables">Environment Variables</a></h2>

<p>A plethora of environment variables must be set to do ZFIN development.
These variables can be broken into two broad categories: 
<a href="#Informix Environment Variables">Informix variables</a>
and <a href="#Makefile Environment Variables">ZFIN Makefile variables</a>.  
Informix variables are those that need to
be defined in order to access an Informix database.  Makefile variables
are additional variables that must be set to use the ZFIN Makefiles and/or
to check files out of <a href="#Source Code Control and CVS">CVS</a>.



<h3 class="zdoc"><a name=".env Files">.env Files</a></h3>

<p>For each development environment, there exists a .env 
file which can be sourced to set all of the environment variables for
that environment.  The .env files are in:

<div class="fileline">
<a href="http://cvs.zfin.org/cvs/cvsweb.cgi/Commons/env/">/research/zcentral/Commons/env/<i>VirtHost</i>.env</a>
</div>

<p>These set the 
<a href="#CVS Commands">CVSROOT</a>, 
<a href="#Informix Environment Variables">Informix</a>, and 
<a href="#Makefile Environment Variables">Makefile</a> environment variables
that are needed for the particular environment.  The variables themselves
are discussed in the following sections.

<p>To set the all the environment variables in your shell, source the script
for the virtual host/development environment you want to use.  For example:

<div class="shellline">
  % source <a href="#ZFIN Commons">/private/ZfinLinks/Commons</a>/env/albino.env
</div>

<p>sets your environment variables for the 
<a href="#albino">albino</a> development environment on embryonix.  If
you get tired of typing all that all the time, you can set up an alias in 
your ~/.cshrc file:

<div class="shellline">
  alias srcalbino source <a href="#ZFIN Commons">/private/ZfinLinks/Commons</a>/env/albino.env
</div>

<p>and then at your shell prompt you could just type

<div class="shellline">
  % srcalbino
</div>

<p>to set up all your environment variables.



<h3 class="zdoc"><a name="Informix Environment Variables">
                          Informix Environment Variables</a></h3>

<p>Before you can access a database in any Informix engine, you must first 
set a number of Informix environment variables.  These variables must
be set:

<table class="definition">
  <caption>Key Informix Environment Variables</caption>
  <tr>
    <th>Variable</th><th>Description</th>
  </tr>
  <tr>
    <td class="term">INFORMIXDIR</td>
    <td>Absolute path of directory where Informix engine is 
        installed and is running.
    </td>
  </tr>
  <tr>
    <td class="term">INFORMIXSERVER</td>
    <td>Name of the Informix server/engine.</td>
  </tr>
  <tr>
    <td class="term">ONCONFIG</td>
    <td>Name of the onconfig file for the Informix engine.  This
        contains the configuration of the server.
    </td>
  </tr>
  <tr>
    <td class="term">INFORMIXSQLHOSTS</td>
    <td>Absolute path of the SQLHOSTS file for the Informix engine.  
        Used by Informix to route connections to the right engine.
    </td>
  </tr>
</table>

<p>In addition, you must also modify the settings of your PATH and 
LD_LIBRARY_PATH environment variables to include Informix binaries and 
libraries.
These are the only environment variables that are required, but there 
are several others that can affect how the server runs.  See the
<a href="#Informix Settings">Informix Settings</a> section for details.

<p>The <a href="#.env Files">.env files</a> described in the 
previous section set these variables
for you.  If you want to set <i>only</i> the Informix environment variables
then several C Shell scripts have been created to 
set these environment variables for you.  They are in

<div class="fileline">
<a href="http://cvs.zfin.org/cvs/cvsweb.cgi/Commons/env/">/private/ZfinLinks/Commons/env/<i>InformixServerName</i></a>
</div>

<p>To set the Informix environment variables in your shell, source the script
for the Informix engine you want to use.  For example:

<div class="shellline">
  % source <a href="#ZFIN Commons">/private/ZfinLinks/Commons</a>/env/wanda
</div>

<p>sets your environment variables for the wanda Informix engine on Embryonix.  If
you get tired of typing all that all the time, you can set up an alias in 
your <kbd>~/.cshrc</kbd> file:

<div class="shellline">
  alias srcwanda source <a href="#ZFIN Commons">/private/ZfinLinks/Commons</a>/env/wanda
</div>

<p>and then at your shell prompt you could just type

<div class="shellline">
  % srcwanda
</div>

<p>to set up your Informix environment variables.

<p>You must be logged on to the machine the Informix engine is on in order for
the environment variables to be effective.  The shell scripts also change your
prompt to let you know what Informix engine you are currently setup to use.



<h3 class="zdoc"><a name="Makefile Environment Variables">
                          Makefile Environment Variables</a></h3>

<p>These environment variables must be set
in addition to the Informix environment variables, in order to use the
ZFIN makefiles.

<table class="definition">
  <caption>Makefile Environment Variables</caption>
  <tr>
    <th>Variable</th>
    <th>Description</th>
  </tr>
  <tr>
    <td class="term">CVSROOT</td>
    <td>Tells <a href="#Source Code Control and CVS">CVS</a> where source 
        files are.  This should always be set to
        <span class="file"><a href="#ZFIN Central">/research/zcentral/</a>Vault/CVSroot</span>.
    </td>
  <tr>
    <td class="term">DBNAME</td>
    <td>Name of the database in 
        <a href="#Informix Environment Variables">$INFORMIXSERVER</a> 
	to work with.  
        Each developer works with their own database(s).  <i>This must 
        agree with the &lt;!--|DB_NAME|--&gt; value in the 
	<a href="#TRANSLATETABLE">translate table file</a></i>.
	See the <a href="#Web Site / Database / Machine Matrix">
	Web Site / Database / Machine Matrix</a> for which database goes with
	which web site.
    </td>
  </tr>
  <tr>
    <td class="term"><a name="TARGETROOT">TARGETROOT</a></td>
    <td>
        The directory where the makefiles put their output.  This identifies 
        the root of the target web site.  This is the 
	<a href="#Generic vs. Specific">specific version</a> of 
        the tree.  The makefiles reside in the 
	<a href="#Generic vs. Specific">generic version.</a> 
        <i>This must be an absolute path</i> and is always of the form 
        <span class="file"><a href="#ZFIN Central">/research/zcentral/</a>www_homes/<i>VirtualHostName</i></span>.
    </td>
  </tr>
  <tr>
    <td class="term">TARGETCGIBIN</td>
    <td>Name of the cgi-bin directory to use.  This name 
        is <i>relative</i> to
        $TARGETROOT and must agree with the &lt!--|CGI_BIN_CIR_NAME|--&gt; value
	in the <a href="#TRANSLATETABLE">translate table file</a>.  
	It typically has the form 
	<span class="file">cgi-bin_<i>VirtualHostName</i></span>.
    </td>
  </tr>
  <tr>
    <td class="term">TARGETFTPROOT</td>
    <td>Full path to the <a href="admin.html#FTP">FTP</a> directory for this 
        site.
        <i>This must be an absolute path and must agree with the 
	&lt;!--|FTP_ROOT|--&gt; value in the 
	<a href="#TRANSLATETABLE">translate table file</a>.</i>
    </td>
  </tr>
  <tr>
    <td class="term"><a name="TRANSLATETABLE">TRANSLATETABLE</a></td>
    <td>
        The file containing the translate table to use
        when converting <a href="#Generic vs. Specific">generic files into 
	their specific counterpart</a>. <i>This must be an absolute path.</i>
        A set of predefined translate tables are defined in the ZFIN Central 
	Commons directory with the names: 
        <a href="http://cvs.zfin.org/cvs/cvsweb.cgi/Commons/env/">
        <span class="file">/private/ZfinLinks/Commons/env/<i>VirtualHostName</i>.tt</span></a>
    </td>
  </tr>
</table>

<!-- ========= MAKEFILES AND THE ZFIN_WWWW SOURCE TREE ================= -->

<h2 class="zdoc"><a name="Makefiles and the ZFIN_WWW Source Tree">
             Makefiles and the ZFIN_WWW Source Tree</a></h2>

<p>The entire web site is produced by a single makefile hierarchy.  All web
site development is also done within that hierarchy.  The makefiles all 
conform to a particular look and feel.  Unfortunately, they are also 
complicated and you need to understand a fair amount about makefiles to 
modify them.  The makefiles are written for <span class="shell">gmake</span>
(<i>a.k.a., GNU Make</i>, a variant of the standard
Unix <span class="shell">make</span> utility.  If you have questions about 
<span class="shell">gmake</span>, see the 
<a href="Products/gnumake.ps"><i>GNU Make Manual</i></a>
or talk to Dave C.

<p>There is a lot of documentation in the makefiles.  If you have
questions about a particular file then the makefile that produces it is a
good place to start.

<p>The <a href="#Web Site Directory Structure">ZFIN web site source tree</a>
contains everything (almost) that goes into
making a ZFIN web site.  It is kept in 
<a href="#Source Code Control and CVS">CVS</a> 
under the ZFIN_WWW project.
Each developer gets their own copy of the source tree and 
<a href="#Updating a File, An Example">makes and tests changes in their copy 
of the tree before those changes are checked back into CVS</a>, 
<a href="#Putting Changes Into Production">tested on the 
preproduction site (almost.zfin.org), and then posted to 
production</a>.

<p>Each directory in the source tree also has its own makefile, and all the
makefiles cooperate to form one coherent makefile hierarchy. 



<h3 class="zdoc"><a name="Generic vs. Specific">Generic vs. Specific</a></h3>

<p>The makefiles reside in and get their source files from <i>generic</i>
or <i>source</i>
directories.  They put their output files in <i>specific</i> or <i>target</i>
directories.  
Files in the generic directory are, well, generic.  They have had all 
references to specific databases, directories, Informix engines, and 
machines replaced with equivalent generic tags.

<p>The specific/target directory approximately mirrors the generic/source
directory, but it contains specific versions of the files where the 
generic tags have been replaced with references to actual databases, 
directories, Informix engines, and machines.

<p>The specific/target directory tree is populated by the makefiles in the 
generic/source directory tree.  The makefiles use a <i>translate table file</i>
to know what specific values to replace the generic tags with.  Which file
to use for the translate table is determined by the 
<a href="#TRANSLATETABLE">$TRANSLATETABLE</a> environment variable.
Standard versions of translate table files for each ZFIN web site 
environment can be found at:

<div class="fileline">
<a href="http://cvs.zfin.org/cvs/cvsweb.cgi/Commons/env/">/private/ZfinLinks/Commons/env/<i>VirtualHostName</i>.tt</a>
</div>


<p>Generic tags have the form

<pre>
  &lt;!--|<i>generic_tag_name</i>|--&gt;
</pre>

<p>For example, <kbd>&lt;!--|DB_NAME|--&gt;</kbd> is the generic tag 
for a database name.

<p>The makefiles use the <kbd>makespecific.pl</kbd> script (which calls the 
<kbd>makespecificworker</kbd> script) to translate a file from 
its generic form to its specific form.  Typically, <kbd>makespecific.pl</kbd>
is called
so that its output file (the specific file) is placed directly in the final
output directory under the <a href="#TARGETROOT">$TARGETROOT</a> directory.
This is what happens with HTML and app page files.

<p>However, files such as C or Java source code files cannot go directly 
into the $TARGETROOT directory hierarchy.  In these cases a 
<a href="#Staging Directories">staging directory</a> is used.  



<h3 class="zdoc"><a name="Recursion">Recursion</a></h3>

<p>The makefiles form a recursive hierarchy.  Invoking <kbd>gmake</kbd> in 
any directory
invokes the makefile in that directory and the makefiles in all that 
directory's subdirectories.  This allows you to create an entire 
<a href="#Web Site Directory Structure">web site directory structure</a>
by typing a single command.  On the down side, it also
means that if you are changing something small in a high level directory 
then every time you run <kbd>gmake</kbd> 
you will have to wait for it to step through
all that directory's subdirectories (determining that there is nothing to do
in those subdirectories, which is quick, but still takes time).

<p>Makefile variables are not passed from parent to child makefiles.
 


<h3 class="zdoc"><a name="Targets">Targets</a></h3>

<p>Targets are what makefiles create.  They either correspond to real 
individual files, or to <i>phony</i> targets which may correspond to a 
set of files or to a particular task for the makefile to carry out.
You invoke a particular target by specifying it in the <kbd>gmake</kbd> 
command.  For example,

<div class="shellline">
  % gmake clean
</div>

<p>There are a standard set of targets that appear in all makefiles.
<br><br>

<table class="definition">
  <caption>Standard Makefile Targets</caption>
<tr>
  <th>Target<th>Description

<tr>
  <td class="term"><a name="all">all</a></td>
  <td>The first target in each makefile.  Therefore it is also the 
      default target.  Causes all the files in this directory and 
      its subdirectories to be made.  Put another way, this creates the output
      of the makefile and all its child makefiles.

<tr>
  <td class="term">clean</td>
  <td>Remove intermediate, temporary, and working files
      in this directory and in all subdirectories.  In many directories there
      are no local intermediate, temporary or working files to remove.

<tr>
  <td class="term">clobber</td>
  <td>Removes the target files produced by the makefiles,
      in this directory and in all subdirectories.
      It basically removes all files from target directories,
      but does not remove the target directories themselves.
      This sometimes also removes the things in your database that are put 
      there by the
      makefiles themselves.  This includes the app pages in the webpages table,
      the database functions defined in 
      <a href="#lib/DB_functions"><kbd>lib/DB_functions</kbd></a>,
      and the contents of the
      <a href="#server_apps/sysexecs">EXECWEB</a> table.

<tr>
  <td class="term">sanitycheck</td>
  <td>Performs a sanity check on files in the directory and
      all its subdirectories.  As of 2004/08, this
      <ul>
        <li>Runs the <a href="#weblint">Informix weblint</a> program 
	  against any app pages in the current directory tree.
      </ul>
      In the future it would be very useful if it also
      <ul>
        <li>Checked STATIC files for presence of 
	    <a href="#Generic vs. Specific">generic tags</a>
	</li>
        <li>Checked <a href="#Makefile Variables">STATIC & GENERIC files</a>
	    for presence of specific
            values that should be replaced with 
	    <a href="#Generic vs. Specific">generic tags</a>
	</li>
        <li>Checked app page source files against what is actually
            loaded into the database.
      </ul>

<tr>
  <td class="term">onetimeonly</td>
  <td>This target currently (2001/06) doesn't do anything.  
      However,
      at some point in the future it may be used to make system wide 
      changes to files
      in all directories in the makefile tree.  When the infrastructure was 
      created a similarly structured target was used to do the 
      initial translation of files from the 
      <a href="#Generic vs. Specific">specific form they had existed in
      to the generic form</a> that is now checked into CVS.
   </td>
</tr>
</table>

<p>In addition, there are several targets that occur in the top makefile and in
only a few other makefiles below it.  These do not propagate throughout the
whole tree but only to portions of it.


<table class="definition">
  <caption>Additional Makefile Targets</caption>
<tr>
  <th>Target<th>Description
<tr>
  <td class="term"><a name="mirrortarget">mirror</a></td>
  <td>
    Used to create a web page hierarchy that
    is then copied by the <a href="#Mirror Sites">mirror sites</a>.  
    This web page
    hierarchy contains only the static web pages.
    This target is present only in high level makefiles
    where some, but not all, of the subdirectories are
    going in to the mirror.

<tr>
  <td class="term"><a name="postloaddb">postloaddb</a></td>
  <td>
    This target should be invoked after calling 
    <a href="#loaddb.pl">loaddb.pl</a> to load a
    database into an already existing development environment. loaddb.pl 
    loads everything, or almost everything, into the DB.  This includes
    <ul>
      <li><a href="Database/DataModel/index.htm">WEBPAGES table</a>
      <li><a href="#sysexec, EXECWEB, and unix_commands">EXECWEB table</a>
      <li>all <a href="db.html#Database Functions">Database functions</a>
    </ul>
    <p>After a load all of these things effectively point
    back to the database that the data was originally 
    unloaded from, usually production.
    Making the postloaddb target causes all of these 
    things to be reloaded with definitions that are appropriate to 
    the local DB.  The <a href="#all">all</a> target
    must have been made previously.

<tr>
  <td class="term">start</td>
  <td>Start processes.  Starts up any processes that need
    to be running for the environment to work.  As of 2003/01, this 
    target doesn't actually start or stop anything.
    The database engine
    and apache are not controlled by this.  The <a href="#all">all</a> target
    must have been made previously.

<tr>
  <td class="term">stop</td>
  <td>Stop processes.  Stops any processes started by the 
    start target.  

</table>




<h3 class="zdoc"><a name="Makefile Variables">Makefile Variables</a></h3>

<p>Makefile variables are distinct from environment variables.  They are
declared inside makefiles and do not exist outside of them.
Unless extra steps are taken, makefile variables are not
passed from parent to child makefiles.  
The set of variables defined in each makefile depends on what the
makefile is doing. 

<p>There are makefile variable naming conventions,
and makefiles that do similar things have the same sets of variables.  In 
general, if a makefile has a need for one of the variables described below,
then the variable is given the name listed below.
<br><br>

<table class="definition">
  <caption>Makefile Variables</caption>
  <tr>
    <th>Name<th>Description
  </tr>
  <tr>
    <td class="term">TOP</td>
    <td>Relative path from this directory to the root of 
      the <a href="#Generic vs. Specific">generic</a> directory.  
      This is basically an 
      indicator of how deep in the tree the current 
      directory is.  Must be defined before make.include
      is included.  TOP is defined in every makefile.<br>
      Example: <br>
      <kbd>&nbsp;&nbsp;TOP = ../../..</kbd>
    </td>
  </tr>
  <tr>
    <td class="term">SUBDIRS
    <td>Subdirectories of the current directory that also contain makefiles.
    </td>
  </tr>
  <tr>
    <td class="term">TARGETDIR</td>
    <td>Identifies the directory within <a href="#TARGETROOT">$TARGETROOT</a>
      where
      the final output files produced by the makefile
      will go.  Has the form <kbd>$(TARGETROOT)/<i>subdirectory</i></kbd>.
      The subdirectory part of that usually mirrors the
      name of the subdirectory the makefile is in.
      <br>Example: 
      <br><kbd>&nbsp;&nbsp;TARGETDIR = $(TARGETROOT)/home</kbd>
    </td>
  </tr>
  <tr>
    <td class="term">GENERICS</td>
    <td>List of <a href="#Generic vs. Specific">generic files 
      that the makefile will translate into specific files.</a>
      <br>Example: 
      <br>&nbsp;&nbsp;<kbd>GENERICS = classify_pubs.apg do_direct.apg</kbd>
    </td>
  </tr>
  <tr>
    <td class="term">STATICS</td>
    <td>List of files that don't need to be translated from
      a generic form into a specific form.  These files 
      don't contain anything that needs translation.
      <br>Example: 
      <br><kbd>&nbsp;&nbsp;STATICS = fish_bgd.gif fish_net.gif</kbd>
    </td>
  </tr>
  <tr>
    <td class="term">SPECIFICTARGETS</td>
    <td>List of specific versions of generic files.  Depending
      upon the nature of the generic file, these may or 
      may not be the final target files.  For app pages
      and HTML files these are usually the final targets,
      but for Java and C files they are intermediate files.
      See discussion of <a href="#Staging Directories">staging directories</a>
      below.
    </td>
  </tr>
  <tr>
    <td class="term">STATICTARGETS</td>
    <td>List of targets that are based on static files.
    </td>
  </tr>
  <tr>
    <td class="term">ENDEMICTARGETS_PRE
      <br>ENDEMICTARGETS_POSTTARGETDIR
      <br>ENDEMICTARGETS_POSTTARGETS
      <br>ENDEMICTARGETS_POST
    </td>
    <td>These 4 variables are used with targets that require
      special handling in the local makefile.  These variables
      allow those makefiles to have special processing for
      these targets and still use the default rules for 
      other targets.  Which of these varaibles a target goes
      into determines when that target will be made in 
      relation to the default targets.  
    </td>
  </tr>
  <tr>
    <td class="term">TARGETS</td>
    <td>List of all targets produced by the makefile.  In other
      words, this is the list of final output files produced
      by the makefile.
      <br>Example: 
      <br><kbd>&nbsp;&nbsp;TARGETS = \
      <br>&nbsp;&nbsp;&nbsp;&nbsp;$(SPECIFICTARGETS) $(STATICTARGETS)</kbd>
    </td>
  </tr>
</table>

<p>Further documentation on all of these variables is provided in the
<a href="http://cvs.zfin.org/cvs/cvsweb.cgi/ZFIN_WWW/Makefile">makefile</a>
at the top of the ZFIN_WWW tree.

<p>If a makefile uses either of the 
<a href="#Makefile Include Files">default rules makefile include files</a>
then the makefile <i>must</i> use the standard variables names.



<h3 class="zdoc"><a name="Makefile Include Files">Makefile Include Files</a></h3>

<p>In the top directory of the source tree there are three files that can
be included in the makefiles:
<br><br>

<table class="zdoc">
<tr>
  <th>Include File <th> Purpose

<tr>
  <td>
    <a href="http://cvs.zfin.org/cvs/cvsweb.cgi/ZFIN_WWW/make.include">make.include</a>
  </td>
  <td>This file is included in <i>every</i> makefile immediately 
    after the
    TOP variable is defined.  It does several things:
    <ul>
      <li>Checks that <kbd>gmake</kbd> (or <kbd>gnumake</kbd>) 
        is being run and not <kbd>make</kbd>.  These 
        makefiles
	make use of several features that are not in the standard make.
      </li>
      <li>Checks that all needed environment variables are set.  If they aren't
        then the make is aborted.
      </li>
      <li>Defines which specific executables and options are to be used in the
        makefiles,  For example, this defines what command and options are to 
	be used to copy a file to its target directory, and where to find 
	<kbd>makespecific.pl</kbd>.
      </li>
    </ul>
    <p>Because this file is included in every makefile, these checks are 
    made and the variables are defined in every makefile.
  </td>
</tr>
<tr>
  <td>
    <a href="http://cvs.zfin.org/cvs/cvsweb.cgi/ZFIN_WWW/make.default.rules">make.defualt.rules</a>
  </td>
  <td>This defines a default set of targets, variables and rules 
    for directories
    that <i>do not</i> contain app pages.  Many directories that contain just 
    regular 
    HTML and/or image files can use the standard set of rules in this file.  If
    this is included then no rules generally need to be defined in the makefile
    including it.
  </td>
</tr>
<tr>
  <td>
    <a href="http://cvs.zfin.org/cvs/cvsweb.cgi/ZFIN_WWW/make.default.apg.rules">make.default.apg.rules</a>
  </td>
  <td>This defines a default set of targets, variables and rules 
    for directories
    that <i>do</i> contain app pages.  This is almost identical to 
    make.default.rules, but varies slightly in the rules.
  </td>
</tr>
</table>

<p>The use of the make.default files greatly reduces the length of 
makefiles.  About 90% of the makefiles include one of the two make.default
files.  This means that all the makefiles have a similar look and feel. 
Makefiles that don't use the default rules tend to be the more complicated 
ones that involve compiling code files.  It also makes system wide changes
to the makefiles much easier to make:  We only have to modify the 2 
make.default files and the 10% of the makefiles that don't use either of
them.  

<p>The make.default files use the 
<a href="#Makefile Variables">standard set of makefile variables</a>.
Any makefile that includes either of them must use the standard set of 
variables.  This forces most makefiles to have a similar look and feel.


<h3 class="zdoc"><a name="Staging Directories">Staging Directories</a></h3>

<p>Staging directories are needed when all of these conditions hold true:
<ul>
  <li>A makefile is producing an executable, object, class or similar such
      files from C, Java or other code files.
  <li>The C, Java, or other code files contain generic tags.
</ul>

<p>In such cases the process for producing the final file that goes into the
web site is at least a 2 step process involving at least 3 levels of files.
First, the 
<i>source file</i> is converted to its <i>specific version</i>, 
with all the tags being replaced with values for the target server.  
Then the <i>specific version</i> is
compiled into an <i>object or class file</i> which is then copied to 
the web site 
directory, either as a standalone file or as part of an archive or executable.
Staging directories are used to hold the files after the first step of this
process.  Staging directories hold intermediate versions of files that are
specific to a particular web site.

<p>All staging directories reside under the Staging subdirectory directly
under the <a href="#TARGETROOT">$TARGETROOT</a> directory.  Each source 
directory that needs a staging directory has its own subdirectory under 
$TARGETROOT/Staging.

<p>Originally, staging directories existed in the source tree.  This led to a
number of problems.  First, it made it very difficult/impossible to produce
multiple destination directories from one source directory.  We don't often
do this, but it is occassionally a handy ability to have (e.g. when producing
the <a href="intro.html#Beta Testing Site">beta test site</a>).  
Secondly, having
staging directories in the source tree led to lots of noise when you ran
<a href="#CVS update">cvs update</a>.  
All of the intermediate files would show up in the output and
you could easily loose the important stuff.



<!-- ========= SOURCE CODE CONTROL AND CVS ============================= -->

<h2 class="zdoc"><a name="Source Code Control and CVS">Source Code Control and CVS</a></h2>

<p>Most of what is used to produce the web site is kept under CVS, a source 
code/revision control system.  All ZFIN source code controlled files are 
kept under one CVS root:

<div class="fileline">
<a href="http://cvs.zfin.org/cvs/cvsweb.cgi/">/research/zcentral/Vault/CVSroot</a>
</div>

<p>The source code and web pages that go into
the site are stored in the ZFIN_WWW project of CVS.  Projects are the largest
logical grouping of files under CVS.  When a developer wants to create a
test web site they first check out the ZFIN_WWW project from CVS (typically
into somewhere under their 
<a href="#ZFIN Users"><kbd>/research/zusers</kbd></a> directory).

<p>The 
<a href="#ZFIN Commons">ZFIN Commons</a> 
is also under CVS control.  The 
ZFIN Commons directories are just checked out versions of the 
Commons CVS project.

<p>CVS is a big product.  It has lots of commands, options, and environment
variables that can be used to achieve various outcomes.  Most of them you 
will not need to know.  The 
<a href="Products/cvs.pdf"><i>CVS Manual</i></a> 
is over 180 pages long.  There is also a web site, 
<a href="https://www.cvshome.org">www.cvshome.org</a> that is dedicated to 
CVS.  Hopefully, this document will discuss most of what you need to know 
to use it.



<h3 class="zdoc"><a name="CVS Commands">CVS Commands</a></h3>

<p>Before you can use any CVS commands you must first set the CVSROOT 
environment variable to the directory that contains the ZFIN CVS projects:

<div class="shellline">
  % setenv CVSROOT <a href="#ZFIN Central">/research/zcentral/</a>Vault/CVSroot
</div>

<p>All of the <a href="#.env Files">.env files</a> set CVSROOT.  You can
also add the above line to your .cshrc file if you want.

<p>All CVS commands have the form 

<div class="shellline">
  cvs [ <a href="#CVS Global Options">global_options</a> ] command [ command_options ] [ command_args ]
</div>

<p>All CVS commands are recursive by default, although this can be 
<a href="#CVS Common Command Options">overrriden</a> for most commands.

<p>The table below lists the most useful commands, with their most 
useful options.  This document does not cover all 
CVS commands or options.  See the
<a href="Products/cvs.pdf"><i>CVS Manual</i></a> 
if this is not enough information for your needs.

<br><br>
<table class="definition">
 <caption>CVS Commands</caption>
 <tr>
  <th>Command</th>
  <th>Purpose</th>
 </tr>
 <tr>
  <td class="term">
    <a name="CVS add">
    cvs add <i>file_directory_list</i></a>
  </td>
  <td>
    Adds new files and/or directories to CVS.  Tells CVS that the next time you
    <a href="#CVS commit">commit</a>, this file or directory should be 
    added to the repository.  Adding a file or directory does not
    in and of itself cause the added item to show up in the repository.
    <br>Please talk to Dave C before adding any new directories.
  </td>
 </tr>
 <tr>
  <td class="term">
    <a name="CVS annotate">
    cvs annotate <i>files</i></a>
  </td>
  <td>
    Use the annotate command to find out in what revision each line in the 
    file was most recently modified.
    <br>See also <a href="#CVSweb">CVSweb</a>.
  </td>
 </tr>
 <tr>
  <td class="term">
    <a name="CVS checkout">cvs&nbsp;checkout&nbsp;[-P]&nbsp;<i>cvs_project</i></a>
  </td>
  <td>
    Called to create a copy of a CVS project in your local directory.  This 
    is generally called only once, when you first create your copy.  After that
    you use the <a href="#CVS update">update</a> command to update your source
    tree.
    Checkout should always be run in the directory where you want the 
    ZFIN_WWW directory to be created.
    See <a href="#CVS Common Command Options">CVS Common Command Options</a>
    below for an explanation of the -P option.
  </td>
 </tr>
 <tr>
  <td class="term">
   <a name="CVS commit">cvs commit [<i>files</i>]</a>
  </td>
  <td>
    Commits changes in your files to the CVS repository so that others can 
    see them.  If no files are specified then all of the changed files 
    in this directory and all subdirectories are committed to the repository.
    If files are specified then only those files are committed.  If others have
    updated any files in directories you are committing then CVS will not let
    you commit until you have gotten those updates from CVS using the 
    <a href="#CVS update">update</a> command.
    <br>Note that this does <i>not</i> in and of itself 
    <a href="#Putting Changes Into Production">propagate the changes to 
    the production web site</a>.  However, it does start this process.
  </td>
 </tr>
 <tr>
  <td class="term">cvs diff [<i>files</i>]</td>
  <td>
    Compares the files in your directories with the files currently stored 
    in CVS.  Useful for determining what changes you have made since the last 
    time you <a href="#CVS update">updated</a> your files.
    <br>See also <a href="#CVSweb">CVSweb</a>.  
  </td>
 </tr>
 <tr>
  <td class="term">cvs diff -r<i>rev1 file</i></td>
  <td>
    Compares a file in your directory with a specific revision in CVS.
    <br>See also <a href="#CVSweb">CVSweb</a>.  
  </td>
 </tr>
 <tr>
  <td class="term">cvs diff -r<i>rev1</i> -r<i>rev2 file</i>
  <td>
    Compares two different revisions of a file in CVS.
    <br>See also <a href="#CVSweb">CVSweb</a>.
  </td>
 </tr>
 <tr>
  <td class="term"><a name="CVS edit">cvs edit <i>files</i></a></td>
  <td>
    Tells CVS that you intend to update the given file(s).  CVS responds
    by registering that fact and giving you a writeable copy of the file(s).
  </td>
 </tr>
 <tr>
  <td class="term"><a name="CVS editors">cvs editors <i>files</i></a></td>
  <td>
    Lists the developers who are currently editing the given files.
  </td>
 </tr>
 <tr>
  <td class="term"><a name="cvsinfo">cvsinfo [<i>files</i>]</a></td>
  <td>
    List the latest and working revision numbers, and the CVS status of files
    in a compact format.
    <br><b>Note:</b>  This is not part of the standard CVS package.  It is a 
    locally written script (that is why the command is one word instead of
    two).  It does not operate recursively.
    <br>See also <a href="#CVSweb">CVSweb</a>.
  </td>
 </tr>
 <tr>
  <td class="term"><a name="CVS log">cvs log [<i>files</i>]</a></td>
  <td>
    List the update history of files.
    <br>See also <a href="#CVSweb">CVSweb</a>.
  </td>
 </tr>
 <tr>
  <td class="term">cvs remove <i>files</i></td>
  <td>This is the complement of the
    <a href="#CVS add">add</a> command.  It tells CVS that you want to remove
    the given files from CVS.  The files won't actually be removed until your
    next <a href="#CVS commit">commit</a> command.  (And even then CVS doesn't
    really remove the file, it just stops giving it to developers.)  You must
    first remove the file from your working directory before you can remove
    it from CVS.
  </td>
 </tr>
 <tr>
  <td class="term">cvs status [<i>files</i>]</td>
  <td>This displays information about the current status of files, such as what
    revision you have, if others have updated it since you got it, and whether
    or not you have updated it.  
    It is similar to but different from the <a href="#CVS log">log</a> command.
    The log command displays the update history of a file, but not its current 
    status.
    <br>See also <a href="#CVSweb">CVSweb</a>.
  </td>
 </tr>
 <tr>
  <td class="term"><a name="CVS unedit">cvs unedit <i>files</i></a></td>
  <td>
    You use the <a href="#CVS edit">edit</a> command to tell CVS to give
    you a writable version of a file so you can update it.  You then
    normally use the <a href="#CVS commit">commit</a> command to commit those
    changes to the CVS repository.  The unedit command is used in the cases
    where you decide that you want to discard the changes you have made, 
    rather than commit.  This unregisters you as an editor of the file in 
    CVS, and restores your file to its unedited state.
    <span class="shell"> 
    <br>% cvs unedit old_file.html
    </span>
 </tr>
 <tr>
  <td class="term">
    <a name="CVS update">
    cvs&nbsp;[-nq]&nbsp;update&nbsp;[-dP]&nbsp;[<i>files</i>]<br><br>
    cvs update -D &nbsp;[<i>date-of-version-you-want</i>] &nbsp;[<i>file-name</i>]<br><br>
    cvs update -p -r<i>rev</i> <i>file</i>
    </a>
  </td>
  <td>
    Updates your working directories so that they are current 
    with what is checked in to CVS.  This basically brings you up to date with 
    the changes that other developers have done.  If you have made changes to 
    a file in your directory, and you haven't committed those change yet, and
    another developer has committed changes to that file then CVS will 
    automatically merge your changes with those from the other developers.

    <p>Options 
    <table class="zdoc">
      <tr>
        <td valign="top">
          <b>-n</b>
        </td>
	<td valign="top">
	  See <a href="#CVS Global Options">CVS Global Options</a> below.
	</td>
      </tr>
      <tr>
        <td valign="top">
	  <b>-q</b>
	</td>
	<td valign="top">
	  See <a href="#CVS Global Options">CVS Global Options</a> below.
	</td>
      </tr>
      <tr>
        <td valign="top">
	  <b>-d</b>
	</td>
	<td valign="top">The update command will <i>not</i> by default pick up new
	  directories that others have added and that are not yet in your tree.
	  In order to get the update command to also pull in any
	  new directories that have been added by others you must specify the
	  <b>-d</b> option.
	</td>
	
	<tr>
        <td valign="top">
	  <b>-D</b>
	</td>
	<td valign="top">To get a previous version of a file as a working copy. See <a href="#example">example</a> below.
	</td>
	
	
      <tr>
        <td valign="top">
	  <b>-P</b>
	</td>
	<td valign="top">
	  See 
	  <a href="#CVS Common Command Options">CVS Common Command Options</a>
	  below.
	</td>
      </tr>
      <tr>
        <td valign="top">
	  <b>-p -r<i>rev</i></b>
	</td>
	<td valign="top">
	  The -p and -r options are used to pipe the specified revision of the 
	  file to STDOUT.  You can then redirect that to a file.  This is the 
	  only safe way to get a copy of a previous revision of a file without
	  setting a sticky tag for the file.  And trust me, you 
	  do <i>not</i> want to set the sticky tag for a file.
	</td>
      </tr> 
      <tr>
        <td valign="top">
	  <b>-A</b>
	</td>
	<td valign="top">
	(taken from <a href="Products/cvs.pdf"><i>CVS Manual</i></a>) 
	 "Sometimes a working copy's revision has extra data associated with 
	 it, for example, it might be on a branch, or restricted to versions
	 prior to a certain date.  Because this data persists -- that is, it
	 applies to subsequent commands in the working copy -- we refer to it
	 as sticky.  You can use the status command to see if any sticky
	 tags or dates are set: "cvs status [filename]". Sticky tags will 
	 remain on your working files until you delete them with 
	 cvs update -A.  The -A option retrieves the version of the file from 
	 the head of the trunk, and forgets any sticky tags, dates, or options.
	 Again, avoid sticky tags if at all possible."
	</td>
      </tr>
    </table>
  </td>
 </tr>
 <tr>
  <td class="term"><a name="CVS watch add">cvs watch add [<i>files</i>]</a></td>
  <td>
    If you are really concerned about changes to a particular file, 
    you can use this command to tell CVS to send you an e-mail whenever 
    anyone <a href="#CVS edit">edit</a>s, 
    <a href="#CVS unedit">unedit</a>s, or 
    <a href="#CVS commit">commit</a>s 
    the given file.
  </td>
 </tr>
</table>
<br>

<a name="example"><b>CVS update -D example:</b></a><br>
 <p>(assuming you're in your CVS folder--change the date to the
appropriate revision):

<p>% cvs update -D 2004/02/17 00:45:57 anatomy.obo
this makes *your copy* of anatomy.obo the one from 2004/02/17 00:45:57

<p>% cvs update -A anatomy.obo
clears this old version, and makes your copy the last-checked-in version
of anatomy.obo in you folder.

<p>This means the update statements above are over-writing your current
version (again, save or check in your version if you've made changes that
aren't checked in yet).

<p>Note: this makes your file a working copy, checkins of the previous
version/working version after reversion will result in checkins to CVS and
changes in production; be careful when doing this and be sure to ask your
collegues questions before proceeding!

<h4 class="zdoc"><a name="CVS Global Options">CVS Global Options</a></h4>

<p>There are a set of options that can be applied to all CVS commands
and have (more or less) the same meaning in all contexts.  These are called
global options in CVS.  They occur in the CVS command immediately after the 
cvs.

<div class="shellline">
  cvs [ global_options ] command [ command_options ] [ command_args ]
</div>

<p>This table lists some of the more useful CVS global options and how you 
might use them.
<br><br>
<table class="definition">
 <tr>
  <th>Option<th>Description
 </tr>
 <tr>
  <td class="term">-H</td>
  <td>Display usage information about the specified CVS command</td>
 </tr>
 <tr>
  <td class="term">-n</td>
  <td>From the 
    <a href="Products/cvs.pdf"><i>CVS Manual</i></a>:
    <br><i>"Do not change any files. Attempt to execute the 
    `cvs_command', but only to issue reports; do not remove, update, 
    or merge any existing files, or create any new files. Note that CVS 
    will not necessarily produce exactly the same output as without `-n'. 
    In some cases the output will be the same, but in other cases CVS will 
    skip some of the processing that would have been required to produce the 
    exact same output."</i>
    <br>This is useful with the <a href="#CVS update">update</a> or
    <a href="#CVS checkout">checkout</a> commands when you want to find out 
    what others have changed and what you have changed 
    without actually importing others' changes into 
    your files.  For example:
    <div class="shell">
    ZFIN_WWW % cvs -nq update 
    U make.default.apg.rules
    U make.default.rules
    ...
    </div>
    indicates that <span class="file">make.default.apg.rules</span> and 
    <span class="file">make.default.rules</span> have been
    updated by you but not yet committed.
  </td>
 </tr>
 <tr>
  <td class="term">-q</td>
  <td>Cause the command to be somewhat quiet; informational messages, such
    as reports of recursion through subdirectories are suppressed.
  </td>
 </tr>
</table>


<h4 class="zdoc"><a name="CVS Common Command Options">CVS Common Command Options</a></h4>

<p>Each CVS command has its own set of command options.  However, a 
subset of those options are common to all or most of the CVS commands.  
Command options follow the cvs command, but precede the command's arguments:

<div class="shellline">
  cvs [ global_options ] command [ command_options ] [ command_args ]
</div>

<p>This table 
discusses some of the more useful common command options.

<br><br>
<table class="definition">
 <tr>
  <th>Option
  <th>Description
 <tr>
  <td class="term"><a name="CVS -l command option">-l</a></td>
  <td>
  Run the command in the current directory only.  CVS is recursive by 
  default.  Without the -l option CVS will apply the command to the current 
  directory and then recursively to all of its subdirectories.  This may
  generally be the right thing to do but it can be tedious when you have
  changed only one local HTML file.
  </td>
 </tr>
 <tr>
  <td class="term"><a name="CVS -P command option">-P</a></td>
  <td>
    Prunes empty directories.  Files can be (and are) removed from CVS cleanly.
    However, empty directories never really go away.  To tell CVS that you 
    don't want to see empty directories as a result of 
    <a href="#CVS update">update</a> or 
    <a href="#CVS checkout">checkout</a> commands.
  </td>
 </tr>
</table>



<h3 class="zdoc"><a name="CVSweb">CVSweb</a></h3>

<p>CVSweb is a GUI that makes CVS repositories readable over the web.
The results from many of the commands listed in 
the <a href="#CVS Commands">CVS Commands</a> section can be viewed using CVSweb.
The ZFIN CVSweb site is
at <a href="http://cvs.zfin.org/cvs/cvsweb.cgi/">http://cvs.zfin.org/cvs/cvsweb.cgi/</a>.  
It is only accessible if you are are connected to the University of Oregon
network.

<p>We don't have any documentation for CVSweb.  Fortunately, it is fairly 
self-explanatory.  Contact Dave C if you have problems with it.



<h3 class="zdoc"><a name="CVS, Locking, and Multiple Developers">
CVS, Locking, and Multiple Developers</a></h3>

<p>CVS uses RCS for its underlying file storage.  Therefore you will 
occasionally see things from RCS popping up in CVS.  However, CVS has
many fundamental differences with RCS, and one of those fundamental
differences is its approach to supporting multiple developers.

<h4 class="zdoc"><a name="The RCS Approach">The RCS Approach</a></h4>
<p>RCS requires developers to first get an exclusive lock on a file before
they can update it.  Only the developer that holds the lock
can update a file and that developer holds the lock until their changes are
checked in, or until they release the lock.  This clearly prevents 2 developers
from making conflicting changes to the file.  Whoever gets the file last
can't start making changes to the file until they get a lock on it,
and when they do get a lock on it, the version of the file they get will 
have the earlier developer's changes already in it.

<p>The down side of this is that forces development to run serially,
rather than in parallel.  Most often developers will be making complementary
rather than conflicting changes to a file.  In such cases there is no need
for RCS's exclusive locking model.  When developers start changing a file
they may not have a clear idea when that change will actually be committed
(the CVS term) or checked in (the RCS term).  It may be that another developer
needs the file for a quick change and can't get it.  Under RCS such changes 
involve negotiation.  The developer who has the lock has to release it so 
the second developer can get it.  The second developer then makes their change
and checks it in.  The first developer then checks it out again and has to
figure out how to re-apply their changes to the now modified file.  
It's complicated.



<h4 class="zdoc"><a name="The CVS Approach">The CVS Approach</a></h4>

<p>CVS assumes that development should occur in parallel
whenever possible.  Its default assumption is that two developers working
on the same file at the same time are likely to be making complementary changes
to the code, rather than conflicting ones.

<P>Developers do not exclusively lock files in CVS.  Rather they register
their intent to modify a file by 
using the <a href="#CVS edit">edit</a> command.
The edit command gives the developer a copy of the file with write permissions
and registers in CVS that the developer has an updateable copy of the file.

<p>If you want to see what other developers are editing a file, you can
run the 
<a href="#CVS editors">editors</a>
command to list who has done an edit command on a file,
but has not yet committed their changes.  If you are really concerned about 
changes to a particular file, you can tell CVS with the 
<a href="#CVS watch add">watch add</a> 
command to send you an e-mail whenever anybody 
issues an edit command on that file.



<h4 class="zdoc"><a name="Conflict Resolution in CVS">Conflict Resolution in CVS</a></h4>

<p>CVS's parallel development model does allow for the possibility of 
conflicting changes being made by multiple developers on a file at 
the same time.

<p>The <a href="#CVS update">update</a>
and <a href="#CVS checkout">checkout</a> commands 
have the possibility of merging your changes 
with those of other developers.  These commands can merge other
developers' changes with your changes in your updated (but as yet
uncommitted) copy of the file.  
Both commands have the potential for conflicts.  The 
<a href="#CVS commit">commit</a> command will not work if it detects and
conflicts.

<p>There are two types of conflict we need to worry about:

<ul>
  <li><b>Logical Conflicts.</b>  This occurs when multiple developers
    make changes to the file that may or may not be in the same
    physical parts of the file, but that introduce logical incompatibilities
    to the file.  Where the changes do not physically overlap, reports 
    this as a merge.  If they do overlap it is reported as a conflict.

  <li><b>Physical Conflicts.</b>  This occurs when multiple developers
    make different changes to the <i>same lines in the file.</i>.  CVS
    detects these cases and flags them.  Physical conflicts are also generally
    logical conflicts as well.  CVS always reports physical conflicts
    as conflicts.
</ul>

<p>The way CVS deals with physical conflicts depends on the command being used.

<p><b>Update and Checkout:</b>

<p>The <a href="#CVS update">update</a> command brings other
developers' committed changes into the files in your directories.  The 
<a href="#CVS checkout">checkout</a> command also does this if used on 
an already existing project directory.
These commands bring
you up to date with what's changed in CVS since the last time you ran
an update or checkout command.  
There are several possible cases that can occur when
you update/checkout a file that you have modified locally, but haven't yet
committed.

<ul>
  <li><b>No other developer has changed the file.</b>  In this case CVS
    does nothing.  It leaves the updated uncommitted file in your directory 
    unchanged.  CVS will tell you that you have modified the file locally.

  <li><b>Another developer has changed the file, but there are no physical 
    conflicts.</b>  In other words, another developer has changed different
    parts of the file then you have.  In this case, CVS will merge the other
    developer's changes with yours in your local copy of the file.  CVS will
    tell you it is doing this.

  <li><b>Another developer has changed the file, and there are physical 
    conflicts.</b>  In other words, another developer has changed at least
    some of the same parts of the file as you have.  In this case CVS will
    merge the two sets of updates together into your copy of the file.  
    Wherever physical conflicts occurred, both versions of the changed lines
    will be included in your copy of the file and they will be specially 
    delimited.  The original version of your file will be copied to a file
    with name "#<i>file_name</i>".  See Section 10.3 of the
    <a href="Products/cvs.pdf"><i>CVS Manual</i></a> 
    for an example of this process.
</ul>


<p><b>Commits:</b>

<p>The <a href="#CVS commit">commit</a> command commits changes that you have
made in your copies of files into the CVS repository.  It brings the CVS
repository up to date with what you have been doing, and therefore makes
your changes visible to others.  Commits, like updates have the potential 
for conflicts.  Here are the cases:

<ul>
  <li><b>No other developer has changed the file.</b>  In this case CVS
    commits your changes to its repository.

  <li><b>Another developer has changed the file, but there are no physical 
    conflicts.</b>  In other words, another developer has changed different
    parts of the file then you have.  In this case, CVS reject the commit
    until you have first merged the other developers' changes into your
    files with an update or checkout command.

  <li><b>Another developer has changed the file, and there are physical 
    conflicts.</b>  In other words, another developer has changed at least
    some of the same parts of the file as you have.  In this case CVS will
    reject your commit request.  It will not commit a file that
    has physical conflicts.  You will need to update the file and resolve
    conflicts before checking it in.
</ul>

<p>Generally, before you do a commit, you should first do an update, look for
any reported merges, and then examine and test the changes that were merged.
Once that has been done, you should repeat the process until no merges are
reported by the update.  Then, and only then, should you commit your changes.



<h3 class="zdoc"><a name="CVS and Large Projects">CVS and Large Projects</a></h3>

<p>Part of the work we do in ZFIN comes in relatively small chunks and has 
a short lifespan from the start of the work to the end of the work.  Such 
work involves only 1 developer and a relatively small number of files.

<p>However, we often do projects that span more than a short period of time
or that involve more that one developer.  On the small end of the spectrum 
there are projects where one developer makes almost all of the changes, except
for one file that is modified by a different developer.  On the other end of
the spectrum are projects that span many months, affect many files, 
and involve most or all of the ZFIN development staff.

<p>The approaches discussed in previous sections will not work in these 
situations.  Previous discussion assumed that changes were being made for 
single developer projects.  In those cases, developers can hang onto all
changes until they are ready to check them in all at once.  The changes are
then propagated to production shortly after that.  Other developers can
then pick up the changes as well.  
In this model, what's 
in CVS and what's in production are always in close agreement with each 
other.  The only coordination
issues that arise are making sure that others' independent projects haven't 
interfered with your changes.

<p>However, coordination issues can get complicated whenever one of these 
conditions occur:

<ul>
<li>More than one developer is working on the same project.
<li>One developer is working on more than one large project.
</ul>

<p>In the first case the problem is how do different developers communicate
their changes to each other, without also placing them in CVS (and therefore
production) before the changes are ready to go into production.  In the
second case the problem is how does the developer keep the two projects 
separate, such that when one set of updates is committed, all of the updates
for that project are committed and none of the updates for the other project
are committed.

<p>We have several possible methods for doing these types of coordination.
Deciding which one to use for a given project is a function of personal
choice and how complicated the project is.  More complicated projects require
the more complicated measures.



<h4 class="zdoc">
<a name="Manual Coordination of Updates for Multiple Developer Projects">
         Manual Coordination of Updates for Multiple Developer Projects</a>
</h4>

<p>Updates can be manually coordinated for 
small projects where the bulk of the work is done by a single developer
and other developers contribute only a few files to the process.
In such cases it is easiest if only the main developer ever edits anything
in CVS or commits changes to CVS.  The main developer can either give others
write access to the files they need to modify, or the other developers can
modify copies of the files and then send them to the main developer.  If the
other developers are modifying their own copies of the files and then sending
them to the main developer, then they should not be doing these modifications
inside their CVS tree, or, if they are, then any modified files should be given
different file names.



<h4 class="zdoc">
<a name="Manual Coordination of Updates for a Single Developer with Multiple Projects">
         Manual Coordination of Updates for a Single Developer with Multiple Projects</a>
</h4>

<p>Often the easiest way to deal with this situation is to assign the developer
a different web site for each project.  The developer then creates a 
separate ZFIN_WWW source tree for each project.  The developer then only 
has to remember which source tree and web site go with which project.

<p>This approach works well in CVS, except that the CVS edit/editors mechanism
gets confused.  This does not cause any hard problems but it may cause you
not to be listed in the output of the cvs editors command when you should.


<h4 class="zdoc">
<a name="Automatic Coordination of Updates and CVS Branching">
         Automatic Coordination of Updates and CVS Branching</a>
</h4>

<p>Tom, can you check this?

<p>The manual methods described in the previous sections are useful,
but won't scale well as the number of developers involved
increases.  This section describes how we can use CVS to deal with 
large projects.

<p>Also see pages 120-130 of 
<a href="Products/cvsbook.ps"><i>Open Source Development With CVS</i></a>
for an alternate and more
complete explanation of this material.

<p>CVS supports <i>branching</i>.  This means that there can be a mainline
version of the code and also one or more divergent versions of the code.  All
of our discussion so far has assumed that we were working on the mainline.
If you don't do anything special in CVS, then by default you will be using
and updating the mainline version of the code.

<p>ZFIN uses CVS branching whenever we start a project that we suspect will 
involve multiple developers updating multiple files, or that we suspect will
span a significant amount of time between start and finish.

<p>The process for creating a new branch is to talk to Dave and:
<ul>
  <li>Settle on a name for the new branch.  
    <p>Branch names have
    the form <i>bYYYY-MM-DD-meaningful-part</i> where the date is the date the 
    branch was created, and <i>meaningful-part</i> is something like "zmap" or 
    "mapper".  The leading b stands for "branch".  CVS tags must start with a 
    letter.
    <br><br>

  <li>Determine which test site(s) you will use for the branch.
    <p>There are several options here.
    <br><br>
    <ul>
      <li>Use a different web site for your mainline and for each of 
        your currently active branches.  This option chews through 
	disk space (and virtual host names)
	but other than that this is the simplest option presented here.
	Contact Dave when you need an additional web site to support a branch.
	When switching from the mainline to the branch (or vice versa) 
	you would need to source the <a href="#.env Files">.env	file</a>
	for the new web site.

	<br><br>
      <li>Use a different web site for your mainline and for each of 
        your currently active branches, but share the branch web sites 
	with other developers working on the same branch.  In other 
	words, multiple developers would be building to	the same web 
	site, each from their own source tree.  This approach saves disk
	space and virtual host names, and does not require developers 
	to check in their changes in order to get them out to other 
	developers.  However, this approach can have serious and subtle 
	problems with unforeseen interactions between the two source trees.  
	Stay away from this one until the disk space or virtual host
	name space becomes a problem.
    </ul>
    <br>
    <p>Since databases are tied to web sites, the choice of which web 
    site to use determines which database will be used.

  <li>Dave will then do several things
    <br><br>
    <ul>
      <li>Make appropriate changes to the 
        <a href="admin.html#httpd.conf Configuration File">Apache configuration 
	file (httpd.conf)</a>
	and Web DataBlade configuration file on the
	<a href="#Development Server">development server</a>. Also update and/or
	create the appropriate .env and .tt files.  Which changes are made
	depend on the decisions about which web sites and databases should
	be used.
	<br><br>
      </li>
      <li>Assign a tag (a symbolic name) to the latest revisions in the 
        mainline. This
	is <i>not</i> the name of the branch, but is closely related to it.
	This tag has the form <i>bYYYY-MM-DD-meaningful-part</i>-base.  This 
	is an easy way to mark which versions were used to start a branch.  
	This is done with the cvs rtag command, for example:
        <div class="shell">
        % cvs rtag b2001-02-13-zmap-base ZFIN_WWW
        </div>
      </li>
      <li>Create a branch with the name <i>bYYYY-MM-DD-meaningful-part</i>.
	This is also done with the cvs rtag command, for example:
        <div class="shell">
        % cvs rtag -b b2001-02-13-zmap ZFIN_WWW
        </div>
      </li>
    </ul>
</ul>

<p>To actually get the source files in that branch you will need to checkout 
a brand new copy of the ZFIN_WWW source file hierarchy.  This should always 
be done into a different location than your mainline ZFIN_WWW directory.  Note
that CVS will quite happily let you checkout the branch files right on top of
your mainline files.  However, if you do this, you will no longer be able to 
update any mainline files because you no longer have access to them.  If you 
check the branch out into a different directory then you can still access both.

<p>The process for creating and populating your web site 
are almost identical to those in the
<a href="#Creating a ZFIN Development Environment, An Example">
Creating a ZFIN Development Environment, An Example</a> section.  

<p>The notable difference is that when you checkout the CVS tree, you should
provide the -r option and tell it which branch you want.  For example.

<div class="shellline">
  % cvs checkout -r b2001-02-13-zmap ZFIN_WWW
</div>

<p>Other than that,
the steps laid out for the mainline will also work for branches.

<p>This will create a ZFIN_WWW directory that contains
versions of files from the given branch (b2001-02-13-zmap in the example).  
From this point on, any
CVS commands you do in the new ZFIN_WWW tree will be on file versions from 
that branch (with the <i>possible</i> exception of cvs add commands).

<p>This means that when you cvs edit and commit files in this tree the 
changes will take place only in the branch; they will not affect the mainline 
code.  This allows developers to send updates to each other in a controlled
fashion, without also putting the updates into the mainline (and thus also into
production).

<p>The version numbers of files look different for branches than they do for
the mainline code.  In ZFIN, the mainline always has the form <i>1.x</i>, 
where <i>x</i>
represents the number of times the file has been updated since it was first
checked in.  Branch file version numbers typically have the form <i>1.x.0.y</i>
where <i>1.x</i> is the file version in the mainline that the branch was first 
created from, and <i>y</i> is the number of times this file has been revised 
in this branch.

<p><kbd>gmake</kbd> will also operate on the branched versions of the files.


<p><b><a name="Keeping up with the Joneses">Keeping up with the Joneses</a></b>

<p>Work on the mainline doesn't stop when a branch is created.  At periodic
intervals during the life of the branch, you should incorporate the mainline
changes into your branch.  This prevents you from getting too out of date.
It is generally a bad idea to wait until you are ready to 
<a href="#Merging Branches Back Into the Mainline">merge your changes
back into the mainline</a> to do this.  Doing this step early and often
will minimize the amount of work you have to do when you get to that step.

<p>To get the latest changes from the mainline into your branch, do the 
following.
<ol>
  <li>CD into your branch ZFIN_WWW directory and source the .env file for
    the branch site.
  <li>Run cvs update with the -j HEAD option
    <div class="shellline">
    % cvs -q update -j HEAD
    </div>
    <p>This will cause any files that were modified in the mainline 
    to be merged with the branch and into your working directory.

  <li>Investigate any conflicts that were reported by the update and
    resolve them.  Test the changes.
    <p>tips for merging:
  <ul>
     <li> If you find a conflict in your branch with a file you know
          wasn't changed, just bring in the latest copy from the
          mainline using: 
          <div class="shellline">
          % cvs update -p -r HEAD > file.html.new 
          % mv file.html.new file.html
          </div> 
          <p>The stdout redirect is important, because you don't want 
          a sticky tag.  More information in the 
          <a href="#CVS Commands">CVS Commands</a> section.
  </ul>
  <li>Commit the files that were updated into your branch.
</ol>


<p><b><a name="Merging Branches Back Into the Mainline">
               Merging Branches Back Into the Mainline</a></b>

<p>Eventually, a project will be ready for prime time.  All of the necessary 
updates will have been done, tested, and committed to the project's branch 
in CVS.  It is now time to merge the branch back into the mainline and put
it up in production.  

<p>At this point the lead developer for the project will need to merge the
branch into the mainline using their mainline copy of the ZFIN_WWW hierarchy.
To do this:

<ol>
  <li> CD into your mainline ZFIN_WWW directory and source the
    .env file for your mainline site.
  <li>Run cvs update with the -j option to merge in the branch.  For our 
    example branch this would be:

    <div class="shellline">
    % cvs -q update -j b2001-02-13-zmap
    </div>

    <p>This will cause any files that were modified in that branch to be merged
    with the mainline.  

  <li>Investigate any conflicts that were reported by the merge and resolve 
    them.  Test the changes.

  <li>Commit the files that were updated by the merge and then propogate them
    to production.

</ol>

<p>There are several possible variations on this theme.  For example we 
could tag the mainline immediately before the merge, and/or immediately 
after it.  Time will tell what works best.





<!-- ========= CREATING A ZFIN DEVELOPMENT ENVIRONMENT, AN EXAMPLE ===== -->

<h2 class="zdoc"><a name="Creating a ZFIN Development Environment, An Example">
Creating a ZFIN Development Environment, An Example</a></h2>

<p>This section walks through an example of setting up a ZFIN development
environment.  
This explanation assumes that this is the first time the environment has
been set up.  Once you set up your environment, you will only occasionally
want to recreate it again from scratch.  

<p>This example sets up albino for the albino.zfin.org URL.
The steps here are the same as
you would take to set up your own environment.

<p>A subset of this material is described with graphics in the 
<a href="#Creating a Test Web Site">Creating a Test Web Site</a> section.

<p>The process:

<ol>
  <li>Setup your environment variables
    <p>Each different environment has its <a href="#.env Files">.env file</a>
    in the <a href="#ZFIN Commons">ZFIN Commons</a> env directory.
    Source the appropriate file
    for your environment:

    <div class="shell">
 embryonix [~]% source /private/ZfinLinks/Commons/env/albino.env
 albino ~% 
    </div>

    <p>Or, since I have an alias defined to do just that command, I could
    have typed:

    <div class="shell">
 embryonix [~]% srcalbino
 albino ~% 
    </div>

    <p>The .env files set your <a href="#Environment Variables">CVSROOT, 
    Informix, and Makefile environment 
    variables</a> to the values that are appropriate for the environment.  They
    also modify your prompt to reflect which ZFIN development environment you
    are using.
    <br><br>
  </li>
  <li>Load data into your database

    <p>I'm going to load the data from production that was unloaded on 
    2000/12/19
    into albino's database, clemdb.  

    <div class="shell">
 albino ~% <a href="#loaddb.pl">loaddb.pl</a> -ee clemdb <a href="#ZFIN Unloads">/research/zunloads/databases/zfindb/</a>2000.12.19.1
 Fri Dec 22 13:30:26 PST 2000 Dropping old database (if it exists)...
 Fri Dec 22 13:30:42 PST 2000 Defining new database...
 Fri Dec 22 13:30:53 PST 2000 Creating list of tables to load...
 Fri Dec 22 13:30:53 PST 2000 Creating preload and postload scripts...
 Fri Dec 22 13:31:22 PST 2000 Disabling indexes, constraints, and triggers...
 Fri Dec 22 13:31:23 PST 2000 Loading data into database...
 Fri Dec 22 13:34:01 PST 2000 Enabling indexes, constraints, and triggers...
 Fri Dec 22 13:35:42 PST 2000 Enabling logging...
 Archive to tape device '/dev/null' is complete.

 Program over.
 WARNING!!!!  You MUST now cd to your ZFIN_WWW directory and type
 WARNING!!!!
 WARNING!!!!    % gmake postloaddb
 WARNING!!!!
 WARNING!!!!  Failure to do this results in very unpleasant
 WARNING!!!!  behavior in your web site, and the ire of all
 WARNING!!!!  your coworkers.

 albino ~% 
    </div>

    <p>Note the warning at the end.  We will take care of this
    in the  <a href="#Make the postloaddb target">
    Make the postloaddb target</a> step below.


    <br><br>
  <li>Get to the directory you are going to put your source tree in.

    <P>For your own environments
    you'll want to cd to /research/zusers/<i>your_login</i>.
    <div class="shell">
 albino ~% cd <a href="#ZFIN Users">/research/zusers/</a>clements/Projects/Zfin
   </div>

    <br><br>
  <li>Checkout the ZFIN_WWW project into your directory.

    <div class="shell">
 albino Zfin % <a href="#CVS checkout">cvs checkout</a> -P ZFIN_WWW
 cvs checkout: Updating ZFIN_WWW
 U ZFIN_WWW/Makefile
 U ZFIN_WWW/make.default.apg.rules
 U ZFIN_WWW/make.default.rules
 U ZFIN_WWW/make.include
 cvs checkout: Updating ZFIN_WWW/cgi-bin
 U ZFIN_WWW/cgi-bin/Makefile
 U ZFIN_WWW/cgi-bin/SuperWebdriver
    </div>

    About 1500 lines deleted here.

    <div class="shell">
 U ZFIN_WWW/server_apps/sysexecs/Makefile
 cvs checkout: Updating ZFIN_WWW/server_apps/sysexecs/encryptpass
 U ZFIN_WWW/server_apps/sysexecs/encryptpass/Makefile
 U ZFIN_WWW/server_apps/sysexecs/encryptpass/encode.c
 U ZFIN_WWW/server_apps/sysexecs/encryptpass/encryptpass
 cvs checkout: Updating ZFIN_WWW/server_apps/sysexecs/make_thumbnail
 U ZFIN_WWW/server_apps/sysexecs/make_thumbnail/Makefile
 U ZFIN_WWW/server_apps/sysexecs/make_thumbnail/make_thumbnail.sh
 albino Zfin % 
    </div>

    <P>You now have the latest and greatest version of all the files in CVS
    that support the ZFIN web site. 


    <br><br>
  <li>Make your web site.

    <p>This step will create the files that are in your web site, and load 
    your app
    pages and database functions.

    <div class="shell">
 albino Zfin % cd ZFIN_WWW/
 albino ZFIN_WWW% <a href="#Makefiles and the ZFIN_WWW Source Tree">gmake</a>
 gmake -C home
 gmake[1]: Entering directory `/research/zusers/clements/Projects/ZFIN_WWW/home'
 mkdir -m 755 /research/zcentral/www_homes/albino/home;
 /private/ZfinLinks/Commons/bin/makespecific.pl index.html \
   /private/ZfinLinks/Commons/env/albino.tt \
   /research/zcentral/www_homes/albino/home/index.html
 /private/ZfinLinks/Commons/bin/makespecific.pl robots.txt \
   /private/ZfinLinks/Commons/env/albino.tt \
   /research/zcentral/www_homes/albino/home/robots.txt
 cp -p fish_bgd.gif /research/zcentral/www_homes/albino/home/fish_bgd.gif
    </div>

    Lots and lots of makefile status output deleted here.

    <div class="shell">
 /bin/cp -fp favicon.ico \
   /research/zcentral/www_homes/albino/cgi-bin_albino/favicon.ico
 /bin/touch \
   /research/zcentral/www_homes/albino/cgi-bin_albino/favicon.ico
 gmake[1]: Leaving directory \
   `/nfs/research/zusers/clements/Zfin/ZFIN_WWW/cgi-bin'
 albino ZFIN_WWW% 
    </div>
    <br><br>
  <li><a name="Make the postloaddb target">Make the postloaddb target</a>

    <p>This step is required after each time 
    <a href="#loaddb.pl">loaddb.pl</a> is run.  It is usually
    run immediately after loaddb.pl, but that is not possible when you are
    first setting up your environment (as this example is doing).  The 
    <a href="#postloaddb">postloaddb</a> target 
    removes everything from the database that
    points back to the database that the data was originally unloaded from,
    and replaces it with data (web pages, SQL functions, etc) that points
    to your development web site instead.

    <div class="shell">
 albino ZFIN_WWW% gmake postloaddb
 gmake -C home postloaddb; gmake -C server_apps postloaddb; \
   gmake -C lib postloaddb; 
 gmake[1]: Entering directory \
   `/research/zusers/clements/Projects/ZFIN_WWW/home'
 gmake -C ZFIN postloaddb
 gmake[2]: Entering directory \
   `/research/zusers/clements/Projects/ZFIN_WWW/home/ZFIN'
 gmake -C APP_PAGES postloaddb
    </div>

    Lots of makefile output deleted here.

    <div class="shell">
 gmake[1]: Leaving directory \
   `/research/zusers/clements/Projects/ZFIN_WWW/lib'
 albino ZFIN_WWW% 
    </div>

  <li>Test the web site

    <p>Everything that is needed for a test web site has now been done.  To 
    test it, run netscape and go to your test site's home page.  In this 
    example this is:

    <div class="urlline">
    <a href="http://albino.zfin.org">http://albino.zfin.org</a>
    </div>

    <p>It should come up.  Click on several static pages and on the database 
    link.  They should all come up fine.  If not then talk to Dave C.

</ol>


<!-- ========= UPDATING A FILE, AN EXAMPLE ============================= -->

<h2 class="zdoc">
<a name="Updating a File, An Example">Updating a File, An Example</a>
</h2>

<p>This section shows an example of how you might update a page.  A subset
of this material is described, with graphics, in the 
<a href="#Updating a File">Updating a File</a> section.

<p>We noticed that in the process of converting the web site to the new
infrastructure 2 items on the ZFIN home page were changed in ways we didn't
want them to be.  In particular:

<ul>
  <li>The MAIN SITE pointer just above the list of mirror sites now points
    back to the test site.  I'd rather have this point to the main production
    site (mainly because we know this will eventually make support of the
    mirror sites easier).
  </li>
  <li><p>The pointer to the Melbourne Australia mirror site doesn't work 
    anymore. In the process of setting up the infrastructure, its address

    <div class="urlline">
    <a href="http://zdb.wehi.edu.au/zdb">http://zdb.wehi.edu.au/zdb</a>
    </div>

    <p>was inadvertently changed to 

    <div class="urlline">
    http://zdb.wehi.edu.au/ZFIN
    </div>

    <p>which doesn't work.  
  </li>
</ul>

<p>To fix these things I'll need to change home/index.html.  Here's the 
process for changing files, using index.html as an example:

<ol>
  <li><p>cd to the directory containing the file(s) you want to change.

    <div class="shellline">
    albino ZFIN_WWW% cd home
    </div>

  </li>
  <li><p>Tell CVS what file(s) you want to edit.

    <div class="shellline">
    albino home% <a href="#CVS edit">cvs edit</a> index.html
    </div>

    <p>This registers the fact that you are editing the file and gives you a 
    writable copy of it.

  </li>
  <li><p>Edit the file(s) using your editor of choice.</li>

  <li><p>Type gmake to put the changes in your test web site.

    <div class="shell">
 albino home% gmake
 research/zfin/central/Commons/bin/makespecific.pl index.html \
   /private/ZfinLinks/Commons/env/albino.tt \
   /research/zcentral/www_homes/albino/home/index.html
 gmake -C ZFIN
 gmake[1]: Entering directory \
   `/research/zusers/clements/Projects/ZFIN_WWW/home/ZFIN'
 gmake -C APP_PAGES
 gmake[2]: Entering directory \
   `/research/zusers/clements/Projects/ZFIN_WWW/home/ZFIN
 /APP_PAGES'

 [lots of makefile status output deleted here]
    </div>

    <p>Note that although we are only doing one thing, the gmake command 
    explores all of the subdirectories of home to check if anything needs 
    to be done there.
    Since nothing does, this runs very quickly.  However, if you want to avoid
    all that output, <i>and you know exactly what you want to remake</i>
    (which is not always clear in the land of makefiles) you could specifically
    just make the target you want to update.  In this case that would be 
    $TARGETROOT/home/index.html:

    <div class="shell">
 albino home% gmake $TARGETROOT/home/index.html 
 /private/ZfinLinks/Commons/bin/makespecific.pl index.html \
   /private/ZfinLinks/Commons/env/albino.tt \
   /research/zcentral/www_homes/albino/home/index.html
 albino home% 
    </div>

    <p>However, this is somewhat dangerous as there may be some non-obvious 
    dependencies involving the file(s) you have updated.  In general, just type
    gmake.

    <br><br>
  <li>Check your test web site to see if the changes work

    <p>In this case we tested the two links.  They now point to the main 
    ZFIN site and the link to the Australian web site now works as well.

    <br><br>
  </li>

  <li>If this change has been 
    <a href="standards.html#Enforcement">non-trivial</a>, then I would need to
    have someone else at ZFIN review the code and sign off on it.  See the
    <a href="standards.html#Enforcement">Enforcement</a> section of the 
    <a href="standards.html">Standards and Best Practices</a> document for
    more on this process.

    <p>In the example here, the change qualifies as trivial, so we don't
    need to get it code reviewed.
  </li>
  <li>Once your change has been made, tested, and 
    <a href="standards.html#Enforcement">possibly code-reviewed</a> check it in.

    <p>This step puts your changes into the files in CVS so that other 
    developers and the production web site can pick them up.
    <div class="shellline">
 albino home% <a href="#CVS commit">cvs commit</a> index.html
    </div>

    <p>In my shell this brings up an emacs window with this text in it:

    <div class="shell">
 BUGZID: none
 CVS: ----------------------------------------------------------------------
 CVS: Enter Log.  Lines beginning with `CVS:' are removed automatically
 CVS: 
 CVS: Modified Files:
 CVS:    index.html 
 CVS: ----------------------------------------------------------------------
    </div>

    <p>There are several things that must be done when commiting a file.
    See <a href="standards.html#Committing Files">Committing Files</a>
    in the <a href="standards.html">Standards and Best Practices</a> document
    for exact details.  Basically, you need to specify the 
    <a href="intro.html#FogBUGZ">FogBUGZ</a> case, if there is one (otherwise
    leave it as none), a description of the changes, and the name of the 
    code reviewer, if there was one.

    <div class="shell">
 BUGZID: none
 Fixed link to MAIN SITE so that it always points at zfin.org, rather than
 at whatever test site your running.  Also fixed link to Australian mirror
 site.  It had been corrupted in the infrastructure conversion.
 CVS: ----------------------------------------------------------------------
 CVS: Enter Log.  Lines beginning with `CVS:' are removed automatically
 CVS: 
 CVS: Modified Files:
 CVS:    index.html 
 CVS: ----------------------------------------------------------------------
    </div>

    <P>Exited the editor which committed the change and displayed 
    this in the shell:

    <div class="shell">
 Checking in index.html;
 /research/zcentral/Vault/CVSroot/ZFIN_WWW/home/index.html,v &lt;-- index.html
 new revision: 1.3; previous revision: 1.2
 done
 albino home% 
    </div>

    <p><b>It is not acceptable practice to enter blank text when doing 
    updates.</b>
    If you enter blank text the rest of the ZFIN team will hunt you down and 
    mock you.
    On the other hand, the above description is probably more verbose than this
    simple change needed.

    <p>Note that we could have typed

    <div class="shell">
 albino home% cvs commit
    </div>

    <p>without specifying the file to commit.  In this case, CVS would 
    have searched this directory and all subdirectories for any changed 
    files and then committed them all at once, with the same update message.

    <br><br>
  <li>See the <a href="#Putting Changes Into Production">
    Putting Changes Into Production</a> section for what to do to get your
    changes out into production.

</ol>


<!-- ========= PUTTTING CHANGES INTO PRODUCTION ======================== -->

<h2 class="zdoc"><a name="Putting Changes Into Production">
             Putting Changes Into Production</a></h2>

<p>Once a change has been committed to CVS it must then be propagated to
the production web site.  This is a multistep process where some steps are
done by the developer that committed the change, and others are done by
someone logged in as the informix user, described as a 
<b>Finisher</b> below.  Currently (2004/11), Dave C, Judy, or Sierra perform
the finisher steps.

<p>A subset of this material is described, with supporting graphics, in the
<a href="#Deploying a Change to Production">Deploying a Change to Production</a>
section.

<p>Here is the process.

<ol>
  <li><b>Developer:</b> Notify a finisher what files you have checked in.
    <br><br>
  <li><b>Finisher:</b> Post the change to the 
      <a href="intro.html#Pre-Production and Sandbox Site">preproduction site</a>.
    <ol>
      <li>Log in as informix on embryonix, and set up your environment to 
        produce the preproduction web site.
	<div class="shell">
	% srcalmost
	% cdalmost
	</div>
	This will set up your environment variables and get you to the 
	ZFIN_WWW source tree that is used to produce both the production 
	and preproduction web sites.
        <br><br>
      <li>Find out what has been updated in CVS
        <div class="shellline">
	% cvsmods
	</div>
	This should list all files that are different between the ZFIN_WWW
	tree and what is checked into CVS.  The list of files with "U" in
	front of them should be the same as the list of updated files the 
	developer sent you.  Investigate any files that were not on the
	developer's list.  
	<p><kbd>cvsmods</kbd> displays the log entries for the changes.
	Check the log entries to see if the commits appear to conform to 
        <a href="standards.html#Committing Files">ZFIN standards for CVS commits</a>. 
	If they don't then:
	<ol>
	  <li>Send an e-mail to the <b>developer</b> asking for more 
            information.  In particular, did the change have a 
            <a href="intro.html#FogBUGZ">FogBUGZ case</a> (if not specified), 
            and/or was the change 
            <a href="standards.html#Enforcement">non-trivial</a>, and if so who
	    code reviewed the change.
          </li>
	  <li>If they think the change was trivial, or they got it code 
	    reviewed, but forgot to put the reviewer's name in the CVS message, 
	    then do the update.  If they just forgot the FogBUGZ case, then
	    do the update.
	  </li>
	  <li>If the change was non-trivial in their opinion and they did 
	    not get it code reviewed, then hold off on the update until they 
	    get it code reviewed.
	  </li>
	</ol>
        <br><br>
      <li>Pull updates from CVS
        <div class="shellline">
	% cvs -q update -dP
	</div>
      <li>Make the site
        <div class="shellline">
	% gmake 
	</div>
      <li>Ask developer to test the preproduction site.
    </ol>
    <br>
  <li><b>Developer:</b> Test the preproduction web site 
    (<a href="http://almost.zfin.org">almost.zfin.org</a>) to see if the changes
    work.  If they don't then determine why, make the appropriate updates in 
    CVS and notify a finisher of the updates, and then repeat the above
    process.  If they do work then notify a finisher that the changes can be
    posted to production.
    <br><br>
  <li><b>Finisher:</b> Post the changes to production.  Log in as 
    informix on the <a href="#Production Server"> production server</a>, 
    set up your environment, and remake the web site.
    <div class="shell">
    % srchelix
    % cdhelix
    % gmake
    </div>
    <p>This step is slightly different if zfin.org is running on the 
    development server:
    <div class="shell">
    % srembryonix
    % cdembryonix
    % gmake
    </div>
    <p>Notify developer that change has gone into production.
    <br><br>
  <li><b>Developer:</b> Test the production web site to see if the changes
    work.  If they don't, well then that is very bad.
</ol>


<!-- ========= UPDATING FILES IN THE ZFIN COMMONS ====================== -->

<h2 class="zdoc"><a name="Updating Files in the ZFIN Commons">
             Updating Files in the ZFIN Commons</a></h2>

<p>The <a href="#ZFIN Commons">ZFIN Commons directory</a> hierarchy is 
managed as the <a href="http://cvs.zfin.org/cvs/cvsweb.cgi/Commons/">Commons</a>
<a href="#Source Code Control and CVS">CVS project</a>.  You can obtain 
copies of it, and edit and commit files in it using the same 
<a href="#CVS Commands">CVS commands</a> that you use with the
<a href="#Makefiles and the ZFIN_WWW Source Tree">ZFIN_WWW</a> CVS 
project.

<p>The process for getting updates into the two "production" copies 
(one on the production server and one on the development server) of
the ZFIN Commons is somewhat different,
however.  The ZFIN_WWW project comes with a hierarchy of
<a href="#Makefiles and the ZFIN_WWW Source Tree">makefiles</a> 
that produce the target website.  These makefiles, along with CVS,
are used to put updates to ZFIN_WWW into production.  The ZFIN Commons
does not have (or need) a makefile hierarchy.  

<p>The production copies of the ZFIN Commons are just checked out 
versions of the CVS project.  That is, they are created with the 
<a href="#CVS checkout">CVS checkout</a> command, and updates are
posted to them using the <a href="#CVS update">CVS update</a> command.

<p>The copies on both servers can be reached via the 
<div class="fileline">
<a href="#/private/ZfinLinks">/private/ZfinLinks/</a>Commons
</div>

<p>sym link.  Both copies are owned by the 
<a href="admin.html#ZFIN Usernames">informix user</a>.  

<p>Getting updates to the ZFIN Commons into wide use is a multistep
process where some steps are
done by the developer that committed the change, and others are done by
someone logged into as the informix user, described as a 
<b>Finisher</b> below.  Currently (2004/11), Dave C, Judy, or Sierra perform
the finisher steps.
Here is the process.

<ol>
  <li><b>Developer:</b> Notify a finisher what files you have checked in.
    <br><br>
  <li><b>Finisher:</b>
    <ol>
      <li>Log in as informix on the 
        <a href="#Development Server">development server</a>, 
	and source any one of the .env
        files in order to get your $CVSROOT set.
	<div class="shellline">
	% srcwanda
	</div>
      </li>
      <li>Find out what has been updated in CVS
        <div class="shellline">
	% cd /private/ZfinLinks/Commons
	% cvs -nq update
	</div>
	This should list all files that have been updated and are awaiting
	checkout.
        <br><br>
      </li>
      <li>Pull updates from CVS
        <div class="shellline">
	% cvs -q update -dP
	</div>
	This step updates the copy of the ZFIN Commons on the development 
	server.
	<br><br>
      </li>
      <li>Restrict access to some files.  Log in as yourself on embryonix, and 
        then
        <div class="shell">
	% srcwanda      # sourcing anyone will work.
	% cd /private/ZfinLinks/Commons/bin
	% sudo chgcommonperms.sh
	</div>
	This restricts execute access to several files that you should be
	informix to run.
	<br><br>
      </li>
      <li><b>Repeat the above steps on the
        <a href="#Production Server">production server</a></b>.  If you don't do
	this step, then the recent chagnes will not be used by the production 
	web site.
      </li>
    </ol>
</ol>



<!-- ========= OBTAINING A DEVELOPMENT WEB SITE ======================== -->

<h2 class="zdoc"><a name="Obtaining a Development Web Site">
             Obtaining a Development Web Site</a></h2>

<p>Web site allocation/assignment is currently (2001/05) handled by Dave C
on an as needed basis.  If you think you need another web site then talk
to Dave C.  The process for doing this is described the
<a href="admin.html#Adding a New Web Site">Adding a New
Web Site</a> and 
<a href="admin.html#Reassigning a Web Site">Reassigning 
a Web Site</a> sections.


<!-- =================================================================== -->
<!-- ========= WEB PAGE DESIGN AND IMPLEMENTATION ====================== -->
<!-- =================================================================== -->

<h1 class="zdoc"><a name="Web Page Design and Implementation">
             Web Page Design and Implementation</a></h1>

<p>The web pages in the site can be divided into two broad classes, static
and dynamic.  This distinction determines where and how the pages are stored
and accessed, who updates them, how they are updated, and the pages' general
look and feel.

<!-- ========= STATIC PAGES ============================================ -->

<h2 class="zdoc"><a name="Static Pages">Static Pages</a></h2>

<p>Static pages do not access the database.  They contain the same content 
every time they are accessed.  Static pages are <i>primarily</i> created and 
updated by biologists, most notably Monte and Sherry.  A few of the static pages
are in the web site's <a href="#home">home directory</a>, 
but the bulk of them are in the <a href="#home/zf_info">/zf_info directory</a>.




<h3 class="zdoc"><a name="Static Page Design Guidelines">Design Guidelines</a></h3>

<p>See the <a href="standards.html#HTML">HTML section</a> of the 
<a href="standards.html">Coding Standards and Best Practices</a> document
for static page design guidelines.



<h3 class="zdoc"><a name="Static Web Page Updates">Static Web Page Updates</a></h3>

<h4 class="zdoc"><a name="How it's done depends on who is doing it">
             How it's done depends on who is doing it</a></h4>

<p>Static web pages can be updated by biologists or technologists.  The process
for doing the updates is different for each group.  Both kinds of updates 
require the involvement of technologists and biologists.


<h4 class="zdoc"><a name="Updates Done By Biologists">Updates Done By Biologists</a></h4>

<p>Biologists need to work with either a technologist or Sherry to update
a static web page.  If you have a static web page you want to update then start 
by asking Sherry.

<p><a name="Static Page Update Guidelines">There</a> are a few guidelines 
that should be followed when updating 
static pages:
<ul>
  <li><p><b>Use relative references</b>.  Use references
    like <kbd>/zf_info/news/jobs.html</kbd> or <kbd>jobs.html</kbd>, 
    rather than absolute references 
    like <kbd>http://zfin.org/zf_info/news/jobs.html</kbd>.  
    Absolute references cause 
    problems for the external mirror sites.  E-mail addresses can continue to 
    be absolute.
  </li>
  <li><p><b>Use the <a href="#Generic vs. Specific">generic tags</a> defined
    in the <a href="#TRANSLATETABLE">translate table</a> file, whenever
    appropriate.</b>
  </li>
</ul>




<h4 class="zdoc"><a name="Updates Done by Technologists">
             Updates Done by Technologists</a></h4>

<P>The steps taken by technologists to update static pages are the
same as those taken to update any other file controlled by CVS.  You
cvs edit it, and then modify, make and review the page until you are happy
with it.  Finally you commit the changed page back into CVS.  See the
<a href="#ZFIN Development Environments">ZFIN Development Environments</a>
section for details on how these steps are done.

<p>The steps for putting the changed pages into the preproduction and 
production web sites are the same as well.  See the 
<a href="#Putting Changes Into Production">Putting Changes Into Production</a>
section.



<h3 class="zdoc"><a name="checklinks">checklinks</a></h3>

<p>ZFIN uses the 
<a href="http://cvs.zfin.org/cvs/cvsweb.cgi/ZFIN_WWW/server_apps/WebSiteTools/checklinks">checklinks script</a>
to verify the consistency of 
links in the ZFIN static web pages.  checklinks verifies both internal and 
external links.  There is a weekly 
<a href="admin.html#cron">cron job</a> that calls checklinks
to verify the production web site.  You can also invoke it on demand to verify
your site by going to the
<kbd><a href="http://cvs.zfin.org/cvs/cvsweb.cgi/ZFIN_WWW/server_apps/WebSiteTools/">ZFIN_WWW/server_apps/WebSiteTools/</a></kbd>
directory and making the run target:

<div class="shell">
  embryonix ZFIN_WWW % cd server_apps/WebSiteTools
  embryonix WebSiteTools % gmake run
</div>

<p>If you are running this on a <a href="#Mirror Sites">mirror site</a>
then this
will check all of pages in the web site (all of which are static) in under 
3 minutes (as of 2002/07).  If you are running this on any site that contains 
dynamic pages then this
will verify both static and dynamic pages, and it will run for several hours.

<p>checklinks produces a text file as output, listing any broken links it
found.  The file is placed in 

<div class="fileline">
  $TARGETROOT/server_apps/WebSiteTools/report
</div>

<p>Checklinks was obtained from 

<div class="urlline">
<a href="http://www.jmarshall.com/tools/cl/">http://www.jmarshall.com/tools/cl/</a> 
</div>

<p>in 2002/06 and became the link checker for ZFIN on 2002/07/01.  It
replaced MOMspider, which had several problems and stopped working 
altogether when ZFIN went to a frameless format in 2002/05.



<!-- ========= DYNAMIC PAGES =========================================== -->

<h2 class="zdoc"><a name="Dynamic (Database) Pages">Dynamic (Database) Pages</a></h2>

<p>Dynamic pages access the database.  Their content often depends on
variables that they were invoked with.  Dynamic pages are created and 
maintained by technologists.  All of the dynamic pages can be found 
under the <a href="#home/ZFIN">/ZFIN</a> directory of the website.

<p>The dynamic pages are written using the Informix Web DataBlade.  They
use a particular markup language to mix the results of SQL queries in
with HTML.  Pages that use the Web DataBlade are known as App Pages
and tend to end with the extension <kbd>.apg</kbd>.


<h3 class="zdoc"><a name="Dynamic Page Design Guidelines">Design Guidelines</a></h3>

<p>See the <a href="standards.html#Web Datablade App Pages">Web Datablade App 
Pages section</a> of the 
<a href="standards.html">Coding Standards and Best Practices</a> document
for dynamic page design guidelines.







<h3 class="zdoc"><a name="Web Datablade">Web DataBlade</a></h3>

<p>ZFIN uses Informix's Web DataBlade product to tie together HTML with
the database.  The best source of documentation on the Web DataBlade is the 
<a href="Products/WebDataBladeApplicationDeveloperGuide.pdf"><i>Informix 
Web DataBlade Module Application Developer's Guide</i></a>.


<h4 class="zdoc"><a name="Debugging App Pages">Debugging App Pages</a></h4>  

<p>Debugging app pages is difficult.  In no particular order, here are
a number of things you can try when your app page does not work:
<p>
<br>
<table class="definition">
  <tr>
    <td class="term">
      <a href="admin.html#Apache Logs">Apache logs</a>
    </td>
    <td>
      The Apache access log will tell you what request was made.  
      The Apache error log will tell you if the page couldn't even 
      be served by Apache.
    </td>
  </tr>
  <tr>
    <td class="term">
      <a href="admin.html#webdriver Log">webdriver Log</a>
    </td>
    <td>
      The webdriver is the part of the Web DataBlade that resides inside
      Apache.  webdriver logging can be turned on on either production or
      developement.  If enabled, entries are written to the webdriver log 
      whenever
      a dynamic page is requested and processed.  The webdriver log will
      sometimes give you insight into what went wrong while processing the
      dynamic page.
    </td>
  </tr>
  <tr>
    <td class="term">
      <a name="weblint"><kbd>weblint</kbd></a>
    <td>
      Run <kbd>weblint</kbd> against the page.  <kbd>weblint</kbd> checks
      web pages for various problems.  There are two <kbd>weblint</kbd>
      executables available:
      <ol>
        <li><kbd>$INFORMIXDIR/extend/web.4.13.FC3/utils/weblint</kbd>
	  This checks the Web DataBlade tags in an app page for syntax errors.
	  This utility is far from perfect and often reports problems where
	  none exists.  However, it can be darn helpful in locating errors
	  where you forgot to type a question mark in Web DataBlade tags.  It
	  does catch a number of errors.  This <kbd>weblint</kbd> is invoked
	  as follows:

	  <div class="shellline">
  % $INFORMIXDIR/extend/web.4.13.FC3/utils/weblint 3 &lt; <i>file.apg</i>
	  </div>

	  <p>Since 2004/08, the Informix weblint is called by autmatically
	  by the makefiles whenever you make an app page.  You can run
	  the Informix weblint on all app pages in a tree by making the
	  <a href="#Targets">sanitycheck target</a>:

	  <div class="shellline">
  % gmake sanitycheck
          </div>
	</li>
        <li><kbd>/local/bin/weblint</kbd>  This can be used to check HTML
	  files.  In other words you can check the integrity of the generated
	  HTML that was sent to the browser.  You can't use this version to
	  check app pages in their raw state because it will choke on
	  all the webdriver tags.  See the manpage for details 
	  on how to run this.
        </li>
      </ol>
    </td>
  </tr>
  <tr>
    <td class="term">
      <kbd><a href="#dbaccess">dbaccess</a></kbd>
    <td>
      If the SQL in your app page is bad then your web page will often 
      display an extremely unhelpful error message such as 

      <div class="shellline">
      Syntax error or access violation.
      </div>

      <p><kbd>weblint</kbd> does not 
      detect badly formed SQL.  Occassionally there may be something helpful 
      in the webdriver log, but usually not.

      <p>One way to check your SQL in these situations is to display the 
      SQL statement in the web page, rather than execute it.  Change the
      MISQL tags to MIVAR tags and then rerun it.  (You may or may not
      have to take 
      out large portions of the rest of the file that depend on the SQL 
      statement.)  You can then cut and paste the SQL from the displayed
      web page into a <kbd>dbaccess</kbd> window and run it there.  
      <kbd>dbaccess</kbd> generally does a much better job of telling you
      exactly where the problem is.
  <tr>
    <td class="term">
      <kbd>MIERROR</kbd>
    <td>
      If there is a constraint in your db that is causing
      errors in your app page (errors on insert, delete, or update), an error
      message such as 
      <div class="shellline">
      Integrity constraint violoation.
      </div>

      <p>will often be displayed.  One way to determine which constraint
      is causing the error is to place the following code at the top of 
      your app page.  It will return which constraint is causing the problem,
      instead of simply the generic 'integrity constraint' error.  
      <div class="shell">
  &lt;?MIERROR&gt; 
    &lt;?MIVAR COND=$(XST,$MI_SQL)&gt; 
             SQL: $MI_SQL
    &lt;?/MIVAR&gt;

    Code:    $MI_ERRORCODE 
    State:   $MI_ERRORSTATE 
    Message: $MI_ERRORMSG 
  &lt;?/MIERROR&gt;
      </div>
    </td>
  </tr>
  <tr>
    <td class="term">
      <kbd><a href="db.html#Infrastructure Functions">get_time()</a></kbd>
    </td>
    <td>
      If you are trying to figure out why an app page is slow then you can
      insert calls to get_time() (usually via execute function) at various
      points in the app page and see how much time elapses between each call.
    </td>
  </tr>
</table>



<h3 class="zdoc"><a name="Java">Java</a></h3>

<p>Kevin?

<h4 class="zdoc"><a name="Applets">Applets</a></h4>

<h4 class="zdoc"><a name="Browser">Browser / Mutant Search Applet</a></h4>

<p>The Browser applet currently runs within select_structures.cgi, which 
is called by fishselect.apg.  The applet essentially acts as an extra fancy
html style select box, allowing the user to select anatomical keywords. 
The keywords are taken from the 'anatomical_parts' table (which was built
from a text file) fed to an applet param by select_structures.cgi.

<h4 class="zdoc"><a name="Annotator">Annotator</a></h4>

<p>The Annotator isn't technically an applet any longer, now it's run either
as a stand-alone application on a client machine, or in batch mode to combine
images with annotation text files.

<p>To run the Annotator in batch mode:<div class="shellline">

   % java -cp annotator.jar Annotator imageName.jpg
</div>

<p>The Annotator will expect a text file named imageName.txt, and the resulting 
combined image will be written as imageName--C.jpg.

<p>To run the annotator interactively, on a machine with a command line the
correct java syntax is 
<span class="shell">java -cp annotator.jar Annotator</span>.  Our
initial Annotator user is using a Macintosh, which adds a layer of difficulty.

<p>To package the Annotator for use on a Mac ... 

<h4 class="zdoc"><a name="MapViewer">MapViewer</a></h4>


<p>However, as of 2001/05, the 
<a href="db.html#Fish_Mutants">current implementation of double mutants</a>
is so badly broken that a missing ChromoUpdate process seems like a light 
breeze in the middle of hurricane.


<h4 class="zdoc"><a name="JDKs and Class Libraries">JDKs and Class Libraries</a></h4>

<p>Kevin, do you want to tackle this?


<h4 class="zdoc"><a name="Javaserver">Javaserver</a></h4>

<p>Many of the applets need access to the database.  At the time the applets
were written Illustra did not support JDBC, a standard mechanism for having
Java programs access SQL databases.  Ted wrote the <i>javaserver</i> to 
fill this
need.  The javaserver was a server side process with its own port that 
emulated a JDBC server.  The javaserver process/code resided in the
server_apps directory.

<p>Informix, however, supports JDBC, and on 2003/01/13, Kevin completed the
conversion to JDBC and ZFIN was able to finally put the much maligned 
javaserver to rest.

<!-- =================================================================== -->
<!-- ========= DATABASE IMPLEMENTATION ================================= -->
<!-- =================================================================== -->

<h1 class="zdoc"><a name="Database Implementation">Database Implementation</a></h1>

<p>This section is a trash can for all of the details about the ZFIN
database that don't fit well into other sections of the ZFIN documentation.
Here is a list of database related documentation that is located elsewhere
in the ZFIN documentation.

<ul>
 <li><a href="intro.html">Introduction</a>
  <ul>
   <li><a href="intro.html#Database Documentation">How to update the database 
       documentation</a>.
  </ul>
 <li><a href="dst.html">Data Submission and Transfer</a>
  <ul>
   <li><a href="dst.html#Automated Data Transfers">Automated Data Transfers</a>
       describes how bulk loads into and out of the database happen.
  </ul>
 <li><a href="db.html#">Database Design</a> 
     Describes the logical design of the ZFIN database at a high level.
  <ul>
   <li><a href="db.html#Schema">Database Schema</a>.  This provides
       detailed descriptions of tables and columns, as well as schema diagrams.
   <li><a href="db.html#Naming Conventions">Naming Conventions</a>.
   <li><a href="db.html#ZDB IDs and ZDB Object Types">ZDB IDs and ZDB Object 
       Types</a>
   <li><a href="db.html#Database Cleaning and Validation">Database 
       Cleaning and Validation</a>
   <li><a href="db.html#Database Functions">Database Functions</a>
  </ul>
 <li><a href="impl.html">Implementation and Development</a> (this document) 
  <ul>
   <li><a href="impl.html#Machines and Informix Engines">Machines and Informix Engines</a>
   <li><a href="impl.html#Web Site / Database / Machine Matrix">Web Site / Database / Machine Matrix</a>
   <li><a href="impl.html#Informix Environment Variables">Informix Environment Variables</a>
  </ul>
 </li>
 <li><a href="standards.html">Coding Standards and Best Practices</a>
  <ul>
   <li><a href="standards.html#Database">Database</a></li>
   <li><a href="standards.html#Temporary Tables in App Pages">Temporary Tables in App Pages</a></li>
  </ul>   
 </li>
 <li>System, Web Server, and 
     <a href="admin.html#Database Administration">Database Administration</a>.  
  <ul>
   <li><a href="admin.html#Products and Licenses">Products and Licenses</a>
   <li><a href="admin.html#Dealing With IBM Corporation">Dealing With IBM Corporation</a>
   <li><a href="admin.html#Backups">Backups</a>
   <li><a href="admin.html#Monitoring">Monitoring</a>
   <li><a href="admin.html#Fixing Immediate Problems">Fixing Immediate Problems</a>
   <li><a href="admin.html#Disk Usage">Disk Usage</a>
   <li><a href="admin.html#Physical Design">Physical Design</a>
  </ul>
 </li>
</ul>


<!-- ========= INFORMIX ================================================ -->

<h2 class="zdoc"><a name="Informix">Informix</a></h2>



<h3 class="zdoc"><a name="Informix Settings">Informix Settings</a></h3>

<p>There are several environment variables that have to be set in order for
Informix to work at all.  Those variables are discussed in the 
<a href="#Informix Environment Variables">Informix Environment Variables</a>
section.  This section discusses additional settings that are not strictly
required, but are darn handy.  Some of these can be set as environment 
variables, some can be set in SQL, and some can be set both ways.

<p>In addition, there is a database function, 
<a href="db.html#Infrastructure Functions">set_session_params()</a>, that you
can call from SQL to set these.

<table class="zdoc">
  <caption>Informix Settings</caption>
  <tr>
    <th>How to set it</th>
    <th>Set by ssp<sup>1</sup></th>
    <th>Explanation</th>
  </tr>
  <tr>
    <td>
      Shell:&nbsp;<span class="shell">setenv&nbsp;PDQPRIORITY&nbsp;HIGH</span><br>
      SQL:&nbsp;<span class="sql">set&nbsp;pdqpriority&nbsp;high;</span>
    </td>
    <td>Yes</td>
    <td>Tells Informix to use as many CPUs as possible to process the query.</td>
  </tr>
  <tr>
    <td>SQL:&nbsp;<span class="sql">set&nbsp;lock&nbsp;mode&nbsp;to&nbsp;wait&nbsp;5;</span></td>
    <td>Yes</td>
    <td>
      Tells Informix to wait 5 seconds before it gives up on getting a lock.
      The default behavior is for Informix to give up immediately if it can't
      get a lock.
    </td>
  </tr>
  <tr>
    <td>SQL:&nbsp;<span class="sql">set&nbsp;isolation&nbsp;to&nbsp;dirty&nbsp;read;</span>
    <td>Yes</td>
    <td>
      Tells Informix to read rows that have update locks on them without
      waiting for the rows to become available.  This means that transactions
      may occassionally see phantom rows.  Updates will still have to wait
      for rows to become available.  We use dirty read because it leads to
      much higher throughput, and the consequences of reading phantom rows
      in ZFIN are inconsequential.
    </td>
  </tr>
</table>

<p>Notes
<p><sup>1</sup> "ssp" expands to
<a href="db.html#Infrastructure Functions">set_session_params()</a>.

    

<h3 class="zdoc"><a name="Informix Tools">Informix Tools</a></h3>

<h4 class="zdoc"><a name="dbaccess"><kbd>dbaccess</kbd></a></h4>

<p><kbd>dbaccess</kbd> is a Unix program for accessing Informix databases.
There are many different ways to run <kbd>dbaccess</kbd>.  Here are a few 
of them.

<ul>
  <li>You can pipe SQL statements into <kbd>dbaccess</kbd> using 
    <kbd>echo</kbd>:
    <div class="shellline">  % echo 'select count(*) from marker' | dbaccess <i>dbname</i></div>
    This is useful for short queries and in scripts.  It is less useful when
    queries are longer, or you are running multiple queries.
    <br><br>
  </li>
  <li>You can put your SQL statements in a file, and then run them from 
    a file:
    <div class="shellline">  % dbaccess <i>dbname</i> <i>filename</i>.sql</div>
    The filename must end with a <kbd>.sql</kbd> extension.
    <br><br>
  </li>
  <li>You can run dbaccess as an interactive utility.  First type
    <div class="shellline">% dbaccess <i>dbname</i></div>
    This throws up a menu interface that looks like this:
    <div class="shell">
  DBACCESS: Query-language Connection Database Table Session Exit
  Use SQL query language.
    </div>
    To pick one of the menu items either type the capitalized letter of the
    menu item, for example <kbd>q</kbd> for Query-language, or arrow over to
    the menu item you want and hit return.  To run SQL queries type 
    <kbd>q</kbd>.  This changes the menu to:
    <div class="shell">
  SQL: New Run Modify Use-editor Output Choose Save Info Drop Exit
  Enter new SQL statements using SQL editor.
    </div>
    You now have at least 2 choices for enterring queries, either New and Modify
     or use-editor.   New and Modify take you into <kbd>dbacess</kbd>'s native 
    query editor, which is truly heinous.  Use-editor takes you into whatever 
    editor you specify with the $DBEDIT environment variable.  Set DBEDIT 
    in your .cshrc file:
    <div class="shellline">  setenv DBEDIT /loacl/bin/emacs</div>
  </li>
</ul>

<p><b>Transactions in <kbd>dbaccess</kbd></b>:  Unless you explicityly start a 
transaction with a <kbd>begin work</kbd> statement, each individual query
runs in its own transaction.  

<p><b>Errors in <kbd>dbaccess</kbd></b>:  If you are using <kbd>dbaccess</kbd>
interactively then it will stop query execution after any error condition.
If you are running it non-interactively, then it reports errors, but 
continues to execute queries until the end of the input.  If you want a 
non-interactive run to abort after an error, then set this environment variable:
<div class="shellline">  setenv DBACCNOIGN 1</div>


<h4 class="zdoc"><a name="dbschema">dbschema</a></h4>

<p><kbd>dbschema</kbd> is a Unix utility for displaying the schema of an
Informix database.  To find out all the options run <kbd>dbschema</kbd>
without any options:
<div class="shellline">  % dbschema</div>
The usage and options are:
<div class="shell">
    dbschema [-q] [-t tabname] [-s user] [-p user] [-r rolename] [-f procname]
             [-hd tabname] -d dbname [-w passwd] [-seq sequence]
             [-u [ia] udtname [all]] [-it [Type]] [-l [num]] [-ss] [filename]

    -q      Suppress the db version from header

    -t      table name or "all" for all tables

    -s      synonyms created by user name
            or "all" for all users

    -ss     generate server specific syntax
    
    -seq    generate sequence specific syntax


    -p      permissions granted to user name
            or "all" for all users

    -w      database password

    -r      create and grant of the role 
            or "all" for all roles :Not a valid option for SE

    -u      Prints the definitions of user-defined data types

    -ui     Prints the definitions of user-defined data types,
            including type inheritance

    -ua     Prints the definitions of user-defined data types,
            including all functions and casts defined over a type

    -u all  Directs dbschema to include all the tables
            in the display of distributions


    -f      SPL routine name
            or "all" for all SPL routines

    -d      database name

            filename is the name
            of file that the SQL
            script goes in.

    -hd     Histograms of the distribution for columns of
            of a specified table, a specific table column,
            or "all" for all tables.
    
    -it     Type of isolation can be DR, CR, CS or RR

    
    -l      set lock mode to wait [number] optional
</div>


<h4 class="zdoc"><a name="finderr">finderr</a></h4>

<p><kbd>finderr</kbd> is a Unix utility for displaying the meanings of 
Informix error numbers.  It is useful with both ISAM and SQL errors:
<div class="shellline">% finderr <i>err_number</i></div>



<!-- ========= ZFIN DATABASE TOOLS====================================== -->

<h2 class="zdoc"><a name="ZFIN Database Tools">ZFIN Database Tools</a></h2>

<p>In addition to the <a href="#Informix Tools">Informix database tools</a>,
there are a number of homegrown ZFIN database tools that are useful as well.



<h3 class="zdoc"><a name="Loading Databases">Loading Databases</a></h3>

<p>The database on the production server is backed up every night.  
This backup is useful for restoring the database in case of serious corruption
or loss of data.  However, it is only useful for restoring the production
database. 

<p>We have a separate mechanism for copying data from one server to another
(usually from production to test or from test to test).  We use the 
<kbd><a href="http://cvs.zfin.org/cvs/cvsweb.cgi/Commons/bin/unloaddb.pl">unloaddb.pl</a></kbd>
and <kbd><a href="http://cvs.zfin.org/cvs/cvsweb.cgi/Commons/bin/loaddb.pl">loaddb.pl</a></kbd> scripts to do this.  
These scripts reside in the
<a href="#ZFIN Commons">ZFIN Commons</a> bin directory 
and are callable by any member of the group.

<p>A <a href="admin.html#cron">cron job</a> 
unloads the production database 5 nights a week.

<p>The nightly unloads are put in this directory:

<div class="fileline">
<a href="#ZFIN Unloads">/research/zunloads/databases/zfindb</a>
</div>


<p><kbd>unloaddb.pl</kbd> creates a whole directory structure to contain 
each unload.
Directories names have the form:

<pre>    YYYY.MM.DD.N</pre>


<h4 class="zdoc"><a name="loaddb.pl">loaddb.pl</a></h4>

<p>To load a database into the test server you will first need to define
the <a href="#Informix Environment Variables">Informix environment variables</a>
for the server you are loading the database into.

<p>You can then load the data using <kbd>loaddb.pl</kbd>.  It has the syntax:

<div class="shell">
  % loaddb.pl [-ee | -e <i>tablename</i> ... ] <i>dbname unload_directory</i>
</div>
<p>Where <kbd><i>dbname</i></kbd> is probably one of the 
<a href="#Web Site / Database / Machine Matrix">standard database 
names</a>, and the <kbd><i>unload_directory</i></kbd> 
is a directory containing a particular
dump of the production database.  

<p>The <kbd>-e <i>tablename</i></kbd> option allows you to exclude a table from the
load.  If you want to exclude more than one table, use the <kbd>-e</kbd> option.
In practice, the tables that are excluded are always
<a href="Database/DataModel/index.htm">DEFLINE and ACCESSION_BANK</a>, two huge 
tables that are useful for 
processing bulk loads from other databases, but that are not used directly 
in the web site.  Excluding them saves lots of disk space and cuts about 6
minutes off the load time.  

<p>For example, to load the 2002/12/05 
version of the production database into clemdb we would type:

<div class="shell">
  albino Projects% loaddb.pl -ee clemdb \
         /research/zunloads/databases/zfindb/2002.12.05.1
</div>

<p><kbd>loaddb.pl</kbd> goes through the following steps:

<ol>
  <li>If the database (clemdb in the example) already exists it is 
    <b>deleted</b>.
  </li>
  <li>The database is created in the server, using the schema that existed in 
    on the source server (production in this example) on the date of the unload.
  </li>
  <li>All constraints, indexes and triggers are disabled.</li>
  <li>The data in all (non-excluded) tables is loaded.</li>
  <li>All constraints, indexes and triggers are enabled.</li>
  <li>Finally, logging is enabled in the database.</li>
</ol>

<p><b>Immediately, after loading data from another database, you should go
to the top of your development directory and make the <a href="#postloaddb">
postloaddb</a> target:</b>

<div class="shellline">
  albino ZFIN_WWW% gmake postloaddb
</div>

<p>This is necessary because the database you just loaded contains the 
app pages, <a href="db.html#Database Functions">database functions</a>, 
and <a href="Database/DataModel/index.htm">EXECWEB</a> table as they
existed on the source database (most likely production) on the day the
unload was done.  If you were to try to use your development web site
immediately after a load, you would find that it either wouldn't
work, or it would be pointing you to the production web site.

<p>The postloaddb target removes all of these things from your database
and then replaces them with versions that use your test web site.

<p>As of 2003/05, the loaddb.pl script is getting painfully slow as the 
amount of data increases.  We will probably replace it with the Informix
High Performance Loader (HPL) before too much longer.




<h3 class="zdoc"><a name="Scan All Character Columns for a String (scancharcols.pl)">
             Scan All Character Columns for a String (scancharcols.pl)</a></h3>

<p>ZFIN has a script,

<div class="fileline">
<a href="http://cvs.zfin.org/cvs/cvsweb.cgi/Commons/bin/scancharcols.pl">/private/ZfinLinks/Commons/bin/scancharcols.pl</a>
</div>

<p>that searches <i>all</i> of the character fields (excluding CLOBs) in the
database looking for a
string.  It reports back any rows that it found that contained the target
string.  The script has like and lower options for doing pattern matching
and case-insensitive searches.  See the script for details.



<h3 class="zdoc"><a name="Scan All Character Columns for Illegal Characters (scrubscan.pl)">
             Scan All Character Columns for Illegal Characters (scrubscan.pl)</a></h3>

<p>ZFIN has a script,

<div class="fileline">
<a href="http://cvs.zfin.org/cvs/cvsweb.cgi/ZFIN_WWW/server_apps/DB_maintenance/scrubscan.pl">ZFIN_WWW/server_apps/DB_maintenance/scrubscan.pl</a>
</div>

<p>that searches all CHAR and VARCHAR fields for illegal characters (such as 
control characters), and legal characters in illegal places 
(leading and trailing spaces) or in illegal combinations (double spaces).
The script reports what columns had bad data in them.  It is run every night by 
a cron job, but can also be run from the command line.

<p>
Use the following in a SQL statement to find affected rows.  Replace 'column'
with the column of interest. 

<p>TrailSpace:length(column) <> octet_length(column) and column <> \" \",
<br>ScrubChar:column <> scrub_char(column)

<h3 class="zdoc"><a name="sysexec, EXECWEB, and unix_commands">
             sysexec, EXECWEB, and unix_commands</a></h3>

<p>Occasionally, an app page will need to call a Unix 
executable to either get a value, or to affect some change in the system.

<p>In ZFIN we use the 
<a href="db.html#Infrastructure Functions"><kbd>sysexec()</kbd></a> 
<a href="db.html#Database Functions">database function</a> to do this.  
<kbd>sysexec()</kbd> takes 2 parameters:
<br<br>
<table class="definition">
  <tr>
    <th>Parameter
    <th>Description

  <tr>
    <td class="term"><kbd>key</kbd>
    <td>A unique identifier for the Unix command/script to execute.

  <tr>
    <td class="term"><kbd>args</kbd>
    <td>The arguments to pass to the command.  This is generally a quoted
      string containing a list of space separated arguments.

</table>

<p><kbd>sysexec()</kbd> uses the <kbd>key</kbd> parameter to get a row from the
<a href="Database/DataModel/index.htm">EXECWEB</a> table
in the database.  The EXECWEB table has two columns.  One contains
the unique ID by which the Unix command is known in ZFIN, and the other
contains the path to the command or script to execute.  
<kbd>sysexec()</kbd> invokes
the path from the matching row in the EXECWEB table and passes the 
<kbd>args</kbd> parameter as the command's argument string.

<P>There are currently (2001/05) these entries in the EXECWEB table:
<br><br>
<table class="definition">
  <tr>
    <th>Key
    <th>Path to Command, and
      <br>Command Description

  <tr>
    <td class="term">encryptpass
    <td><kbd><i>$TARGETROOT</i>/server_apps/sysexecs/encryptpass/encryptpass</kbd>
      <br>Used to encrypt user passwords.

  <tr>
    <td class="term">get_image_stats
    <td><kbd><i>$TARGETROOT</i>/server_apps/sysexecs/image_stats/get_image_stats.pl</kbd>
      <br>Given an image file this returns the width and height of the image 
        in the file, in pixels.  This is called by the 
	<a href="db.html#Image Functions"><kbd>get_image_stats()</kbd></a>
	<A href="db.html#Database Functions">database function</a>.
	This script uses the <a href="admin.html#Netpbm">Netpbm</a> package.

  <tr>
    <td class="term">make_thumbnail
    <td><kbd><i>$TARGETROOT</i>/server_apps/sysexecs/make_thumbnail/make_thumbnail.sh</kbd>
      <br>Creates a thumbnail of an image.
	This script uses the <a href="admin.html#Netpbm">Netpbm</a> package.

  <tr>
    <td class="term">rm
    <td><kbd>/bin/rm</kbd>
      <br>Used when ZFIN needs to delete a file on the server.

</table>

<P>The rm command is an example of a simple unix command that is invoked 
directly by ZFIN.  ZFIN invokes such commands without a script
surrounding the call to the command.  Which Unix commands can be invoked 
directly by ZFIN is defined in 
<kbd>ZFIN_WWW/server_apps/sysexecs/unix_commands/Makefile</kbd>.



<!-- ========= ODBC CONNECTIONS ======================================== -->

<h2 class="zdoc"><a name="ODBC Connections">ODBC Connections</a></h2>
<p>ODBC is a good way for windows users to connect to our Informix Database.
The following recipe describes how to set up an ODBC connection to a test 
database.  Settings used to connect to test databases on wanda or wavy are 
different than those used to connect to our production database on wildtype.
<br><br>
To configure ODBC:
<br><br>
go to: control panel<br>
administrative tools<br>
Data Sources (ODBC)<br>
Click Add<br>
<br><br>
For Wanda<br>
User DSN<br>
General<br>
Data Source Name : name_you_choose (I said hoover since my databases is 
hoovdb)<br>
Description: description_you_choose (null)<br>
<br><br>
Connection<br>
Server Name :  wanda<br>
Host Name :  embryonix.cs.uoregon.edu<br>
Service : 2002<br>
Protocol : onsoctcp<br>
Options : Null<br>
Database Name : your_dbname (hoovdb)<br>
User Id : your_loginname (staylor)<br>
Password : your_password (*)<br>
<br><br>
tips: put it your username and password before selecting the Database Name
or else it will try to establish a connection w/o username and password
and it will fail.  If it is configured correctly, after userid and
password are in, it should give a pull down list of dbs to connect to:
including sysmaster for wanda, and all of the user databases.
<br><br>
click Apply and Test Connection<br>
if the test does not come back in 2 sec then one of the settings are incorrect.
<br><br>




<!-- =================================================================== -->
<!-- ========= UNIQUERY ================================================ -->
<!-- =================================================================== -->

<h1 class="zdoc"><a name="Uniquery">Uniquery</a></h1>

<p>
    The site-wide search feature is implemented using the open-source java-based Lucene 
    search engine. Lucene defines the underlying index format and functionality for 
    adding to and querying the index. It does not, however, provide any ready-built 
    spidering tool or any front-end for querying. We have implemented custom applications 
    that use the Lucene engine to perform these tasks. More information about Lucene can 
    be found at 
    <a href="http://lucene.apache.org/java/docs/index.html">http://lucene.apache.org/java/docs/index.html</a>
</p>


<!-- ========= IndexApp ================================================ -->

<h2 class="zdoc"><a name="IndexApp">IndexApp</a></h2>
<p>
    The IndexApp is responsible for spidering the website to build the index that we search 
    against. It is configured with a list of URLs to start from, it requests each URL and 
    parses the page that is returned, indexing the content and adding all of the new URLs 
    that it finds to its list of URLs to parse. It only spiders URLs pointing to the same 
    host it is working on which keeps it from attempting to spider the entire Internet. It
    is based on code from the spindle library from 
    <a href="http://www.bitmechanic.com">www.bitmechanic.com</a>, originally written 
    by James Cooper.
</p>



<h3 class="zdoc"><a name="Runtime Options">Runtime Options</a></h3>
<table border="0">
    <tr>
        <td class="term"><b>-d <i>dirname</i></b></td>
        <td>
            directory to create the index in (required)
        </td>
    </tr>
    <tr>
        <td class="term"><b>-u <i>filename</i></b></td>
        <td>
            url file (required). This is a file containing the list of URLs to
            index (one per line). Note that the spider won't discover html forms 
            on web pages, so if the output of a form needs to be indexed it 
            should be constructed as a GET request with appropriate parameters 
            and added to this file. We currently don't have any way to index 
            the output of a form that only supports POST input.
        </td>
    </tr>
    <tr>
        <td class="term"><b>-i <i>filename</i></b></td>
        <td>
            include keywords. This is a file of keywords (one per line) that 
            URLs must include to be processed. We currently do not use this option.
        </td>
    </tr>
    <tr>
        <td class="term"><b>-e <i>filename</i></b></td>
        <td>
            exclude keywords. This is a file of keywords (one per line) that URLs 
            must not contain or they will be excluded from processing. We currently 
            use this option to prevent pages like view_mapplet from being crawled
        </td>
    </tr>
    <tr>
        <td class="term"><b>-c <i>filename</i></b></td>
        <td>
            crawl-only keywords. This is a file of keywords (one per line) that 
            will cause a page to be crawled only and not indexed if they are present 
            in the URL. We currently use this option to keep form results pages from 
            being indexed. We want the search results crawled because they point to 
            the interesting pages, but the lists of results themselves are uninteresting.
        </td>
    </tr>
    <tr>
        <td class="term"><b>-a</b></td>
        <td>
            append to index. This option allows incremental additions to indexes, 
            but has not been tested and is currently unused.
        </td>
    </tr>
    <tr>
        <td class="term"><b>-m <i>filename</i></b></td>
        <td>
            mime types. This is a file of mime-types (one per line) to be indexed. Pages 
            that return a mime-type not in this list will be ignored. This option is currently unused.
        </td>
    </tr>
    <tr>
        <td class="term"><b>-t <i>num</i></b></td>
        <td>
            thread count. Defines the number of threads to use concurrently when spidering 
            the site. Currently set to 4.
        </td>
    </tr>
    <tr>
        <td class="term"><b>-l&nbsp;<i>directoryname</i></b></td>
        <td>
            logging directory. provide a relative or absolute path to the directory where
            log files are to be placed (the directory must exist already). If not specified
            defaults to logging in the run directory.
        </td>
    </tr>
    <tr>
        <td class="term"><b>-v</b></td>
        <td>
            verbose mode. When the flag is included, every url that is logged as it is
            indexed, crawled or ignored. This can generate a substantial amount of logging
            (~40 MegaBytes per run) so can be turned off. Regardless of whether this flag
            is present or not, errors will be logged to the log directory.
        </td>
    </tr>
    
    
</table>



<h3 class="zdoc"><a name="IndexApp Implementation Notes">Implementation Notes</a></h3>
<p>
    The spider class is derived from an existing one but has some ZFIN specific enhancements.
</p>
<p>
    <b>Index Fields</b> Documents are indexed as follows:<br>
</p>
    <table border="0">
        <tr>
            <td class="term"><b>url</b></td>
            <td>
                the url is stored in the indexed so it can be used in the results but is not
                indexed and is not searchable against
            </td>
        </tr>
        <tr>
            <td class="term"><b>body</b></td>
            <td>
                we index the body of the page and store the full contents in the index so
                that we can use it to display highlighted text in the results
            </td>
        </tr>
        <tr>
            <td class="term"><b>title</b></td>
            <td>
                we index the title and store it in the database so it can be used when displaying 
                results, but we don't currently search against it.
            </td>
        </tr>
        <tr>
            <td class="term"><b>type</b></td>
            <td>
                see below (document boosting) for details
            </td>
        </tr>
    </table>
<p>
    <b>Document Boosting</b> Some ZFIN pages should have higher precedence in the results screens 
    that other pages. To support this, the spider examines the URL for each page it indexes and 
    adds a type field to the index based on what it finds. Check the code for the most current 
    list though, note that any changes to the list in the spider code will require the SearchApp 
    to be updated as well.
</p>
<p>
    <b>Logging</b> The logging behavior has been changed considerably from the original Spider code. 
    This was primarily for development and profiling purposes. 
</p>



<h3 class="zdoc"><a name="IndexApp Classes">Classes</a></h3>
<p>
    <b>Spider</b> The main class, this is where all the logic is. Each page is parsed as 
    it is spidered and only the actual content of the page is indexed, the HTML tags themselves 
    are ignored.
</p>



<!-- ========= UniquerySupport ========================================= -->

<h2 class="zdoc"><a name="UniquerySupport">UniquerySupport</a></h2>
<p>
    This project contains shared code used in both the SearchApp and IndexApp projects.
</p>



<h3 class="zdoc"><a name="UniquerySupport Classes">Classes</a></h3>
<p>
    <b>ZfinAnalyzer</b> Implements a very simple analyzer. It uses the WhiteSpace tokenizer, 
    the LowerCase filter, and a simple list of StopWords.
</p>
<p>
    <b>SearchCategory</b> Represents a category of search results. We split search results
    into different categories (Genes/Markers/Clones, Anatomy, etc.). The SearchCategory class 
    has a list of all top-level categories as well as the types within each category. 
    Genes/Markers/Clones is a category, it is then broken down into the different sub-types so
    that they can be sorted in order.
</p>



<!-- ========= SearchApp =============================================== -->

<h2 class="zdoc"><a name="SearchApp">SearchApp</a></h2>
<p>
    The SearchApp implements the front-end searching functionality. It consists of one primary
    JSP page for displaying results and a back-end java bean that interfaces with the Lucene index.
</p>
<p>
    <b>Clumsy JSP code</b> The JSP code does not strictly adhere to best practices. 
    A better implementation of the JSP pages would use custom app tags or perhaps a 
    framework like Struts. However, it was deemed that the overhead of adding that 
    sort of infrastructure was greater than the benefit from it. Again, if the app 
    grows in scope this is a decision that will need to be revisited, the decision 
    only applies because of the small size of the app.
</p>


<h3 class="zdoc"><a name="Configuration">Configuration</a></h3>
<p>
    The search app takes a single configuration parameter: path_to_index that contains the 
    absolute path to the Lucene index directory.
</p>



<h3 class="zdoc"><a name="SearchApp Implementation Notes">Implementation Notes</a></h3>
<p>
    The implementation of the SearchApp is relatively straightforward though perhaps a bit 
    inelegant.
</p>
<p>
    <b>Document Boosting</b> document boosting by type is implemented by prefixing the query 
    string with a query by type. This looks a bit weird but is the suggested method of 
    implementing document type boosting as per the Lucene FAQ.
</p>
<p>
    <b>Highlighted Text</b> the highlighter is used to highlight portions of the page that 
    contain found query results. This is one of the reasons that the SearchResults and Hit 
    classes were created. Note that SearchResults are not cached between requests, this 
    keeps the code simple and while it may not be the most efficient implementation it 
    still performs adequately.
</p>



<h3 class="zdoc"><a name="SearchApp Classes">Classes</a></h3>
<p>
    <b>SearchBean</b> The main class that does the searches. There are two search types, 
    one that just returns a result count (used for the summarized results page) and 
    another that returns detailed hits with the results terms highlighted.
</p>
<p>
    <b>Hit</b> Represents a single hit, created so that we could hang on to the highlighted 
    text.
</p>
<p>
    <b>SearchResults</b> Represents the entire results list. Has some support for paging behavior.
</p>



<h3 class="zdoc"><a name="Deployment to Development">Deployment to Development</a></h3>
<p>
    The SearchApp is deployed to Tomcat running behind Apache. For more information on Tomcat 
    go to <a href="http://jakarta.apache.org/tomcat/">http://jakarta.apache.org/tomcat/</a>
</p>
<p>
    Apache forwards requests to Tomcat using mod_proxy and Tomcat is currently configured 
    so that it will only respond to requests from localhost meaning that you cannot hit 
    the JSP page without going through Apache.
</p>
<p>
    Every developer has their own virtual host configured in Apache and every developer 
    can have their own copy of the SearchApp running in Tomcat.
</p>
<p>
    To get a new virtual host working with the j2ee SearchApp , perform the following steps:
</p>
    <ol>
        <li>
            Edit the http.conf file to add proxy configuration inside the desired virtual 
            host configuration section. Add the following 3 lines to the configuration:
            <div class="shell">
&lt;IfModule mod_proxy.c&gt;
  ProxyRequests Off

  # Uniquery redirct to Tomcat.
  ProxyPass /SearchApp/ http://localhost:8080/SearchApp_quark/
  ProxyPassReverse /SearchApp/ http://localhost:8080/SearchApp_quark/
&lt;/IfModule&gt;
            </div>
        </li>
        <li>
            Add host specific tomcat configuration file SearchApp_<i>hostname</i>.xml in 
            /private/apps/tomcat/conf/Catalina/localhost/
            
            To configure the virtual host quark, you would create a new configuration file 
            /private/apps/tomcat/conf/Catalina/localhost/SearchApp_quark.xml with the content: 
            <div class="shell">
&lt;Context 
  path="/SearchApp_quark"
  docBase="/research/zfin/central/www_homes/quark/server_apps/j2ee/SearchApp" 
  debug="0" privileged="false"&gt;
    &lt;Logger className="org.apache.catalina.logger.FileLogger"
            prefix="SearchApp_quark_log"
            suffix=".txt"
            timestamp="true"/&gt;
&lt;/Context&gt;
            </div>
            This will create an instance of the SearchApp that points to the shared development 
            index file. if you need to generate or test your own index, you can point to a 
            specific one by adding the path_to_index parameter in the context like so:
            <div class="shell">
&lt;Context 
  path="/SearchApp_quark"
  docBase="/research/zfin/central/www_homes/quark/server_apps/j2ee/SearchApp" 
  debug="0" privileged="false"&gt;
    &lt;Parameter 
        name="path_to_index"
        value="/research/zfin/central/indexes/index_quark" 
        override="true"/&gt;
    &lt;Logger className="org.apache.catalina.logger.FileLogger"
            prefix="SearchApp_quark_log"
            suffix=".txt"
            timestamp="true"/&gt;
&lt;/Context&gt;
            </div>
        </li>
        <li>
            Restart apache and tomcat. Note that apache does not normally need to be restarted 
            when tomcat is restarted and likewise tomcat does not normally need to be restarted 
            when apache is restarted. They both need to be restarted in this instance to pick 
            up changes to their configuration.      
        </li>
    </ol>



<!-- =================================================================== -->
<!-- ========= BLAST =================================================== -->
<!-- =================================================================== -->

<h1 class="zdoc"><a name="BLAST">BLAST</a></h1>

<p>BLAST (Basic Local Alignment Search Tool) is the most commonly used sequence alignment tool. It compares sequences with nucleotide and protein databases, and detects local as well as global alignments. ZFIN has a BLAST server running on an <a href="#BLAST Server">Apple G5 Xserve cluster</a> at

<div class="urlline">
  <a href="http://genomix.cs.uoregon.edu">http://genomix.cs.uoregon.edu</a>
</div>

<p>The <a href="http://gridengine.sunsource.net">Grid Engine</a> provides distributed computing and resource management.

<!-- ========= BLAST Version ======================================= -->

<h2 class="zdoc"><a name="BLASTVersion">BLAST Version</a></h2>
<p>
The BLAST Algorithm in use is <a href="http://blast.wustl.edu">WU-BLAST version 2.0, Gish W. (1996-2000)</a>. ZFIN obtained academic license, and have access to the latest release at http://blast.wustl.edu/blast/dist/ZFIN_LICENSE_KEY. Peiran keeps the license key. 
</p>
<p>
To update BLAST, download blast2.macoxs-g5.tar.Z package and extract it at /common/zfin/wublast/:

 <div class="shell">
   cd /common/zfin/wublast
   wget http://blast.wustl.edu/blast/dist/ZFIN_LICENSE_KEY/blast2.macoxs-g5.tar.Z
   gunzip blast2.macoxs-g5.tar.Z
   tar xvf blast2.macoxs-g5.tar   
 </div>
</p>

<!-- ========= Environment Variables ===================================== -->

<h2 class="zdoc"><a name="Environment Variables">Environment Variables</a></h2>
<p>
To have the WUBLAST running, several environment variable need to be set. Under
/common/zfin directory, we created an etc/ subdirectory and defined zfin.bashrc and
zfin.cshrc.login files with the following content:
<p>
zfin.bashrc
 <div class="shell">
   export PATH="/common/zfin/wublast:/common/bin:/bin:/sbin/:/usr/bin:/usr/sbin:\
                /common/sbin:/usr/local/bin:/usr/local/sbin"
   export MANPATH="/usr/share/man:/common/man:/usr/local/man"
   export WUBLASTMAT=/common/zfin/wublast/matrix
   export WUBLASTFILTER=/common/zfin/wublast/filter
   export BLASTDB=/private/blastdb/wu-db
   . /common/sge/default/common/settings.sh
 </div>

<p>
zfin.cshrc.login
 <div class="shell">
   setenv PATH "/common/zfin/wublast:/common/bin:/bin:/sbin/:/usr/bin:/usr/sbin:\
                /common/sbin:/usr/local/bin:/usr/local/sbin"
   setenv MANPATH "/usr/share/man:/common/man:/usr/local/man"
   setenv WUBLASTMAT "/common/zfin/wublast/matrix"
   setenv WUBLASTFILTER "/common/zfin/wublast/filter"
   setenv BLASTDB "/private/blastdb/wu-db"
   source /common/sge/default/common/settings.csh
 </div>
<p>
In the same directory, there are bashrc and cshrc.login files which source bioteam's 
configuration file and zfin's configuration file. These two files are copied over to
the /etc directory at the head node as well as each compute node. 


<!-- ========= BLAST Files and BLAST Databases ================================== -->

<h2 class="zdoc"><a name="Files and Databases">BLAST Files and BLAST Databases</a></h2>
<p>
In this documentation, the FASTA format sequence file used to format a BLAST database is
referred to as a BLAST file. A BLAST database consists of four files with extensions,  
 .xni for index file, .xnd for definition file, .xnt for table file and .xns for actual 
sequence file in the case of a nucleotide database. For a protein databas, the extensions
 are .pni, .pnd, .pnt and .pns. 

<h3 class="zdoc"><a name="Format BLAST DBs">Format BLAST DBs</a></h3>
xdformat is a WUBLAST utility program that is used to produce a BLAST database in XDF 
(eXtended Database Format) from one or more input files in FASTA format. To find out all 
the options, run xdformat without any options:
 <div class="shell">
   % xdformat
 </div> 
The common usage and options are:
 <div class="shell">

  Create a database:
    xdformat [-p|-n] [options] fadb
    xdformat [-p|-n] -o xdbname [options] fadb...

  Append sequences to an existing database:
    xdformat [-p|-n] -a xdbname [options] fadb...

       -p          sequence type is protein
       -n          sequence type is nucleotide
       -o oname    output xdb name, default is stdout  
       -a aname    append sequences to an existing database
       -t title    database descriptive title
       -e lname    log file        
       -I          index sequence indentifiers while formatting
       -T[tag]     index all identifiers in the [tag] class
 </div>
Examples:
  <div class="shell">

  Create a database:
    xdformat -n -I -Tgb1 -t "Genbank Zebrafish database" -e xdformat_gb_zf.log -o gbk_gb_zf  gbk_gb_zf.fa

  Append sequences to an existing database:
    xdformat -n -e xdformat_gb_zf.log -a gbk_gb_zf  nc_gb_zf.fa ;

  </div>
<h3 class="zdoc"><a name="Retrieve FASTA Files">Retreive FASTA Files</a></h3>
xdget is a WUBLAST utility program that is used to retrieve sequences by identifier from an 
indexed XDF database and report the sequences in FASTA format. To fine out all the options,
run xdget without any options:
 <div class="shell">
   % xdget
 </div> 
The common usage and options are:
 <div class="shell">
  Retrieve sequences by identifier:
    xdget [-p|-n] [options] xdbname ids...

  Retrieve sequences by identifier, with identifiers read from file(s):
    xdget [-p|-n] -f [options] xdbname idfiles...

       -p          sequence type is protein
       -n          sequence type is nucleotide
       -o oname    output file name, default is stdout   
       -e lname    log file       
       -f          read identifiers from file(s)
       -T[tag]     restrict identifier lookups to those in the [tag] class
 </div>
Examples:
  <div class="shell">
  Retrieve sequence by identifier:
   xdget -p sptr_zf Q708S8
  Retrieve sequences by identifier, with identifiers read from file(s):
   xdget -n -f -Tgb1 -e fasta_retrieve.log -o output.fa gbk_zf_all accession.in

  </div>

<h3 class="zdoc"><a name="DBs Update">BLAST DBs Update</a></h3>

We maintains 23 nucleotide databases and 4 protein databases for in-house BLAST needs. 
The data sources include NCBI GenBank, Ensembl, RefSeq, SwissProt/TrEMBL, TIGR, NCBI Trace
 Archive, VEGA and ZFIN. 
<p>
ZFIN Morphonio database and ZFIN MicroRNA database are formatted from locally stored 
sequences and are updated nightly via a cron job. The rest of the databases are updated 
on a weekly basis via a cron job.
<p> 
The scripts that update and format the BLAST files are organized by source and invoked 
by a master script -- blastdbupdate.pl.

 <div class="shell">
          % ls /Users/peirans/DbScripts
 </div>
<p>
BLAST files are organized by source, and each data source has a time-stamp file named 
<i>source_name</i>.ftp whose content is the ftp site, data path and data file name 
as well as a log file named <i>source_name</i>.report. 

 <div class="shell">
          % ls /common/data/BLAST_files
 </div>
<p>
BLAST databases are distributed to all nodes for performance concern.
To view,
  <div class="shell">
          % ls /private/blastdb/Current
 </div> 
<p>
The master script is launched every Friday night 6pm. It reads the content of the corresponding 
time-stamp file to figure out the data location and the file name of the new data which could be 
an incremental release or a whole new release. Then the script probes the new file, compares the 
time stamp with the <i>source_name</i>.ftp file. If there is new data, it invokes the specific 
script to download, process and format the new data, otherwise, it continues to test the next
data source. 
<p>
The updating process normally takes 3~4 hours with a large chunk of time consumed for data 
transfer and synchronization from head node to five compute nodes. Every 2 monthes, when a full 
GenBank release comes out, the process takes ~30 hours with ~24 hours for source files downloading. 

<!-- ========= BLAST Commands ================================== -->

<h2 class="zdoc"><a name="BLAST Commands">BLAST Commands</a></h2>
There are five BLAST programs, each of which in WUBLAST is a symlink of <i>blasta</i> executable.
<table class="zdoc">
<tr>
  <th>Program</th>
  <th>Description</th>
</tr>
<tr>
  <td>blastn</td>
  <td>Compares a nucleotide query sequence against a nucleotide sequence database</td>
</tr>
<tr>
  <td>blastp</td>   
  <td>Compares an amino acid query sequence against a protein sequence database</td>  
</tr>
<tr>
   <td>blastx</td>
   <td>Compares a nucleotide query sequence translated in all reading frames against a protein sequence database.</td>
</tr>
<tr>
   <td>tblastn</td>
   <td>Compares a protein query sequence against a nucleotide sequence database dynamically translated in all reading frames. </td>
</tr>
<tr>
   <td>tblastx</td>
   <td>Compares the six-frame translations of a nucleotide query sequence against the six-frame translations of a nucleotide sequence database. </td>
</tr>
</table>  
<p>   
To find out all options of each program, run the program with no parameter. For example,

 <div class="shell">
   % blastn
 </div> 

An example of command line execution:
 <div class="shell">
   % blastn my_database my_query.in -o my_result.out -e 1e-20 -filter dust
 </div> 

<h3 class="zdoc"><a name="Single node execution">Single node execution</a></h3>
Genomix has Grid Engine 6.0 in-use for resource management and load balancing. Jobs submitted to 
the Grid Engine are put into a queue. The scheduler identifies the least loaded node and dispatches
jobs in a FIFO manner as resource gets available. 
<p>
Two commonly used commands for submitting jobs to the Grid Engine are:
<table class="zdoc" align="center">
<tr>
  <td>qrsh</td>
  <td>submit an interactive rsh session to Grid Engine.</td>
</tr>
<tr>
  <td>qsub</td>
  <td>submit a batch job to Grid Engine. The job executes at the background and send output to files. </td>
</tr>
</table>
To find out all options and other Grid Engine User Commands, run:
 <div class="shell">
   % man qrsh
 or 
   % man qsub
 </div> 
Some options are:
  <div class="shell">
   -o path   The path used for the stdout stream of the job. 
   -N name   The name of the job. 
   -cwd      Execute the job from the current working directory.
   -V        Specifies that all environment variables active within the 
             untility be exported to the context of the job
   -now y/n  Specifies whether the job would be return as failure or 
             put into a pending queue in the case that the job could 
             not be started immediately. -now y is the default.
   -j y/n    Specifies whether or not the stderr stream of the job is 
             merged into the stdout stream.
 </div> 
<p>
To submit a blast job for single node execution, follow the examples:
 <div class="shell">
  To submit an interactive blast command:
    % qrsh -cwd -V -now no "blastn db_name query.in -e 1e-50 -o result.out"

  To submit a batch job, include the batch job into a shell script:
    % qsub -cwd batch_job.sh
  By default, the file name for the stdout has the form job_name.ojob_id
 </div> 

<h3 class="zdoc"><a name="Clustered execution">Clustered execution</a></h3>
Grid Engine supports single- as well as multiple-node jobs. <i>qsub</i> has a "-t"
option to submit a so called <i>Array Job</i>, i.e. an array of identical tasks being 
differentiated only by an index number and being treated by Grid Engine almost
like a series of jobs. 
 <div class="shell">
   -t n[-m[:s]]   The option arguments specify the number of array 
                  job tasks and the index number which will be associated 
                  with the tasks. The  index numbers will be exported to 
                  the job tasks via the environment variable SGE_TASK_ID.
                  The option arguments n, m, and s will be 
                  available through the environment variables SGE_TASK_FIRST,
                  SGE_TASK_LAST, and SGE_TASK_STEPSIZE. 
 </div>
<p>
To segment a single blast job and execute it with multiple nodes, use the wrapper
program <i>mblasta</i>. 

  <div class="shell">
  Usage:  mblasta [arguments]

  arguments:
     -p :   blast program
     -d :   target database
     -i :   query file
     -o :   ouput file (must be accessible from all nodes)
     -debug:   do not submit jobs, print intentions to STDOUT
     -dbsplit: # of segments of target db
     -isplit:  # of splitted query files

     (all blasta options apply)

  Example:  ./mblasta -p blastn -d gbk_gb_zf -i query.fa -o results -dbsplit 4

  </div>
<p>
Three ways to segment a blast job:
<ul>
  <li> segment the target database(s) only
  <li> segment the query sequences only 
  <li> segment both the target database(s) and the query sequences
</ul>
With 6 dual-CPU nodes, there are 12 job slots, and so 12 sub-jobs could be scheduled 
all at once. Some sub-jobs finish much faster than the rest. Based on experiments, 
we picked 16 as the default maxmum. 
<p>
If -dbsplit and -isplit are not set, the program detects the number of query sequences 
and use that as the isplit number. That number has a cap of 16. If the isplit number is 
less than 16, dbsplit is set to be the devision of 16 by isplit. For example, if there 
is 4 query sequences, isplit is 4, dbsplit is 4. If there is 3 query sequences, isplit
is 3 and dbsplit is 5. 
<p> 
If -isplit is set, use it unless it is greater than the actual number of query 
sequences in which case the total number would be used. Then calculate the dbsplit
accordingly as describ above unless -dbsplit is passed in. 
<p>
Batch blast query scheduled for clustered execution is highly suggested 
to run during off work hours. If it has to run during peak hours, please segment the 
job to less than 6 sub-jobs so that one to two nodes would be available for other 
interactive requests. 
<p>
<i>mblasta</i> calls <i>mblastamerge</i> which is a program that resembles segmented 
blast results and re-calculate the statistics. 

<!-- ========= Web-based BLAST ================================= -->

<h2 class="zdoc"><a name="Web-based BLAST">Web-based BLAST</a></h2>
BLAST execution is wrapped in a web application for a user friendly interface to 
submit blast jobs and view blast results. The URL is 
<a href="http://genomix.cs.uoregon.edu">http://genomix.cs.uoregon.edu</a>

<h3 class="zdoc"><a name="Execution">Execution</a></h3>
When the target blast database is a Zebrafish database (except for  
Zebrafish Trace Archive), the job is a single-node exeuction. When the target database
includes Human or Mouse database or Zebrafish Trace Archive, the job is segmented with 
the default 16 segments and executed on the whole cluster. 

<h3 class="zdoc"><a name="Executables">Executables</a></h3>
The HTML files are at 
 <div class="shell">
   /Library/WebServer/Documents/zfin
 </div>
The CGI scripts are at 
 <div class="shell">
  /Library/WebServer/CGI-Executables/zfin
 </div>

The script that converts plain text BLAST results into html format 
with graphical display, linkouts to data source and links to matching zfin records 
when available is at
 <div class="shell">
    /common/bin/blast2html
 </div>

<h3 class="zdoc"><a name="Results">Results</a></h3>
Each web execution of a BLAST job is given a job ID starts with "W", followed by 7 
digits of a random number. This job ID is used to create directory at 
 <div class="shell">
    /common/scratch/zblast
 </div>  
The directory saves the input query file, plain text blast result fike, html format 
blast result file, image file and error files. Results directories that are over 10
days old would be dropped. 
<p>
A utility program <i>reportUsage.pl</i> is scheduled as a daily cron job to collect 
data from blast jobs of the day, and outputs each job Id, start time and duration in 
a file named with the date at /Users/peirans/UsageReport. This statistics are examed
occasionally to learn user activities and tune execution strategies as needed. 
   

<!-- =================================================================== -->
<!-- ========= MIRROR SITES ============================================ -->
<!-- =================================================================== -->

<h1 class="zdoc"><a name="Mirror Sites">Mirror Sites</a></h1>

<P>ZFIN has 3 mirror sites around the world that contain the 
<a href="#Static Pages">static pages</a> from ZFIN.  These are referred to 
as <i>external</i> mirror sites, because they reside outside of ZFIN.
In addition, ZFIN has an <i>internal</i> mirror site, 
<a href="http://mirror.zfin.org">mirror.zfin.org</a>,
that is used to support the 
external mirror sites, and to provide non-ZFINners access to this 
documentation.

<p>As of 2001/05, the external ZFIN Mirror sites and their contacts are:
<br><br>
<table class="zdoc">
  <tr>
    <th>Mirror Site
    <th>Contact
  <tr>
    <td valign="top"><a href="http://zdb.wehi.edu.au/zdb">http://zdb.wehi.edu.au/zdb</a>
      <br>Melbourne, Australia
      <br>at <a href="http://www.wehi.edu.au">The Walter & Eliza Hall Institute</a>
    <td valign="top">Tony Kyne, <a href="mailto:tony@wehi.edu.au">tony@wehi.edu.au</a>
      Head, Information Technology Services
  <tr>
    <td valign="top"><a href="http://www.grs.nig.ac.jp:6070/index.html">
                 http://www.grs.nig.ac.jp:6070/index.html</a> 
      <br>Mishima, Japan
      <br>at <a href="http://www.nig.ac.jp/">The National Institute of 
      Genetics</a>
    <td valign="top">Yukiko Yamazaki, 
      <a href="mailto:yyamazaki@lab.nig.ac.jp">yyamazaki@lab.nig.ac.jp</a>
      <br>(also Masataka Wakabayashi,
      <a href="mailto:waka@chanko.lab.nig.ac.jp">waka@chanko.lab.nig.ac.jp</a>)
  <tr>
    <td valign="top"><a href="http://www-igbmc.u-strasbg.fr/index.html">
                 http://www-igbmc.u-strasbg.fr/index.html</a>
      <br>Strasbourg, France
      <br>at <a href="http://www-igbmc.u-strasbg.fr/">
      Institut de Genetique et de Biologie Moleculaire et Cellulaire</a>
    <td valign="top">Frederic Plewniak, 
      <a href="mailto:plewniak@titus.u-strasbg.fr">plewniak@titus.u-strasbg.fr</a>
</table>


<p>There is some bad history with mirror sites.  When ZFIN was first set up,
the mirror sites used the <kbd>mirror</kbd> utility (version 2.8) 
to setup and then update their mirrors nightly. 
<kbd>mirror</kbd> uses anonymous <a href="admin.html#FTP">FTP</a> 
to copy file hierarchies 
from one server to another.  It copies only what has changed since the last
time it was run.  
<kbd>mirror</kbd> was run against the production database itself.

<p>However, all of this broke on 2000/03/15 when ZFIN was moved from 
<a href="#Past Servers">zfishstix to chromix</a>.
The FTP directories never got set up on chromix and the mirrors
were not notified that zfishstix was no longer the server.  The mirror sites
remained stuck at 2000/03 until 2000/11, when things were partially fixed.
Things weren't fully fixed until 2001/05 when ZFIN finally got a new mirror
setup established.  

<p>The mirror sites now update their mirrors using a process that is described
in 

<div class="urlline">
<a href="ftp://zfin.org/pub/transfer/Mirror/README">ftp://zfin.org/pub/transfer/Mirror/README</a>
</div>

<p>That file describes other files in the FTP directory that are 
useful for setting up a ZFIN mirror site.  It also contains directions on 
how to do this.  <b>The README file is the best source of documentation
on how the mirror sites interact with ZFIN.  Read it.</b>

<p>ZFIN still uses <kbd>mirror</kbd> (version 2.9 now), 
and <kbd>mirror</kbd> still uses 
anonymous <a href="admin.html#FTP">FTP</a>.  The tree that is mirrored is at:

<div class="urlline">
<a href="ftp://zfin.org/pub/transfer/Mirror/site">ftp://zfin.org/pub/transfer/Mirror/site</a>
</div>

<p>This is the tree for the <a href="http://mirror.zfin.org">mirror.zfin.org</a>
site.  The mirrors are now built
off of this site, rather than directly off of the production web site.

<p><a href="http://mirror.zfin.org">mirror.zfin.org</a> is a ZFIN
mirror site.  ZFIN mirror sites 
differ from the production site in a number of ways.  Mirrors 
contain only static pages and are missing many of the directories that 
exist in production.

<p>The files backing the mirror.zfin.org site actually reside inside the
FTP directory on the same server that is currently hosting zfin.org.
Whenever we move zfin.org, we also move mirror.zfin.org and the files
backing it.  Since the mirror sites pull from its file tree, and since 
<a href="http://mirror.zfin.org/zfin_doc/">http://mirror.zfin.org/zfin_doc/</a>
is the URL we give to funders and other people outside ZFIN to give them 
access to this documentation,
mirror.zfin.org can be viewed as a production web site.

<p><a href="http://mirror.zfin.org">mirror.zfin.org</a> is updated daily
using a 
<a href="http://cvs.zfin.org/cvs/cvsweb.cgi/ZFIN_WWW/server_apps/WebSiteTools/makemirror.csh">cron job</a>
that pulls any recent changes
from <a href="#Source Code Control and CVS">CVS</a>.

<!-- =================================================================== -->
<!-- ========= LoadUp: How the whole darn thing works ================== -->
<!-- =================================================================== -->

<h1 class="zdoc"><a name="ZFIN LoadUp">ZFIN LoadUp</a></h1>

<p>These directories were created in December of 2004 to hold images 
(imageLoadUp) and PDFs (PDFLoadUp).

<p>The process of storing BLOBs in the filesytem is dependent on:<br>
 <ol>
  <li><a href="impl.html#Filesystem space and permissions">
      Filesystem space and permissions</a></li>

  <li><a href= "impl.html#Database pointers">
      Database pointers</a></li>

  <li><a href= "impl.html#Apache configuration">
      Apache configuration</a></li>

  <li><a href= "impl.html#Environment and .tt files">
      Environment and .tt files</a></li>

  <li><a href= "impl.html#Development filesystem syncing">
      Development filesystem sync</a></li>
 </ol>


<!-- ========= Filesystem space and permissions ================== -->

<h2 class="zdoc"><a name="Filesystem space and permissions">
      </a>Filesystem space and permissions</h2>

<p>Currently, images and pdfs are stored in
<a href="impl.html#ZFIN Prod LoadUp">/research/zprod/loadUp</a> on production
and <a href="impl.html#ZFIN Central LoadUp">/research/zcentral/loadUp</a>.

<p>These directories are owned by 'zfishweb'
and belong to the 'www' group.  Permissions are set this way to facillitate 
loading files from the web.  Loading from the web is controlled by a
<a href="http://cvs.zfin.org/cvs/cvsweb.cgi/ZFIN_WWW/cgi-bin/upload.cgi">cgi</a>.  
It is currently called from the image,publication, and FX curation interfaces.

<p>Each loadUp directory contains a /bkup/ directory.  /bkup directories hold
copies of images or pdfs that have been replaced by users (see documentation in
upload.cgi).


<!-- ========= Database pointers ================================= -->

<h2 class="zdoc"><a name="Database pointers">
      </a>Database pointers</h2>

<p>Database columns that used to hold BLOBs, now hold complete filenames.
For example, for image ZDB-IMAGE-010101-1 in the fish_image table,
The fimg_image column now holds the value 'ZDB-IMAGE-010101-1.jpg'.
The fimg_image_with_annotation column now holds the value
'ZDB-IMAGE-010101-1_annot.jpg', and the fimg_thumbnail column
now holds 'ZDB-IMAGE-010101-1_thumb.jpg'.  Likewise, in the publication table,
pub_file now holds values like 'ZDB-PUB-010101-1.pdf'.

<table class="zdoc">
  <caption>LoadUp file Naming Conventions</caption>
  <tr>
    <td class="term">imageLoadUp:Images</td>
    <td>
      ZDB-IMAGE-XXXXXX-XXX.[jpg/gif/jpeg]
    </td>
  </tr>
  <tr>
    <td class="term">imageLoadUp:Thumbnails</td>
    <td>
      ZDB-IMAGE-XXXXXX-XXX_thumb.[jpg/gif/jpeg]
    </td>
  </tr>
  <tr>
    <td class="term">imageLoadUp:Images with Annotation</td>
    <td>
      ZDB-IMAGE-XXXXXX-XXX_annot.[jpg/gif/jpeg]
    </td>
  </tr>
  <tr>
    <td class="term">PDFLoadUp:PDF files</td>
    <td>
      ZDB-PUB-XXXXXX-XXX.pdf (zdb-id of the pub this pdf is associated with)
    </td>
  </tr>
</table>


<!-- ========= Apache Configuration ============================== -->

<h2 class="zdoc"><a name="Apache configuration">
      </a>Apache Configuration</h2>

<p>Apache is configured to recognize /imageLoadUp as an alias of
/research/zcentral (or zprod if on production server)/loadUp/imageLoadup
and /PDFLoadUp as an alias of 
/research/zcentral (or zprod if on production server)/loadUp/PDFLoadup



<!-- ========= Environment and .tt files ========================= -->

<h2 class="zdoc"><a name="Environment and .tt files">
      </a>Environment and .tt files</h2>

<p>All translate table files on development define the following generic tags
to access these directories:
<br><br>
&lt;!--|IMAGE_LOAD|--&gt;		/imageLoadUp<br>
&lt;!--|LOADUP_FULL_PATH|--&gt;	/research/zcentral/loadUp<br>
&lt;!--|PDF_LOAD|--&gt;		/PDFLoadUp<br>

<p>Production translate table files are slightly different, as is 
the embryonix.tt file.


<p>The apache configuration, along with these generic tags, mean that in 
apg pages, to access a file in /research/zcentral/loadUp/imageLoadUp
one only needs to code &lt;!--|IMAGE_LOAD|--&gt;/$filename; where $filename is like 
'ZDB-IMAGE-040101-1'.  Note, the '/' required after the image tag and before 
the filename. 

<p>While the above apache configuration makes it easy to find the files
via an apg page, scripts that do not run through the apache server (like
upload.cgi) need a full path to know where to store an uploaded file).<br>
&lt;!--|LOADUP_FULL_PATH|--&gt; is used for this purpose.

<p>In these kinds of scripts it is not uncommon to see<br>
&lt;!--|LOADUP_FULL_PATH|--&gt;&lt;!--|IMAGE_LOAD|--&gt;/$filename as a filepath. 

<p>It is important to note that all development databases and .tt files 
point to 
the same directory on embryonix.  This means that any images loaded from hoover
can also be seen on albino. This does not mean that records will be created
in clemdb if they are created in hoovdb, however.

<p>To view images (without using a webdatablade apg), follow the example
below, substituting hoover with your website name and the zdb_id with the
image_zdb_id that you're looking for:

<p>http://hoover.zfin.org/imageLoadUp/ZDB-IMAGE-010101-1.jpg

<p>When zfin.org is on embryonix, the apache configuration changes 
slightly, and
the embryonix.tt file always points to slightly different directories.
This is to prevent changes on the development machine to affect the
production repository of pdfs/images.  

<p>See /private/http/conf/http.conf.embryonix, and embryonix.tt for details.

<p>It is possible to make your own, personal filesystem for testing pdf,
image loading.  Please speak to Dave C. or Sierra for help with this.



<!-- ========= Development filesystem syncing ===================== -->

<h2 class="zdoc"><a name="Development filesystem syncing">
      </a>Development Filesystem Syncing</h2>

<p>Every night, via a cron job, (<a href="http://cvs.zfin.org/cvs/cvsweb.cgi/ZFIN_WWW/server_apps/DB_maintenance/loadUp/remove_orphan_files.pl">remove_orphan_files.pl</a>), 
the production image/pdf filesystem is 
compared to the development filesystem.  If there are differences, the 
production filesystem is automatically replicated in development.  This
is accomplished using a (<a href="http://cvs.zfin.org/cvs/cvsweb.cgi/ZFIN_WWW/server_apps/DB_maintenance/loadUp/rsync.pl">perl wrapper</a>) 
to execute rsync. 

<p>Note: this may mean deleting or copying files from the production filesystem
to the development filesystem. See the rsync man pages for more details.

<br><br>
</div>  <!-- class="zdoc" -->

<script language="JavaScript" src="/footer.js" type="text/javascript"></script>

